@article{Aamodt1994CaseBasedReasoningFoundational,
  title = {Case-{{Based Reasoning}} - {{Foundational Issues}}, {{Methodological Variations}}, and {{System Approaches}}.},
  author = {Aamodt, Agnar and Plaza, Enric},
  date = {1994-01-01},
  journaltitle = {AI Commun.},
  doi = {10.3233/AIC-1994-7104}
}

@unpublished{Abels2021FocusingKnowledgebasedGraph,
  title = {Focusing {{Knowledge-based Graph Argument Mining}} via {{Topic Modeling}}},
  author = {Abels, Patrick and Ahmadi, Zahra and Burkhardt, Sophie and Schiller, Benjamin and Gurevych, Iryna and Kramer, Stefan},
  date = {2021-02-03},
  eprint = {2102.02086},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.02086},
  urldate = {2021-03-01},
  abstract = {Decision-making usually takes five steps: identifying the problem, collecting data, extracting evidence, identifying pro and con arguments, and making decisions. Focusing on extracting evidence, this paper presents a hybrid model that combines latent Dirichlet allocation and word embeddings to obtain external knowledge from structured and unstructured data. We study the task of sentence-level argument mining, as arguments mostly require some degree of world knowledge to be identified and understood. Given a topic and a sentence, the goal is to classify whether a sentence represents an argument in regard to the topic. We use a topic model to extract topic- and sentence-specific evidence from the structured knowledge base Wikidata, building a graph based on the cosine similarity between the entity word vectors of Wikidata and the vector of the given sentence. Also, we build a second graph based on topic-specific articles found via Google to tackle the general incompleteness of structured knowledge bases. Combining these graphs, we obtain a graph-based model which, as our evaluation shows, successfully capitalizes on both structured and unstructured data.}
}

@inproceedings{Abu-Aisheh2015ExactGraphEdit,
  title = {An {{Exact Graph Edit Distance Algorithm}} for {{Solving Pattern Recognition Problems}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Pattern Recognition Applications}} and {{Methods}}},
  author = {Abu-Aisheh, Zeina and Raveaux, Romain and Ramel, Jean-Yves and Martineau, Patrick},
  date = {2015},
  pages = {271--278},
  publisher = {SciTePress},
  location = {Lisbon, Portugal},
  doi = {10.5220/0005209202710278},
  url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0005209202710278},
  urldate = {2025-06-01},
  abstract = {Digital Library},
  eventtitle = {International {{Conference}} on {{Pattern Recognition Applications}} and {{Methods}}},
  isbn = {978-989-758-076-5}
}

@inproceedings{Adda-Decker2000InvestigatingTextNormalization,
  title = {Investigating Text Normalization and Pronunciation Variants for {{German}} Broadcast Transcription},
  booktitle = {6th {{International Conference}} on {{Spoken Language Processing}} ({{ICSLP}} 2000)},
  author = {Adda-Decker, Martine and Adda, Gilles and Lamel, Lori},
  date = {2000-10-16/2000-10-20},
  pages = {4},
  location = {Beijing, China},
  url = {https://www.isca-speech.org/archive/archive_papers/icslp_2000/i00_1266.pdf},
  urldate = {2018-09-05},
  abstract = {In this paper we describe our ongoing work concerning lexical modeling in the LIMSI broadcast transcription system for German. Lexical decomposition is investigated with a twofold goal: lexical coverage optimization and improved letter-to-sound conversion. A set of about 450 decompounding rules, developed using statistics from a 300M word corpus, reduces the OOV rate from 4.5\% to 4.0\% on a 30k development text set. Adding partial inflection stripping, the OOV rate drops to 2.9\%. For letterto-sound conversion, decompounding reduces cross-lexeme ambiguities and thus contributes to more consistent pronunciation dictionaries. Another point of interest concerns reduced pronunciation modeling. Word error rates, measured on 1.3 hours of ARTE TV broadcast, vary between 18 and 24\% depending on the show and the system configuration. Our experiments indicate that using reduced pronunciations slightly decreases word error rates.},
  langid = {english}
}

@article{Adewumi2016SystematicLiteratureReview,
  title = {A Systematic Literature Review of Open Source Software Quality Assessment Models},
  author = {Adewumi, Adewole and Misra, Sanjay and Omoregbe, Nicholas and Crawford, Broderick and Soto, Ricardo},
  date = {2016-11-08},
  journaltitle = {SpringerPlus},
  shortjournal = {SpringerPlus},
  volume = {5},
  number = {1},
  pages = {1936},
  issn = {2193-1801},
  doi = {10.1186/s40064-016-3612-4},
  url = {https://doi.org/10.1186/s40064-016-3612-4},
  urldate = {2023-10-05},
  abstract = {Many open source software (OSS) quality assessment models are proposed and available in the literature. However, there is little or no adoption of these models in practice. In order to guide the formulation of newer models so they can be acceptable by practitioners, there is need for clear discrimination of the existing models based on their specific properties. Based on this, the aim of this study is to perform a systematic literature review to investigate the properties of the existing OSS quality assessment models by classifying them with respect to their quality characteristics, the methodology they use for assessment, and their domain of application so as to guide the formulation and development of newer models. Searches in IEEE Xplore, ACM, Science Direct, Springer and Google Search is performed so as to retrieve all relevant primary studies in this regard. Journal and conference papers between the year 2003 and 2015 were considered since the first known OSS quality model emerged in 2003.}
}

@article{Afantenos2018ComparingDecodingMechanisms,
  title = {Comparing Decoding Mechanisms for Parsing Argumentative Structures},
  author = {Afantenos, Stergos and Peldszus, Andreas and Stede, Manfred},
  date = {2018-11-12},
  journaltitle = {Argument \& Computation},
  shortjournal = {AAC},
  volume = {9},
  number = {3},
  pages = {177--192},
  issn = {19462174, 19462166},
  doi = {10.3233/AAC-180033},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AAC-180033},
  urldate = {2023-10-26}
}

@inproceedings{Agarwal2022GraphNLIGraphbasedNatural,
  title = {{{GraphNLI}}: {{A Graph-based Natural Language Inference Model}} for {{Polarity Prediction}} in {{Online Debates}}},
  shorttitle = {{{GraphNLI}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {Agarwal, Vibhor and Joglekar, Sagar and Young, Anthony P. and Sastry, Nishanth},
  date = {2022-04-25},
  series = {{{WWW}} '22},
  pages = {2729--2737},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3485447.3512144},
  url = {https://dl.acm.org/doi/10.1145/3485447.3512144},
  urldate = {2023-07-26},
  abstract = {Online forums that allow participatory engagement between users have been transformative for public discussion of important issues. However, debates on such forums can sometimes escalate into full blown exchanges of hate or misinformation. An important tool in understanding and tackling such problems is to be able to infer the argumentative relation of whether a reply is supporting or attacking the post it is replying to. This so called polarity prediction task is difficult because replies may be based on external context beyond a post and the reply whose polarity is being predicted. We propose GraphNLI, a novel graph-based deep learning architecture that uses graph walk techniques to capture the wider context of a discussion thread in a principled fashion. Specifically, we propose methods to perform root-seeking graph walks that start from a post and captures its surrounding context to generate additional embeddings for the post. We then use these embeddings to predict the polarity relation between a reply and the post it is replying to. We evaluate the performance of our models on a curated debate dataset from Kialo, an online debating platform. Our model outperforms relevant baselines, including S-BERT, with an overall accuracy of 83\%.},
  isbn = {978-1-4503-9096-5}
}

@article{Agarwal2023GraphBasedContextAwareModel,
  title = {A {{Graph-Based Context-Aware Model}} to {{Understand Online Conversations}}},
  author = {Agarwal, Vibhor and Young, Anthony P. and Joglekar, Sagar and Sastry, Nishanth},
  date = {2023-11-03},
  journaltitle = {ACM Transactions on the Web},
  shortjournal = {ACM Trans. Web},
  volume = {18},
  number = {1},
  pages = {10:1--10:27},
  issn = {1559-1131},
  doi = {10.1145/3624579},
  url = {https://dl.acm.org/doi/10.1145/3624579},
  urldate = {2023-11-23},
  abstract = {Online forums that allow for participatory engagement between users have been transformative for the public discussion of many important issues. However, such conversations can sometimes escalate into full-blown exchanges of hate and misinformation. Existing approaches in natural language processing (NLP), such as deep learning models for classification tasks, use as inputs only a single comment or a pair of comments depending upon whether the task concerns the inference of properties of the individual comments or the replies between pairs of comments, respectively. However, in online conversations, comments and replies may be based on external context beyond the immediately relevant information that is input to the model. Therefore, being aware of the conversations’ surrounding contexts should improve the model’s performance for the inference task at hand. We propose GraphNLI,1 a novel graph-based deep learning architecture that uses graph walks to incorporate the wider context of a conversation in a principled manner. Specifically, a graph walk starts from a given comment and samples “nearby” comments in the same or parallel conversation threads, which results in additional embeddings that are aggregated together with the initial comment’s embedding. We then use these enriched embeddings for downstream NLP prediction tasks that are important for online conversations. We evaluate GraphNLI on two such tasks - polarity prediction and misogynistic hate speech detection - and find that our model consistently outperforms all relevant baselines for both tasks. Specifically, GraphNLI with a biased root-seeking random walk performs with a macro-F1 score of 3 and 6 percentage points better than the best-performing BERT-based baselines for the polarity prediction and hate speech detection tasks, respectively. We also perform extensive ablative experiments and hyperparameter searches to understand the efficacy of GraphNLI. This demonstrates the potential of context-aware models to capture the global context along with the local context of online conversations for these two tasks.}
}

@inproceedings{Agater2025SLANGOInitialBlueprint,
  title = {{{SLANGO}} - {{The Initial Blueprint}} of~{{Privacy-Oriented Legal Query Assistance}}: {{Exploring}} the~{{Potential}} of~{{Retrieval-Augmented Generation}} for~{{German Law Using SPR}}},
  shorttitle = {{{SLANGO}} - {{The Initial Blueprint}} of~{{Privacy-Oriented Legal Query Assistance}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Agater, Jérôme and Memari, Ammar},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  pages = {208--221},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77918-3_15},
  abstract = {This paper introduces an application of Large Language Models (LLMs) in the context of Retrieval-Augmented Generation (RAG) to the problem of privacy-preserving question answering in a legal setting. As Germany’s voluminous legal documents, including laws and court decisions, are not stored accurately in the inherent knowledge of LLMs, LLMs are prone to producing unreliable or non-existent references. By augmenting the inherent knowledge with ground truth facts retrieved from a Neo4J database, the answer-generating system can cite the facts directly. By using a locally run LLM, we mitigate the need for cloud-based data processing, preventing privacy-relevant data from leaving the system. Our preliminary results with selected legal questions show the system’s ability to provide plausible legal answers. This research lays the foundation for further studies, opening the possibility for integrating more sophisticated RAG techniques and building a user interface with deterministic quoting for precise citation and ease of use. Our study presents a step towards deploying AI in sensitive legal settings, promising a future where legal questions can be answered correctly by LLMs without sacrificing data privacy.},
  isbn = {978-3-031-77918-3},
  langid = {english}
}

@inproceedings{Aicher2024BEABuildingEngaging,
  title = {{{BEA}}: {{Building Engaging Argumentation}}},
  shorttitle = {{{BEA}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Aicher, Annalena and Weber, Klaus and André, Elisabeth and Minker, Wolfgang and Ultes, Stefan},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {279--295},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_17},
  abstract = {Exchanging arguments and knowledge in conversations is an intuitive way for humans to form opinions and reconcile opposing viewpoints. The vast amount of information available on the internet, often accessed through search engines, presents a considerable challenge. Managing and filtering this overwhelming wealth of data raises the potential for intellectual isolation. This can stem either from personalized searches that create “filter bubbles” by considering a user’s history and preferences, or from the intrinsic, albeit unconscious, tendency of users to seek information that aligns with their existing beliefs, forming “self-imposed filter bubbles”. To address this issue, we introduce a model aimed at engaging the user in a critical examination of presented arguments and propose the use of a virtual agent engaging in a deliberative dialogue with human users to facilitate a fair and unbiased opinion formation. Our experiments have demonstrated the success of these models and their implementation. As a result, this work offers valuable insights for the design of future cooperative argumentative dialogue systems.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Ajjour2018VisualizationTopicSpace,
  title = {Visualization of the {{Topic Space}} of {{Argument Search Results}} in Args.Me},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Ajjour, Yamen and Wachsmuth, Henning and Kiesel, Dora and Riehmann, Patrick and Fan, Fan and Castiglia, Giuliano and Adejoh, Rosemary and Fröhlich, Bernd and Stein, Benno},
  date = {2018-11},
  pages = {60--65},
  publisher = {Association for Computational Linguistics},
  location = {Brussels, Belgium},
  doi = {10.18653/v1/D18-2011},
  url = {https://www.aclweb.org/anthology/D18-2011},
  urldate = {2020-09-28},
  abstract = {In times of fake news and alternative facts, pro and con arguments on controversial topics are of increasing importance. Recently, we presented args.me as the first search engine for arguments on the web. In its initial version, args.me ranked arguments solely by their relevance to a topic queried for, making it hard to learn about the diverse topical aspects covered by the search results. To tackle this shortcoming, we integrated a visualization interface for result exploration in args.me that provides an instant overview of the main aspects in a barycentric coordinate system. This topic space is generated ad-hoc from controversial issues on Wikipedia and argument-specific LDA models. In two case studies, we demonstrate how individual arguments can be found easily through interactions with the visualization, such as highlighting and filtering.}
}

@inproceedings{Ajjour2019DataAcquisitionArgument,
  title = {Data {{Acquisition}} for {{Argument Search}}: {{The}} Args.Me {{Corpus}}},
  shorttitle = {Data {{Acquisition}} for {{Argument Search}}},
  booktitle = {{{KI}} 2019: {{Advances}} in {{Artificial Intelligence}}},
  author = {Ajjour, Yamen and Wachsmuth, Henning and Kiesel, Johannes and Potthast, Martin and Hagen, Matthias and Stein, Benno},
  editor = {Benzmüller, Christoph and Stuckenschmidt, Heiner},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {48--59},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-30179-8_4},
  abstract = {Argument search is the study of search engine technology that can retrieve arguments for potentially controversial topics or claims upon user request. The design of an argument search engine is tied to its underlying argument acquisition paradigm. More specifically, the employed paradigm controls the trade-off between retrieval precision and recall and thus determines basic search characteristics: Compiling an exhaustive argument corpus offline benefits precision at the expense of recall, whereas retrieving arguments from the web on-the-fly benefits recall at the expense of precision. This paper presents the new corpus of our argument search engine args.me, which follows the former paradigm. We freely provide the corpus to the community. With 387 606 arguments it is one of the largest argument resources available so far. In a qualitative analysis, we compare the args.me corpus acquisition paradigm to that of two other argument search engines, and we report first empirical insights into how people search with args.me.},
  isbn = {978-3-030-30179-8},
  langid = {english}
}

@inproceedings{Ajjour2025ExploringLLMPriming,
  title = {Exploring {{LLM Priming Strategies}} for {{Few-Shot Stance Classification}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Ajjour, Yamen and Wachsmuth, Henning},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {11--23},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.2/},
  urldate = {2025-07-28},
  abstract = {Large language models (LLMs) are effective in predicting the labels of unseen target instances if instructed for the task and training instances via the prompt. LLMs generate a text with higher probability if the prompt contains text with similar characteristics, a phenomenon, called priming, that especially affects argumentation. An open question in NLP is how to systematically exploit priming to choose a set of instances suitable for a given task. For stance classification, LLMs may be primed with few-shot instances prior to identifying whether a given argument is pro or con a topic. In this paper, we explore two priming strategies for few-shot stance classification: one takes those instances that are most semantically similar, and the other chooses those that are most stance-similar. Experiments on three common stance datasets suggest that priming an LLM with stance-similar instances is particularly effective in few-shot stance classification compared to baseline strategies, and behaves largely consistently across different LLM variants.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Akbik2018ContextualStringEmbeddings,
  title = {Contextual {{String Embeddings}} for {{Sequence Labeling}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  date = {2018-08},
  pages = {1638--1649},
  publisher = {Association for Computational Linguistics},
  location = {Santa Fe, New Mexico, USA},
  url = {https://www.aclweb.org/anthology/C18-1139},
  urldate = {2021-02-09},
  abstract = {Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair},
  eventtitle = {{{COLING}} 2018}
}

@inproceedings{Aker2017ExtensibleMultilingualOpen,
  title = {An {{Extensible Multilingual Open Source Lemmatizer}}},
  booktitle = {{{RANLP}} 2017 - {{Recent Advances}} in {{Natural Language Processing Meet Deep Learning}}},
  author = {Aker, Ahmet and Petrak, Johann and Sabbah, Firas},
  date = {2017-11-10},
  pages = {40--45},
  publisher = {Incoma Ltd. Shoumen, Bulgaria},
  doi = {10.26615/978-954-452-049-6_006},
  url = {http://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP006.pdf},
  urldate = {2021-03-08},
  abstract = {We present GATE DictLemmatizer, a multilingual open source lemmatizer for the GATE NLP framework that currently supports English, German, Italian, French, Dutch, and Spanish, and is easily extensible to other languages. The software is freely available under the LGPL license. The lemmatization is based on the Helsinki Finite-State Transducer Technology (HFST) and lemma dictionaries automatically created from Wiktionary. We evaluate the performance of the lemmatizers against TreeTagger, which is only freely available for research purposes. Our evaluation shows that DictLemmatizer achieves similar or even better results than TreeTagger for languages where there is support from HFST. The performance drops when there is no support from HFST and the entire lemmatization process is based on lemma dictionaries. However, the results are still satisfactory given the fact that DictLemmatizer isopen-source and can be easily extended to other languages. The software for extending the lemmatizer by creating word lists from Wiktionary dictionaries is also freely available as open-source software.},
  eventtitle = {{{RANLP}} 2017 - {{Recent Advances}} in {{Natural Language Processing Meet Deep Learning}}},
  isbn = {978-954-452-049-6}
}

@inproceedings{Al-Debagy2018ComparativeReviewMicroservices,
  title = {A {{Comparative Review}} of {{Microservices}} and {{Monolithic Architectures}}},
  booktitle = {2018 {{IEEE}} 18th {{International Symposium}} on {{Computational Intelligence}} and {{Informatics}} ({{CINTI}})},
  author = {Al-Debagy, Omar and Martinek, Peter},
  date = {2018-11},
  pages = {000149--000154},
  issn = {2471-9269},
  doi = {10.1109/CINTI.2018.8928192},
  url = {https://ieeexplore.ieee.org/abstract/document/8928192},
  urldate = {2023-11-21},
  abstract = {Microservices' architecture is getting attention in the academic community and the industry, and mostly is compared with monolithic architecture. Plenty of the results of these research papers contradict each other regarding the performance of these architectures. Therefore, these two architectures are compared in this paper, and some specific configurations of microservices' applications are evaluated as well in the term of service discovery. Monolithic architecture in concurrency testing showed better performance in throughput by 6\% when compared to microservices architecture. The load testing scenario did not present significant difference between the two architectures. Furthermore, a third test comparing microservices applications built with different service discovery technologies such as Consul and Eureka showed that applications with Consul presented better results in terms of throughput.},
  eventtitle = {2018 {{IEEE}} 18th {{International Symposium}} on {{Computational Intelligence}} and {{Informatics}} ({{CINTI}})}
}

@inproceedings{Al-Khatib2020EndtoEndArgumentationKnowledge,
  title = {End-to-{{End Argumentation Knowledge Graph Construction}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Al-Khatib, Khalid and Hou, Yufang and Wachsmuth, Henning and Jochim, Charles and Bonin, Francesca and Stein, Benno},
  date = {2020-04-03},
  volume = {34},
  pages = {7367--7374},
  doi = {10.1609/aaai.v34i05.6231},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/6231},
  urldate = {2020-09-28},
  abstract = {This paper studies the end-to-end construction of an argumentation knowledge graph that is intended to support argument synthesis, argumentative question answering, or fake news detection, among others. The study is motivated by the proven effectiveness of knowledge graphs for interpretable and controllable text generation and exploratory search. Original in our work is that we propose a model of the knowledge encapsulated in arguments. Based on this model, we build a new corpus that comprises about 16k manual annotations of 4740 claims with instances of the model's elements, and we develop an end-to-end framework that automatically identifies all modeled types of instances. The results of experiments show the potential of the framework for building a web-based argumentation graph that is of high quality and large scale.}
}

@inproceedings{Aleksovski2008UsingMultipleOntologies,
  title = {Using Multiple Ontologies as Background Knowledge in Ontology Matching},
  author = {Aleksovski, Zharko and family=Kate, given=Warner, prefix=ten, useprefix=false and family=Harmelen, given=Frank, prefix=van, useprefix=false},
  date = {2008},
  abstract = {Using ontology as a background knowledge in ontology matching is being actively investigated. Recently the idea attracted attention because of the growing number of available ontologies, which in turn opens up new opportunities, and reduces the problem of finding candidate background knowledge. Particularly interesting is the approach of using multiple ontologies as background knowledge, which we explore in this paper. We report on an experimental study conducted using real-life ontologies published online. The first contribution of this paper is an exploration about how the matching performance behaves when multiple background ontologies are used cumulatively. As a second contribution, we analyze the impact that different types of background ontologies have to the matching performance. With respect to the precision and recall, more background knowledge monotonically increases the recall, while the precision depends on the quality of the added background ontology, with high quality tending to increase, and the low quality tending to decrease the precision.}
}

@article{Aleven2003UsingBackgroundKnowledge,
  title = {Using Background Knowledge in Case-Based Legal Reasoning: {{A}} Computational Model and an Intelligent Learning Environment},
  shorttitle = {Using Background Knowledge in Case-Based Legal Reasoning},
  author = {Aleven, Vincent},
  date = {2003-11},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {150},
  number = {1--2},
  pages = {183--237},
  issn = {00043702},
  doi = {10.1016/S0004-3702(03)00105-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000437020300105X},
  urldate = {2019-08-20},
  abstract = {Researchers in the field of AI and Law have developed a number of computational models of the arguments that skilled attorneys make based on past cases. However, these models have not accounted for the ways that attorneys use middle-level normative background knowledge (1) to organize multi-case arguments, (2) to reason about the significance of differences between cases, and (3) to assess the relevance of precedent cases to a given problem situation. We present a novel model, that accounts for these argumentation phenomena. An evaluation study showed that arguments about the significance of distinctions based on this model help predict the outcome of cases in the area of trade secrets law, confirming the quality of these arguments. The model forms the basis of an intelligent learning environment called CATO, which was designed to help beginning law students acquire basic argumentation skills. CATO uses the model for a number of purposes, including the dynamic generation of argumentation examples. In a second evaluation study, carried out in the context of an actual legal writing course, we compared instruction with CATO against the best traditional legal writing instruction. The results indicate that CATO’s example-based instructional approach is effective in teaching basic argumentation skills. However, a more “integrated” approach appears to be needed if students are to achieve better transfer of these skills to more complex contexts. CATO’s argumentation model and instructional environment are a contribution to the research fields of AI and Law, Case-Based Reasoning, and AI and Education.},
  langid = {english}
}

@inproceedings{Alhamzeh2022ItTimeReason,
  title = {It's {{Time}} to {{Reason}}: {{Annotating Argumentation Structures}} in {{Financial Earnings Calls}}: {{The FinArg Dataset}}},
  shorttitle = {It's {{Time}} to {{Reason}}},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{Financial Technology}} and {{Natural Language Processing}} ({{FinNLP}})},
  author = {Alhamzeh, Alaa and Fonck, Romain and Versmée, Erwan and Egyed-Zsigmond, Elöd and Kosch, Harald and Brunie, Lionel},
  date = {2022-12},
  pages = {163--169},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, United Arab Emirates (Hybrid)},
  url = {https://aclanthology.org/2022.finnlp-1.22},
  urldate = {2023-07-26},
  abstract = {With the goal of reasoning on the financial textual data, we present in this paper, a novel approach for annotating arguments, their components and relations in the transcripts of earnings conference calls (ECCs). The proposed scheme is driven from the argumentation theory at the micro-structure level of discourse. We further conduct a manual annotation study with four annotators on 136 documents. We obtained inter-annotator agreement of lphaU = 0.70 for argument components and lpha = 0.81 for argument relations. The final created corpus, with the size of 804 documents, as well as the annotation guidelines are publicly available for researchers in the domains of computational argumentation, finance and FinNLP.},
  eventtitle = {{{FinNLP}} 2022}
}

@online{Allen-Zhu2024PhysicsLanguageModels,
  title = {Physics of {{Language Models}}: {{Part}} 3.3, {{Knowledge Capacity Scaling Laws}}},
  shorttitle = {Physics of {{Language Models}}},
  author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  date = {2024-04-08},
  eprint = {2404.05405},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.05405},
  urldate = {2024-04-09},
  abstract = {Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation. More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include: * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train. * Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.},
  pubstate = {prepublished}
}

@incollection{Allen2003NaturalLanguageProcessing,
  title = {Natural Language Processing},
  booktitle = {Encyclopedia of {{Computer Science}}},
  author = {Allen, James F.},
  date = {2003-01-01},
  pages = {1218--1222},
  publisher = {{John Wiley and Sons Ltd.}},
  location = {GBR},
  abstract = {Natural language processing (NLP) refers to computer systems that analyze, attempt to understand, or produce one or more human languages, such as English, Japanese, Italian, or Russian. The input might be text, spoken language, or keyboard input. The task might be to translate to another language, to comprehend and represent the content of text, to build a database or generate summaries, or to maintain a dialogue with a user as part of an interface for database/information retrieval (q.v.). This article addresses issues in natural language comprehension and generation from text or keyboard input. Similar techniques can be used for spoken language by adding a system for speech recognition (see SPEECH RECOGNITION AND SYNTHESIS).},
  isbn = {978-0-470-86412-8}
}

@article{Allwein2000ReducingMulticlassBinary,
  title = {Reducing {{Multiclass}} to {{Binary}}: {{A Unifying Approach}} for {{Margin Classifiers}}},
  shorttitle = {Reducing {{Multiclass}} to {{Binary}}},
  author = {Allwein, Erin L. and Schapire, Robert E. and Singer, Yoram},
  date = {2000},
  journaltitle = {Journal of Machine Learning Research},
  volume = {1},
  pages = {113--141},
  issn = {ISSN 1533-7928},
  url = {https://jmlr.org/papers/v1/allwein00a.html},
  urldate = {2025-07-07},
  abstract = {We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm.  The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used.  We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms.  The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms.  We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.},
  issue = {Dec}
}

@inproceedings{Alshomary2020ExtractiveSnippetGeneration,
  title = {Extractive {{Snippet Generation}} for {{Arguments}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Alshomary, Milad and Düsterhus, Nick and Wachsmuth, Henning},
  date = {2020-07-25},
  series = {{{SIGIR}} '20},
  pages = {1969--1972},
  publisher = {Association for Computing Machinery},
  location = {Virtual Event, China},
  doi = {10.1145/3397271.3401186},
  url = {https://doi.org/10.1145/3397271.3401186},
  urldate = {2020-09-02},
  abstract = {Snippets are used in web search to help users assess the relevance of retrieved results to their query. Recently, specialized search engines have arisen that retrieve pro and con arguments on controversial issues. We argue that standard snippet generation is insufficient to represent the core reasoning of an argument. In this paper, we introduce the task of generating a snippet that represents the main claim and reason of an argument. We propose a query-independent extractive summarization approach to this task that uses a variant of PageRank to assess the importance of sentences based on their context and argumentativeness. In both automatic and manual evaluation, our approach outperforms strong baselines.},
  isbn = {978-1-4503-8016-4}
}

@inproceedings{Alshomary2022GeneratingContrastiveSnippets,
  title = {Generating {{Contrastive Snippets}} for {{Argument Search}}},
  booktitle = {Computational {{Models}} of {{Argument}}},
  author = {Alshomary, Milad and Rieskamp, Jonas and Wachsmuth, Henning},
  date = {2022},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {353},
  pages = {21--31},
  publisher = {IOS Press},
  location = {Cardiff, Wales},
  doi = {10.3233/FAIA220138},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA220138},
  urldate = {2022-09-15},
  abstract = {In argument search, snippets provide an overview of the aspects discussed by the arguments retrieved for a queried controversial topic. Existing work has focused on generating snippets that are representative of an argument’s content while remaining argumentative. In this work, we argue that the snippets should also be contrastive, that is, they should highlight the aspects that make an argument unique in the context of others. Thereby, aspect diversity is increased and redundancy is reduced. We present and compare two snippet generation approaches that jointly optimize representativeness and contrastiveness. According to our experiments, both approaches have advantages, and one is able to generate representative yet sufficiently contrastive snippets.}
}

@inproceedings{Alsinet2018ProbabilisticAuthorCenteredModel,
  title = {A {{Probabilistic Author-Centered Model}} for {{Twitter Discussions}}},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge-Based Systems}}. {{Theory}} and {{Foundations}}},
  author = {Alsinet, Teresa and Argelich, Josep and Béjar, Ramón and Esteva, Francesc and Godo, Lluis},
  editor = {Medina, Jesús and Ojeda-Aciego, Manuel and Verdegay, José Luis and Pelta, David A. and Cabrera, Inma P. and Bouchon-Meunier, Bernadette and Yager, Ronald R.},
  date = {2018},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {683--695},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-91476-3_56},
  abstract = {In a recent work some of the authors have developed an argumentative approach for discovering relevant opinions in Twitter discussions with probabilistic valued relationships. Given a Twitter discussion, the system builds an argument graph where each node denotes a tweet and each edge denotes a criticism relationship between a pair of tweets of the discussion. Relationships between tweets are associated with a probability value, indicating the uncertainty on whether they actually hold. In this work we introduce and investigate a natural extension of the representation model, referred as probabilistic author-centered model. In this model, tweets by a same author are grouped, describing his/her opinion in the discussion, and are represented with a single node in the graph, while edges stand for criticism relationships between author’s opinions. In this new model, interactions between authors can give rise to circular criticism relationships, and the probability of one opinion criticizing another is evaluated from the criticism probabilities among the individual tweets in both opinions.},
  isbn = {978-3-319-91476-3},
  langid = {english}
}

@unpublished{Alvez2019CommonsenseReasoningUsing,
  title = {Commonsense {{Reasoning Using WordNet}} and {{SUMO}}: A {{Detailed Analysis}}},
  shorttitle = {Commonsense {{Reasoning Using WordNet}} and {{SUMO}}},
  author = {Álvez, Javier and Gonzalez-Dios, Itziar and Rigau, German},
  date = {2019-09-06},
  eprint = {1909.02314},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.02314},
  urldate = {2021-02-01},
  abstract = {We describe a detailed analysis of a sample of large benchmark of commonsense reasoning problems that has been automatically obtained from WordNet, SUMO and their mapping. The objective is to provide a better assessment of the quality of both the benchmark and the involved knowledge resources for advanced commonsense reasoning tasks. By means of this analysis, we are able to detect some knowledge misalignments, mapping errors and lack of knowledge and resources. Our final objective is the extraction of some guidelines towards a better exploitation of this commonsense knowledge framework by the improvement of the included resources.}
}

@article{Aly2024TabVerTabularFact,
  title = {{{TabVer}}: {{Tabular Fact Verification}} with {{Natural Logic}}},
  shorttitle = {{{TabVer}}},
  author = {Aly, Rami and Vlachos, Andreas},
  date = {2024},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {1648--1671},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  doi = {10.1162/tacl_a_00722},
  url = {https://aclanthology.org/2024.tacl-1.89/},
  urldate = {2025-07-29},
  abstract = {Fact verification on tabular evidence incentivizes the use of symbolic reasoning models where a logical form is constructed (e.g., a LISP-style program), providing greater verifiability than fully neural approaches. However, these logical forms typically rely on well-formed tables, restricting their use in many scenarios. An emerging symbolic reasoning paradigm for textual evidence focuses on natural logic inference, which constructs proofs by modeling set-theoretic relations between a claim and its evidence in natural language. This approach provides flexibility and transparency but is less compatible with tabular evidence since the relations do not extend to arithmetic functions. We propose a set-theoretic interpretation of numerals and arithmetic functions in the context of natural logic, enabling the integration of arithmetic expressions in deterministic proofs. We leverage large language models to generate arithmetic expressions by generating questions about salient parts of a claim which are answered by executing appropriate functions on tables. In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.4, outperforming both fully neural and symbolic reasoning models by 3.4 points. When evaluated on TabFact without any further training, our method remains competitive with an accuracy lead of 0.5 points.}
}

@article{AlZubaer2023PerformanceAnalysisLarge,
  title = {Performance Analysis of Large Language Models in the Domain of Legal Argument Mining},
  author = {Al Zubaer, Abdullah and Granitzer, Michael and Mitrović, Jelena},
  date = {2023},
  journaltitle = {Frontiers in Artificial Intelligence},
  volume = {6},
  issn = {2624-8212},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2023.1278796},
  urldate = {2024-02-10},
  abstract = {Generative pre-trained transformers (GPT) have recently demonstrated excellent performance in various natural language tasks. The development of ChatGPT and the recently released GPT-4 model has shown competence in solving complex and higher-order reasoning tasks without further training or fine-tuning. However, the applicability and strength of these models in classifying legal texts in the context of argument mining are yet to be realized and have not been tested thoroughly. In this study, we investigate the effectiveness of GPT-like models, specifically GPT-3.5 and GPT-4, for argument mining via prompting. We closely study the model's performance considering diverse prompt formulation and example selection in the prompt via semantic search using state-of-the-art embedding models from OpenAI and sentence transformers. We primarily concentrate on the argument component classification task on the legal corpus from the European Court of Human Rights. To address these models' inherent non-deterministic nature and make our result statistically sound, we conducted 5-fold cross-validation on the test set. Our experiments demonstrate, quite surprisingly, that relatively small domain-specific models outperform GPT 3.5 and GPT-4 in the F1-score for premise and conclusion classes, with 1.9\% and 12\% improvements, respectively. We hypothesize that the performance drop indirectly reflects the complexity of the structure in the dataset, which we verify through prompt and data analysis. Nevertheless, our results demonstrate a noteworthy variation in the performance of GPT models based on prompt formulation. We observe comparable performance between the two embedding models, with a slight improvement in the local model's ability for prompt selection. This suggests that local models are as semantically rich as the embeddings from the OpenAI model. Our results indicate that the structure of prompts significantly impacts the performance of GPT models and should be considered when designing them.}
}

@article{Amgoud2002InferringInconsistencyPreferenceBased,
  title = {Inferring from {{Inconsistency}} in {{Preference-Based Argumentation Frameworks}}},
  author = {Amgoud, Leila and Cayrol, Claudette},
  date = {2002-06-01},
  journaltitle = {Journal of Automated Reasoning},
  shortjournal = {Journal of Automated Reasoning},
  volume = {29},
  number = {2},
  pages = {125--169},
  issn = {1573-0670},
  doi = {10.1023/A:1021603608656},
  url = {https://doi.org/10.1023/A:1021603608656},
  urldate = {2025-09-09},
  abstract = {Argumentation is a promising approach to handle inconsistent knowledge bases, based on the justification of plausible conclusions by arguments. Because of inconsistency, however, arguments may be defeated by counterarguments (or defeaters). The problem is thus to select the most acceptable arguments. In this paper we investigate preference-based acceptability. The basic idea is to accept undefeated arguments and also arguments that are preferred to their defeaters. We say that these arguments defend themselves against their defeaters. We define argumentation frameworks based on that preference-based acceptability. Finally, we study associated inference relations for reasoning with inconsistent knowledge bases.},
  langid = {english}
}

@article{Amgoud2002ReasoningModelBased,
  title = {A {{Reasoning Model Based}} on the {{Production}} of {{Acceptable Arguments}}},
  author = {Amgoud, Leila and Cayrol, Claudette},
  date = {2002-03-01},
  journaltitle = {Annals of Mathematics and Artificial Intelligence},
  shortjournal = {Annals of Mathematics and Artificial Intelligence},
  volume = {34},
  number = {1},
  pages = {197--215},
  issn = {1573-7470},
  doi = {10.1023/A:1014490210693},
  url = {https://doi.org/10.1023/A:1014490210693},
  urldate = {2025-09-09},
  abstract = {Argumentation is a reasoning model based on the construction of arguments and counter-arguments (or defeaters) followed by the selection of the most acceptable of them. In this paper, we refine the argumentation framework proposed by Dung by taking into account preference relations between arguments in order to integrate two complementary points of view on the concept of acceptability: acceptability based on the existence of direct counter-arguments and acceptability based on the existence of defenders. An argument is thus acceptable if it is preferred to its direct defeaters or if it is defended against its defeaters. This also refines previous works by Prakken and Sartor, by associating with each argument a notion of strength, while these authors embed preferences in the definition of the defeat relation. We propose a revised proof theory in terms of AND/OR trees, verifying if a given argument is acceptable, which better reflects the dialectical form of argumentation.},
  langid = {english}
}

@article{Amgoud2006FinalReviewReport,
  title = {Final {{Review}} and {{Report}} on {{Formal Argumentation System}}},
  author = {Amgoud, Leila and Bodenstaff, L. and Caminada, M. and McBurney, P. and Parsons, Simon and Prakken, Henry and Veenen, J. and Vreeswijk, Gerard},
  date = {2006-01-01}
}

@inproceedings{Amin2018CasebasedReasoningNatural,
  title = {Case-Based {{Reasoning}} in {{Natural Language Processing}}: {{Word2vec VS fastText}}},
  booktitle = {Proceedings of the 23rd {{UK Workshop}} on {{Case-Based Reasoning}}},
  author = {Amin, Kareem and Lancaster, George and Kapetanakis, Stelios and Althoff, Klaus-Dieter and Dengel, Andreas and Petridis, Miltos},
  date = {2018-12},
  publisher = {{School of Computing, Engineering and Mathematics, University of Brighton, UK}},
  location = {Cambridge, United Kingdom},
  abstract = {Businesses can benefi t greatly from analysing their document assets. These can vary greatly from plain text messages across customer support tickets to complex message exchanges and workflow logs within countless business transactions. Decoding text-based domain knowledge can be a challenging task due to the need for a comprehensive representation and evaluation of the business process ontology, activities, rules and paths. To provide an adequate process coverage, significant time and monetary resources should be invested as well as a high maintenance portfolio, especially for large processes and environments that change dynamically. This work investigates a novel natural language processing path which combines Case-based Reasoning and Deep Neural Networks. Our aim is to minimize the effort from domain experts while extracting domain knowledge from rich text, containing domain abbreviations, grammatically incorrect text and mixed language. Our proposed approach seems promising and a possible future direction in the industry.},
  eventtitle = {23rd {{UK Workshop}} on {{Case-Based Reasoning}}}
}

@inproceedings{Amorim2022ConnectingNonFunctionalRequirements,
  title = {Connecting {{Non-Functional Requirements}} to {{Open Source Ecosystems Health}}},
  booktitle = {Proceedings of the 16th {{Brazilian Symposium}} on {{Software Components}}, {{Architectures}}, and {{Reuse}}},
  author = {Amorim, Simone da Silva and Mcgregor, John D. and family=Almeida, given=Eduardo Santana, prefix=de, useprefix=false and Garcia Chavez, Christina von Flach},
  date = {2022-10-03},
  series = {{{SBCARS}} '22},
  pages = {76--80},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3559712.3559719},
  url = {https://dl.acm.org/doi/10.1145/3559712.3559719},
  urldate = {2023-10-05},
  abstract = {One efficient way to perceive the effects of design decisions is by analyzing and evaluating Non-Functional Requirements (NFRs). A design decison can contribute positively or negatively toward specific NFRs. In their turn, NFRs describe how the software operates, representing essential quality characteristics of the software systems. In addition, the typical way of perceiving the “quality” of a software ecosystem is through the concept of ecosystem health and its health indicators. Considering the descriptive nature of NFRs representing a quality characteristic of the system, they could be a feasible way to know ecosystem health. Through their connection with the health indicators, it is possible to sketch paths to understand the influence of the NFRs on the health indicators and realize how the ecosystem health perceives the design decisions. This study aims to understand and map influences from NFRs to health indicators based on evidence found in KDE, a real-world ecosystem. We conducted mixed-methods research, including a survey with ecosystem experts and an adapted practitioner-evidence framework. Findings present a high-level descriptive mapping with connections between NFRs and health indicators, besides explaining evidence found in the KDE ecosystem.},
  isbn = {978-1-4503-9745-2}
}

@inproceedings{Angeli2014NaturalLINaturalLogic,
  title = {{{NaturalLI}}: {{Natural Logic Inference}} for {{Common Sense Reasoning}}},
  shorttitle = {{{NaturalLI}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Angeli, Gabor and Manning, Christopher D.},
  date = {2014-10},
  pages = {534--545},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1059},
  url = {https://www.aclweb.org/anthology/D14-1059},
  urldate = {2020-06-04},
  eventtitle = {{{EMNLP}} 2014}
}

@inproceedings{Ansel2024PyTorch2Faster,
  title = {{{PyTorch}} 2: {{Faster Machine Learning Through Dynamic Python Bytecode Transformation}} and {{Graph Compilation}}},
  shorttitle = {{{PyTorch}} 2},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}, {{Volume}} 2},
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  date = {2024-04-27},
  series = {{{ASPLOS}} '24},
  volume = {2},
  pages = {929--947},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3620665.3640366},
  url = {https://dl.acm.org/doi/10.1145/3620665.3640366},
  urldate = {2025-07-15},
  abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27× inference and 1.41× training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
  isbn = {979-8-4007-0385-0}
}

@online{Anthony2017ThinkingFastSlow,
  title = {Thinking {{Fast}} and {{Slow}} with {{Deep Learning}} and {{Tree Search}}},
  author = {Anthony, Thomas and Tian, Zheng and Barber, David},
  date = {2017-12-03},
  eprint = {1705.08439},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.08439},
  url = {http://arxiv.org/abs/1705.08439},
  urldate = {2024-03-28},
  abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
  pubstate = {prepublished}
}

@book{Aristotle2007RhetoricTheoryCivic,
  title = {On {{Rhetoric}}: {{A Theory}} of {{Civic Discourse}}},
  shorttitle = {On {{Rhetoric}}},
  author = {{Aristotle}},
  translator = {Kennedy, George A.},
  date = {2007},
  edition = {2},
  publisher = {Oxford University Press},
  location = {New York},
  url = {https://archive.org/details/on-rhetoric-a-theory-of-civic-discourse-aristotle},
  urldate = {2025-09-08},
  isbn = {978-0-19-530509-8},
  langid = {english},
  pagetotal = {349}
}

@article{Artstein2008InterCoderAgreementComputational,
  title = {Inter-{{Coder Agreement}} for {{Computational Linguistics}}},
  author = {Artstein, Ron and Poesio, Massimo},
  date = {2008-09-30},
  journaltitle = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {555--596},
  publisher = {MIT Press},
  issn = {0891-2017},
  doi = {10.1162/coli.07-034-R2},
  url = {https://doi.org/10.1162/coli.07-034-R2},
  urldate = {2021-02-23},
  abstract = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.}
}

@incollection{Artstein2017InterannotatorAgreement,
  title = {Inter-Annotator {{Agreement}}},
  booktitle = {Handbook of {{Linguistic Annotation}}},
  author = {Artstein, Ron},
  editor = {Ide, Nancy and Pustejovsky, James},
  date = {2017},
  pages = {297--313},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-024-0881-2_11},
  url = {https://doi.org/10.1007/978-94-024-0881-2_11},
  urldate = {2021-02-23},
  abstract = {This chapter touches upon several issues in the calculation and assessment of inter-annotator agreement. It gives an introduction to the theory behind agreement coefficients and examples of their application to linguistic annotation tasks. Specific examples explore variation in annotator performance due to heterogeneous data, complex labels, item difficulty, and annotator differences, showing how global agreement coefficients may mask these sources of variation, and how detailed agreement studies can give insight into both the annotation process and the nature of the underlying data. The chapter also reviews recent work on using machine learning to exploit the variation among annotators and learn detailed models from which accurate labels can be inferred. I therefore advocate an approach where agreement studies are not used merely as a means to accept or reject a particular annotation scheme, but as a tool for exploring patterns in the data that are being annotated.},
  isbn = {978-94-024-0881-2},
  langid = {english}
}

@inproceedings{Arumae2019AnnotatingCreatingSummary,
  title = {Towards {{Annotating}} and {{Creating Summary Highlights}} at {{Sub-sentence Level}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{New Frontiers}} in {{Summarization}}},
  author = {Arumae, Kristjan and Bhatia, Parminder and Liu, Fei},
  date = {2019-11},
  pages = {64--69},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-5408},
  url = {https://aclanthology.org/D19-5408},
  urldate = {2023-10-26},
  abstract = {Highlighting is a powerful tool to pick out important content and emphasize. Creating summary highlights at the sub-sentence level is particularly desirable, because sub-sentences are more concise than whole sentences. They are also better suited than individual words and phrases that can potentially lead to disfluent, fragmented summaries. In this paper we seek to generate summary highlights by annotating summary-worthy sub-sentences and teaching classifiers to do the same. We frame the task as jointly selecting important sentences and identifying a single most informative textual unit from each sentence. This formulation dramatically reduces the task complexity involved in sentence compression. Our study provides new benchmarks and baselines for generating highlights at the sub-sentence level.}
}

@article{Arumugam2022DevelopmentArgumentBased,
  title = {Development of Argument Based Opinion Mining Model with Sentimental Data Analysis from Twitter Content},
  author = {Arumugam, S. S.},
  date = {2022},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {34},
  number = {15},
  pages = {e6956},
  issn = {1532-0634},
  doi = {10.1002/cpe.6956},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6956},
  urldate = {2023-10-20},
  abstract = {In present scenario, social networks have developed massive in practice and society impact. Specifically, micro-blogging is on trend in various platforms, such as Twitter, Instagram to evaluate public opinions for various issues. In recent times, some methods are developed for evaluating Twitter messages, based on the sentiment and opinions presented in tweets, corresponding to the hash-tags and keywords. However, these models have some issues in handling the contradictory content and inconsistent data. Considering with this, this article presents an argument based opinion mining model with sentimental data analysis, for extracting specific argument, which is assessed in bottom-up manner from the content from society emotion's reflects on the messages. Moreover, this model makes the user to pull out the arguments from a document set, which contains content from commercial sites, to extract the mostly argued positive and negative content. This model use natural language processing techniques, extraction of argument words for defining the decisions. The classification Naive Bayes classification is used for categorizing the results widely under agreed or disagreed. The experimental results prove that the proposed model provides feasible and appropriate results in argument analysis from Twitter content.},
  langid = {english}
}

@inproceedings{Asai2023TaskawareRetrievalInstructions,
  title = {Task-Aware {{Retrieval}} with {{Instructions}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Asai, Akari and Schick, Timo and Lewis, Patrick and Chen, Xilun and Izacard, Gautier and Riedel, Sebastian and Hajishirzi, Hannaneh and Yih, Wen-tau},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {3650--3675},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.225},
  url = {https://aclanthology.org/2023.findings-acl.225/},
  urldate = {2025-07-15},
  abstract = {We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query. We introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task retrieval system trained on BERRI with instructions that can adapt to a new task without any parameter updates. TART advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X2-Retrieval, to better reflect real-world scenarios in which diverse domains and tasks are pooled. TART significantly outperforms competitive baselines in this setup, further highlighting the effectiveness of guiding retrieval with instructions.},
  eventtitle = {Findings 2023}
}

@thesis{Ashley1988ModellingLegalArgument,
  type = {phdthesis},
  title = {Modelling Legal Argument: Reasoning with Cases and Hypotheticals},
  shorttitle = {Modelling Legal Argument},
  author = {Ashley, Kevin D.},
  date = {1988},
  institution = {University of Massachusetts},
  location = {USA},
  abstract = {This dissertation is about adversarial, case-based reasoning and the HYPO program that performs adversarial reasoning with cases and hypotheticals in the legal domain. The dissertation identifies and describes basic case-based operations, an adversarial, case-based reasoning process, a schematic structure for case-based arguments, the kinds of counter-examples that arise and the knowledge sources necessary to support adversarial, case-based reasoning. The HYPO program embodies the methodology. It comprises: (1) a structured Case Knowledge Base ("CKB") of actual legal cases; (2) an indexing scheme ("dimensions") for retrieval of relevant cases from the CKB; (3) methods for analyzing problem situations and retrieving relevant cases; (4) methods for interpreting and assessing the relevancy of past cases by "positioning" the problem situation with respect to relevant existing cases in the CKB as seen from the viewpoint of the problem at hand and finding the most-on-point cases; (5) methods for comparing/contrasting cases (e.g., citing, distinguishing, finding counter-examples); (6) methods for posing hypotheticals that test the sensitivity of the problem situation to changes, particularly with regard to potentially adverse effects of new damaging facts coming to light and existing favorable ones being discredited; (7) methods for generating "3-ply" argument outlines to play out realistic legal arguments citing cases in a manner familiar to attorneys; and (8) methods for explaining alternative decisions of the problem situation by posing hypotheticals, comparing arguments and summarizing the precedents. HYPO's performance compares favorably to that of judges and attorneys in actual legal cases. The law is an excellent domain to study case-based reasoning since by its very nature it: (1) espouses a doctrine of precedent in which prior cases are the primary tools for justifying legal conclusions; and (2) employs precedential reasoning to make up for the lack of strong domain models with which to reason deductively about problem situations. The law is also a paradigm for adversarial case-based reasoning; there are "no right answers", only arguments pitting interpretations of cases and facts against each other. The dissertation addresses issues of central concern to Artificial Intelligence including: relevance and credit assignment, indexing and inference control, argumentation, analogical reasoning and explanation.},
  annotation = {Order No: GAX88-13198}
}

@inproceedings{Ashley1997ReasoningSymbolicallyPartially,
  title = {Reasoning {{Symbolically About Partially Matched Cases}}},
  booktitle = {Proceedings of the 15th {{International Joint Conference}} on {{Artifical Intelligence}} - {{Volume}} 1},
  author = {Ashley, Kevin D. and Aleven, Vincent},
  date = {1997},
  series = {{{IJCAI}}'97},
  pages = {335--341},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  url = {http://dl.acm.org/citation.cfm?id=1624162.1624212},
  urldate = {2019-08-20},
  abstract = {In teaching case-based argumentation skills, the CATO program, an intelligent learning environment, guides students' assessments of partial matches between problems and cases by generating alternative interpretations of the similarities and differences. CATO's Factor Hierarchy captures information about the significance of similarities and differences given the normative purposes of the domain classification. Its algorithms for emphasizing or downplaying significance tailor interpretations to the comparison context, block interpretations strongly contradicted by other factors and strategically determine how and how abstractly to characterize a difference. An empirical evaluation confirmed CATO's effectiveness in teaching basic argumentation skills.},
  venue = {Nagoya, Japan}
}

@inproceedings{Asowo2025EnsembleModellingFeature,
  title = {An {{Ensemble Modelling}} of~{{Feature Engineering}} and~{{Predictions}} for~{{Enhanced Fake News Detection}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Asowo, Patricia and Lal, Sangeeta and Ani, Uchenna Daniel},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  pages = {225--231},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77918-3_16},
  abstract = {The threat of fake news jeopardizing the credibility of online news platforms, particularly on social media, underscores the need for innovative solutions. This paper proposes a creative engine for detecting fake news, leveraging advanced machine learning techniques, specifically Bidirectional En-coder Representations by Transformers (BERT). Our approach involves feature selection from news content and social contexts, combining predictions from multiple models, including Random Forest, BERT, GRU, LSTM, and a voting ensemble model. Through extensive evaluation of the WELFake dataset, our method highlights an impressive accuracy of 99\%, surpassing baselines and existing systems. Our study highlights the crucial role of hyperparameter tuning, improving the performance of the BERT model to 100\%.},
  isbn = {978-3-031-77918-3},
  langid = {english}
}

@online{AssociationForComputationalLinguistics2019POSTaggingState,
  title = {{{POS Tagging}} ({{State}} of the Art)},
  author = {{Association for Computational Linguistics}},
  date = {2019-03-04},
  url = {https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art)},
  urldate = {2021-02-09}
}

@inproceedings{Atkinson2007ArgumentationStandardsProof,
  title = {Argumentation and Standards of Proof},
  booktitle = {Proceedings of the 11th International Conference on {{Artificial}} Intelligence and Law},
  author = {Atkinson, Katie and Bench-Capon, Trevor},
  date = {2007-06-04},
  series = {{{ICAIL}} '07},
  pages = {107--116},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1276318.1276339},
  url = {https://dl.acm.org/doi/10.1145/1276318.1276339},
  urldate = {2025-05-08},
  abstract = {In this paper we examine some previous AI and Law attempts to characterise standards of proof, and relate these to the notions of acceptability found in argumentation frameworks, an approach which forms the basis of much recent work on argumentation. We distinguish between the justification of facts and the justication of choices relating to the law and its interpretation. Standards of proof most naturally arise in connection with facts, but points of law have analogous degrees of justification.},
  isbn = {978-1-59593-680-6}
}

@inproceedings{Auer2007DBpediaNucleusWeb,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and Cudré-Mauroux, Philippe},
  date = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {722--735},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-76298-0_52},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76298-0},
  langid = {english}
}

@online{Ayoobi2024ProtoArgNetInterpretableImage,
  title = {{{ProtoArgNet}}: {{Interpretable Image Classification}} with {{Super-Prototypes}} and {{Argumentation}} [{{Technical Report}}]},
  shorttitle = {{{ProtoArgNet}}},
  author = {Ayoobi, Hamed and Potyka, Nico and Toni, Francesca},
  date = {2024-08-21},
  eprint = {2311.15438},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.15438},
  url = {http://arxiv.org/abs/2311.15438},
  urldate = {2024-12-19},
  abstract = {We propose ProtoArgNet, a novel interpretable deep neural architecture for image classification in the spirit of prototypical-part-learning as found, e.g., in ProtoPNet. While earlier approaches associate every class with multiple prototypical-parts, ProtoArgNet uses super-prototypes that combine prototypical-parts into a unified class representation. This is done by combining local activations of prototypes in an MLP-like manner, enabling the localization of prototypes and learning (non-linear) spatial relationships among them. By leveraging a form of argumentation, ProtoArgNet is capable of providing both supporting (i.e. `this looks like that') and attacking (i.e. `this differs from that') explanations. We demonstrate on several datasets that ProtoArgNet outperforms state-of-the-art prototypical-part-learning approaches. Moreover, the argumentation component in ProtoArgNet is customisable to the user's cognitive requirements by a process of sparsification, which leads to more compact explanations compared to state-of-the-art approaches.},
  pubstate = {prepublished}
}

@online{Bach2025CaseBasedReasoningMeets,
  title = {Case-{{Based Reasoning Meets Large Language Models}}: {{A Research Manifesto For Open Challenges}} and {{Research Directions}}},
  shorttitle = {Case-{{Based Reasoning Meets Large Language Models}}},
  author = {Bach, Kerstin and Bergmann, Ralph and Brand, Florian and Caro-Martínez, Marta and Eisenstadt, Viktor and W. Floyd, Michael and Jayawardena, Lasal and Leake, David and Lenz, Mirko and Malburg, Lukas and H. Ménager, David and Minor, Mirjam and Schack, Brian and Watson, Ian and Wilkerson, Kaitlynne and Wiratunga, Nirmalie},
  date = {2025-03},
  number = {hal-05006761},
  eprint = {hal-05006761},
  eprinttype = {HAL},
  url = {https://hal.science/hal-05006761},
  abstract = {In recent years, the surge of Generative Artificial Intelligence (GenAI ), particularly Large Language Models (LLMs), has led to a significant increase in the use of hybrid systems, which combine the strengths of different Artificial Intelligence (AI) paradigms to achieve better performance and efficiency. Although LLMs demonstrate remarkable effectiveness across numerous tasks due to their flexibility and general knowledge, they often face challenges related to accuracy, explainability, and their limited memory. Case-Based Reasoning (CBR), on the other hand, excels by recalling past experiences and using them to solve new problems, making it particularly well suited for tasks that require contextual understanding and decision-making. However, CBR systems suffer from issues such as the acquisition of various kinds of knowledge and the application of methods during the 4R cycle. In this paper, we identify several challenges plaguing LLMs and CBR systems and propose opportunities to combine the strengths of both methodologies to address these challenges. In addition, we outline future research directions for the community to explore.},
  pubstate = {prepublished}
}

@online{Baek2024ResearchAgentIterativeResearch,
  title = {{{ResearchAgent}}: {{Iterative Research Idea Generation}} over {{Scientific Literature}} with {{Large Language Models}}},
  shorttitle = {{{ResearchAgent}}},
  author = {Baek, Jinheon and Jauhar, Sujay Kumar and Cucerzan, Silviu and Hwang, Sung Ju},
  date = {2024-04-11},
  eprint = {2404.07738},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.07738},
  url = {http://arxiv.org/abs/2404.07738},
  urldate = {2024-04-16},
  abstract = {Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts. To enhance its productivity, we propose a ResearchAgent, a large language model-powered research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature. Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic graph but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers. In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively. Further, they are instantiated with human preference-aligned large language models whose criteria for evaluation are derived from actual human judgments. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results.},
  pubstate = {prepublished}
}

@online{Balaguer2024RAGVsFinetuning,
  title = {{{RAG}} vs {{Fine-tuning}}: {{Pipelines}}, {{Tradeoffs}}, and a {{Case Study}} on {{Agriculture}}},
  shorttitle = {{{RAG}} vs {{Fine-tuning}}},
  author = {Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz de Freitas and Filho, Roberto de M. Estevão and Hendry, Todd and Holstein, Daniel and Marsman, Jennifer and Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O. and Padilha, Rafael and Sharp, Morris and Silva, Bruno and Sharma, Swati and Aski, Vijay and Chandra, Ranveer},
  date = {2024-01-30},
  eprint = {2401.08406},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.08406},
  url = {http://arxiv.org/abs/2401.08406},
  urldate = {2024-09-03},
  abstract = {There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47\% to 72\%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.},
  pubstate = {prepublished}
}

@article{Balakrishnan2014StemmingLemmatizationComparison,
  title = {Stemming and {{Lemmatization}}: {{A Comparison}} of {{Retrieval Performances}}},
  shorttitle = {Stemming and {{Lemmatization}}},
  author = {Balakrishnan, Vimala and Ethel, Lloyd-Yemoh},
  date = {2014},
  journaltitle = {Lecture Notes on Software Engineering},
  shortjournal = {LNSE},
  volume = {2},
  number = {3},
  pages = {262--267},
  issn = {23013559},
  doi = {10.7763/LNSE.2014.V2.134},
  url = {http://www.lnse.org/show-34-165-1.html},
  urldate = {2021-03-08},
  abstract = {The current study proposes to compare document retrieval precision performances based on language modeling techniques, particularly stemming and lemmatization. Stemming is a procedure to reduce all words with the same stem to a common form whereas lemmatization removes inflectional endings and returns the base or dictionary form of a word. Comparisons were also made between these two techniques with a baseline ranking algorithm (i.e. with no language processing). A search engine was developed and the algorithms were tested based on a test collection. Both mean average precisions and histograms indicate stemming and lemmatization to outperform the baseline algorithm. As for the language modeling techniques, lemmatization produced better precision compared to stemming, however the differences are insignificant. Overall the findings suggest that language modeling techniques improves document retrieval, with lemmatization technique producing the best result.}
}

@inproceedings{Baltes2018SOTorrentReconstructingAnalyzing,
  title = {{{SOTorrent}}: Reconstructing and Analyzing the Evolution of Stack Overflow Posts},
  shorttitle = {{{SOTorrent}}},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Mining Software Repositories}}},
  author = {Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan},
  date = {2018-05-28},
  series = {{{MSR}} '18},
  pages = {319--330},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3196398.3196430},
  url = {https://doi.org/10.1145/3196398.3196430},
  urldate = {2021-03-18},
  abstract = {Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.},
  isbn = {978-1-4503-5716-6}
}

@inproceedings{Banarescu2013AbstractMeaningRepresentation,
  title = {Abstract {{Meaning Representation}} for {{Sembanking}}},
  booktitle = {Proceedings of the 7th {{Linguistic Annotation Workshop}} and {{Interoperability}} with {{Discourse}}},
  author = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  date = {2013-08},
  pages = {178--186},
  publisher = {Association for Computational Linguistics},
  location = {Sofia, Bulgaria},
  url = {https://www.aclweb.org/anthology/W13-2322},
  urldate = {2020-10-16}
}

@inproceedings{Bao2022GenerativeModelEndtoEnd,
  title = {A {{Generative Model}} for {{End-to-End Argument Mining}} with {{Reconstructed Positional Encoding}} and {{Constrained Pointer Mechanism}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bao, Jianzhu and He, Yuhang and Sun, Yang and Liang, Bin and Du, Jiachen and Qin, Bing and Yang, Min and Xu, Ruifeng},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  date = {2022-12},
  pages = {10437--10449},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.713},
  url = {https://aclanthology.org/2022.emnlp-main.713},
  urldate = {2024-02-10},
  abstract = {Argument mining (AM) is a challenging task as it requires recognizing the complex argumentation structures involving multiple subtasks. To handle all subtasks of AM in an end-to-end fashion, previous works generally transform AM into a dependency parsing task. However, such methods largely require complex pre- and post-processing to realize the task transformation. In this paper, we investigate the end-to-end AM task from a novel perspective by proposing a generative framework, in which the expected outputs of AM are framed as a simple target sequence. Then, we employ a pre-trained sequence-to-sequence language model with a constrained pointer mechanism (CPM) to model the clues for all the subtasks of AM in the light of the target sequence. Furthermore, we devise a reconstructed positional encoding (RPE) to alleviate the order biases induced by the autoregressive generation paradigm. Experimental results show that our proposed framework achieves new state-of-the-art performance on two AM benchmarks.},
  eventtitle = {{{EMNLP}} 2022}
}

@software{Bargnesi2014Jsongraphspecification,
  title = {Json-Graph-Specification},
  author = {Bargnesi, Tony and Hayes, William},
  date = {2014-05-04},
  origdate = {2014-05-09T18:15:39Z},
  url = {https://github.com/jsongraph/json-graph-specification},
  urldate = {2022-04-22},
  abstract = {A proposal for representing graph structure (nodes / edges) in JSON.},
  organization = {JSON Graph}
}

@article{Baron1969NoteHistoricalDevelopment,
  title = {A {{Note}} on the {{Historical Development}} of {{Logic Diagrams}}: {{Leibniz}}, {{Euler}} and {{Venn}}},
  shorttitle = {A {{Note}} on the {{Historical Development}} of {{Logic Diagrams}}},
  author = {Baron, Margaret E.},
  date = {1969-05},
  journaltitle = {The Mathematical Gazette},
  volume = {53},
  number = {384},
  pages = {113--125},
  issn = {0025-5572, 2056-6328},
  doi = {10.2307/3614533},
  url = {https://www.cambridge.org/core/journals/mathematical-gazette/article/abs/note-on-the-historical-development-of-logic-diagrams-leibniz-euler-and-venn/A74F019FAD2F50603BD8D009572C7C92},
  urldate = {2025-02-13},
  abstract = {Lessons on sets have become commonplace in schools today. Venn diagrams proliferate and, even in primary schools, children can be seen sorting and classifying objects by size, colour and shape and placing them in spaces marked out on the floor by chalk outlines or wooden hoops. Older children learn that such diagrams are named after the English logician, John Venn, and that through them we can represent the relations of membership and inclusion and the operations of union, intersection and complementation. A rectangle is drawn to represent the universe U: subsets of U are represented by the interiors of circles, or other closed curves within U, i.e. subspaces of the rectangle. The elements of U are represented by points within the rectangle, the elements of a subset A by points within the corresponding subspace of the rectangle and the elements of A′ by points within the rectangle but outside the region representing A.},
  langid = {english}
}

@article{Baroni2011IntroductionArgumentationSemantics,
  title = {An Introduction to {{Argumentation Semantics}}},
  author = {Baroni, Pietro and Caminada, Martin and Giacomin, Massimiliano},
  date = {2011-12},
  journaltitle = {The Knowledge Engineering Review},
  volume = {26},
  number = {4},
  pages = {365--410},
  issn = {1469-8005, 0269-8889},
  doi = {10.1017/S0269888911000166},
  url = {https://www.cambridge.org/core/journals/knowledge-engineering-review/article/an-introduction-to-argumentation-semantics/B168A719570B2DBB29CAB4EF1451CD0F?utm_source=chatgpt.com},
  urldate = {2025-09-09},
  abstract = {This paper presents an overview on the state of the art of semantics for abstractargumentation, covering both some of the most influential literature proposalsand some general issues concerning semantics definition and evaluation. As tothe former point, the paper reviews Dung's original notions ofcomplete, grounded, preferred, and stable semantics, as well as subsequentlyproposed notions like semi-stable, ideal, stage, and CF2 semantics, consideringboth the extension-based and the labelling-based approaches with respect totheir definitions. As to the latter point, the paper presents an extensive setof general properties for semantics evaluation and analyzes the notions ofargument justification and skepticism. The final part of the paper is focused onthe discussion of some relationships between semantics properties anddomain-specific requirements.},
  langid = {english}
}

@inproceedings{Baroni2014DonCountPredict,
  title = {Don't Count, Predict! {{A}} Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
  date = {2014-06},
  pages = {238--247},
  publisher = {Association for Computational Linguistics},
  location = {Baltimore, Maryland},
  doi = {10.3115/v1/P14-1023},
  url = {https://www.aclweb.org/anthology/P14-1023},
  urldate = {2021-02-07},
  eventtitle = {{{ACL}} 2014}
}

@book{Baroni2018HandbookFormalArgumentation,
  title = {Handbook of {{Formal Argumentation}}},
  editor = {Baroni, Pietro and Gabbay, Dov M. and Giacomin, Massimiliano and family=Torre, given=Leendert, prefix=van der, useprefix=false},
  date = {2018},
  volume = {1},
  publisher = {College Publications},
  isbn = {978-1-84890-275-6},
  langid = {english},
  pagetotal = {1016}
}

@thesis{Bartz2024VisionBasedSimilarityComputation,
  type = {bathesis},
  title = {Vision-{{Based Similarity Computation}} for {{Case-Based Retrieval}} of {{Argument Graphs}}},
  author = {Bartz, Kilian},
  date = {2024-04-30},
  institution = {Trier University},
  location = {Trier, Germany},
  abstract = {Over the last few years, there has been a surge of interest in Artificial Intelligence (AI) vision models, however, using them for automatic processing of drawn natural visual representations like graphs has not yet been evaluated. This could enable many new applications, especially the implementation of a fast and scalable retrieval for argumentation machines. In this thesis, I propose a new approach to implement structural retrieval in the context of a case-based argumentation machine using a Swin Transformer v2 model to trans- form visualized Argument Graphs (AGs) into dense embeddings on which similarities can be calculated very efficiently. In an experimental, iterative approach, I conceptualize multiple suitable visualization designs based on node-link graph drawings and treemaps, which aim to capture an AG’s structure. These visualizations are used to train corresponding vision models for the embedding process. I demonstrate that even though treemap-based designs show more promising training behavior, more comprehensive node-link drawings exhibit better retrieval performance for complex queries. The A* search from previous works outperforms vision-based argument retrieval for simple queries; however, a higher query complexity noticeably increases the quality of vision-based argument retrieval. Furthermore, vision-based similarity computation can improve computation times by several orders of magnitude and also exhibits promising scaling of retrieval quality with larger training data sets and longer training durations. However, due to the complex and opaque information processing of Transformer models, the structural similarity values produced by the vision-based approach can be unpredictable and counterintuitive. Additionally, vision-based argument retrieval does not represent a complete replacement for A* search as it does not provide mappings from the query to the case base arguments.},
  langid = {english}
}

@inproceedings{Bartz2025RetrievingArgumentGraphs,
  title = {Retrieving {{Argument Graphs Using Vision Transformers}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Bartz, Kilian and Lenz, Mirko and Bergmann, Ralph},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {32--45},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.argmining-1.4},
  url = {https://aclanthology.org/2025.argmining-1.4/},
  abstract = {Through manual annotation or automated argument mining processes, arguments can be represented not only as text, but also in structured formats like graphs. When searching for relevant arguments, this additional information about the relationship between their elementary units allows for the formulation of fine-grained structural constraints by using graphs as queries. Then, a retrieval can be performed by computing the similarity between the query and all available arguments. Previous works employed Graph Edit Distance (GED) algorithms such as A* search to compute mappings between nodes and edges for determining the similarity, which is rather expensive. In this paper, we propose an alternative based on Vision Transformers where arguments are rendered as images to obtain dense embeddings. We propose multiple space-filling visualizations and evaluate the retrieval performance of the vision-based approach against an existing A* search-based method. We find that our technique runs orders of magnitude faster than A* search and scales well on larger argument graphs while achieving competitive results.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Bassani2022RanxBlazingFastPython,
  title = {Ranx: {{A Blazing-Fast Python Library}} for~{{Ranking Evaluation}} and~{{Comparison}}},
  shorttitle = {Ranx},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Bassani, Elias},
  editor = {Hagen, Matthias and Verberne, Suzan and Macdonald, Craig and Seifert, Christin and Balog, Krisztian and Nørvåg, Kjetil and Setty, Vinay},
  date = {2022},
  pages = {259--264},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-99739-7_30},
  abstract = {This paper presents ranx, a Python evaluation library for Information Retrieval built on top of Numba. ranx provides a user-friendly interface to the most common ranking evaluation metrics, such as MAP, MRR, and NDCG. Moreover, it offers a convenient way of managing the evaluation results, comparing different runs, performing statistical tests between them, and exporting LaTeX~tables ready to be used in scientific publications, all in a few lines of code. The efficiency brought by Numba, a just-in-time compiler for Python code, makes the adoption ranx convenient even for industrial applications.},
  isbn = {978-3-030-99739-7},
  langid = {english}
}

@inproceedings{Bassi2025OldGoldLLMBased,
  title = {Old but {{Gold}}: {{LLM-Based Features}} and {{Shallow Learning Methods}} for {{Fine-Grained Controversy Analysis}} in {{YouTube Comments}}},
  shorttitle = {Old but {{Gold}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Bassi, Davide and Marino, Erik Bran and Vieira, Renata and Pereira, Martin},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {46--57},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.5/},
  urldate = {2025-07-28},
  abstract = {Online discussions can either bridge differences through constructive dialogue or amplify divisions through destructive interactions. paper proposes a computational approach to analyze dialogical relation patterns in YouTube comments, offering a fine-grained framework for controversy detection, enabling also analysis of individual contributions. experiments demonstrate that shallow learning methods, when equipped with these theoretically-grounded features, consistently outperform more complex language models in characterizing discourse quality at both comment-pair and conversation-chain levels.studies confirm that divisive rhetorical techniques serve as strong predictors of destructive communication patterns. work advances understanding of how communicative choices shape online discourse, moving beyond engagement metrics toward nuanced examination of constructive versus destructive dialogue patterns.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Bastardo-Rojas2025VisualQuestionAnswering,
  title = {Visual {{Question Answering}} to~{{Generate Case-Based Explanations}} for~{{Image Classification}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Bastardo-Rojas, Ángel and Caro-Martínez, Marta},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15662},
  pages = {37--51},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_3},
  abstract = {To clarify how artificial intelligence models for image classification obtain their predictions, instance-based explanations give examples of other predictions obtained by those models for similar images. XAI (eXplainable Artificial Intelligence) techniques find those examples by getting images similar to the query using similarity metrics that consider image features. In this work, we introduce a Case-Based Reasoning approach that obtains these types of explanations (explanations by examples and counterfactuals) using Visual Question Answering techniques that take advantage of the high performance achieved by Large Language Models. We have evaluated our approach through an offline evaluation, in a context where images to classify and explain may be low quality. We obtained promising results and confirmed the benefits of using multimodal models in XAI, combining knowledge focused on images and natural language.},
  eventtitle = {{{ICCBR}} 2025},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@inproceedings{Batra2025PredictingImplicitArguments,
  title = {Predicting {{Implicit Arguments}} in {{Procedural Video Instructions}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Batra, Anil and Sevilla-Lara, Laura and Rohrbach, Marcus and Keller, Frank},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {30399--30419},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.1467/},
  urldate = {2025-07-29},
  abstract = {Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like verb,what,where/with. Procedural instructions are highly elliptic, for instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second step's where argument is inferred from the context, referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models' contextual reasoning, requiring entity tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of what and where/with from multi-modal procedural data given the verb. Lastly, we propose iSRL-Qwen2-VL, which achieves a 17\% relative improvement in F1-score for what-implicit and a 14.7\% for where/with-implicit semantic roles over GPT-4o.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Bavaresco2025LLMsInsteadHuman,
  title = {{{LLMs}} Instead of {{Human Judges}}? {{A Large Scale Empirical Study}} across 20 {{NLP Evaluation Tasks}}},
  shorttitle = {{{LLMs}} Instead of {{Human Judges}}?},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Bavaresco, Anna and Bernardi, Raffaella and Bertolazzi, Leonardo and Elliott, Desmond and Fernández, Raquel and Gatt, Albert and Ghaleb, Esam and Giulianelli, Mario and Hanna, Michael and Koller, Alexander and Martins, Andre and Mondorf, Philipp and Neplenbroek, Vera and Pezzelle, Sandro and Plank, Barbara and Schlangen, David and Suglia, Alessandro and Surikuchi, Aditya K and Takmaz, Ece and Testoni, Alberto},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {238--255},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-short.20/},
  urldate = {2025-07-29},
  abstract = {There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-252-7}
}

@inproceedings{Baydin2011CBRCommonsenseReasoning,
  title = {{{CBR}} with {{Commonsense Reasoning}} and {{Structure Mapping}}: {{An Application}} to {{Mediation}}},
  shorttitle = {{{CBR}} with {{Commonsense Reasoning}} and {{Structure Mapping}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Baydin, Atılım Güneş and López de Mántaras, Ramon and Simoff, Simeon and Sierra, Carles},
  editor = {Ram, Ashwin and Wiratunga, Nirmalie},
  date = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {378--392},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23291-6_28},
  abstract = {Mediation is an important method in dispute resolution. We implement a case based reasoning approach to mediation integrating analogical and commonsense reasoning components that allow an artificial mediation agent to satisfy requirements expected from a human mediator, in particular: utilizing experience with cases in different domains; and structurally transforming the set of issues for a better solution. We utilize a case structure based on ontologies reflecting the perceptions of the parties in dispute. The analogical reasoning component, employing the Structure Mapping Theory from psychology, provides a flexibility to respond innovatively in unusual circumstances, in contrast with conventional approaches confined into specialized problem domains. We aim to build a mediation case base incorporating real world instances ranging from interpersonal or intergroup disputes to international conflicts.},
  isbn = {978-3-642-23291-6},
  langid = {english}
}

@thesis{Becker2017EngageGoogleData,
  type = {Study Project Report},
  title = {Engage {{Google Data}} with {{BI}}},
  author = {Becker, Marco and Hunecke, Kira and Lenz, Mirko and Reufels, Rene and Sahitaj, Premtim},
  date = {2017-10-16},
  institution = {Trier University},
  location = {Trier, Germany},
  pagetotal = {123}
}

@inproceedings{Becker2017EnrichingArgumentativeTexts,
  title = {Enriching {{Argumentative Texts}} with {{Implicit Knowledge}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Becker, Maria and Staniek, Michael and Nastase, Vivi and Frank, Anette},
  editor = {Frasincar, Flavius and Ittoo, Ashwin and Nguyen, Le Minh and Métais, Elisabeth},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {84--96},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-59569-6_9},
  abstract = {Retrieving information that is implicit in a text is difficult. For argument analysis, revealing implied knowledge could be useful to judge how solid an argument is and to construct concise arguments. We design a process for obtaining high-quality implied knowledge annotations for German argumentative microtexts, in the form of simple natural language statements. This process involves several steps to promote agreement and monitors its evolution using textual similarity computation. To further characterize the implied knowledge, we annotate the added sentences with semantic clause types and common sense knowledge relations. To test whether the knowledge could be retrieved automatically, we compare the inserted sentences to Wikipedia articles on similar topics. Analysis of the added knowledge shows that (i) it is characterized by a high proportion of generic sentences, (ii) a majority of it can be mapped to common sense knowledge relations, and (iii) it is similar to sentences found in Wikipedia.},
  isbn = {978-3-319-59569-6},
  langid = {english}
}

@inproceedings{Becker2019AssessingDifficultyClassifying,
  title = {Assessing the {{Difficulty}} of {{Classifying ConceptNet Relations}} in a {{Multi-Label Classification Setting}}},
  booktitle = {{{RELATIONS}} - {{Workshop}} on Meaning Relations between Phrases and Sentences},
  author = {Becker, Maria and Staniek, Michael and Nastase, Vivi and Frank, Anette},
  date = {2019-05},
  publisher = {Association for Computational Linguistics},
  location = {Gothenburg, Sweden},
  doi = {10.18653/v1/W19-0801},
  url = {https://www.aclweb.org/anthology/W19-0801},
  urldate = {2020-09-09},
  abstract = {Commonsense knowledge relations are crucial for advanced NLU tasks. We examine the learnability of such relations as represented in ConceptNet, taking into account their specific properties, which can make relation classification difficult: a given concept pair can be linked by multiple relation types, and relations can have multi-word arguments of diverse semantic types. We explore a neural open world multi-label classification approach that focuses on the evaluation of classification accuracy for individual relations. Based on an in-depth study of the specific properties of the ConceptNet resource, we investigate the impact of different relation representations and model variations. Our analysis reveals that the complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work.}
}

@article{Becker2020ExplainingArgumentsBackground,
  title = {Explaining {{Arguments}} with {{Background Knowledge}}},
  author = {Becker, Maria and Hulpuş, Ioana and Opitz, Juri and Paul, Debjit and Kobbe, Jonathan and Stuckenschmidt, Heiner and Frank, Anette},
  date = {2020-07-01},
  journaltitle = {Datenbank-Spektrum},
  shortjournal = {Datenbank Spektrum},
  volume = {20},
  number = {2},
  pages = {131--141},
  issn = {1610-1995},
  doi = {10.1007/s13222-020-00348-6},
  url = {https://doi.org/10.1007/s13222-020-00348-6},
  urldate = {2020-09-09},
  abstract = {Most information we consume as a society is obtained over the Web. News – often from questionable sources – are spread online, as are election campaigns; calls for (collective) action spread with unforeseen speed and intensity. All such actions have argumentation at their core, and the conveyed content is often strategically selected or rhetorically framed. The responsibility of critical analysis of arguments is thus tacitly transferred to the content consumer who is often not prepared for the task, nor aware of the responsibility. The ExpLAIN project aims at making the structure and reasoning of arguments explicit – not only for humans, but for Robust Argumentation Machines that are endowed with language understanding capacity. Our vision is a system that is able to deeply analyze argumentative text: that identifies arguments and counter-arguments, and reveals their internal structure, conveyed content and reasoning. A particular challenge for such a system is to uncover implicit knowledge which many arguments rely on. This requires human background knowledge and reasoning capacity, in order to explicate the complete reasoning of an argument. This article presents ongoing research of the ExpLAIN project that aims to make the vision of such a system a tangible aim. We introduce the problems and challenges we need to address, and present the progress we achieved until now by applying advanced natural language and knowledge processing methods. Our approach puts particular focus on leveraging available sources of structured and unstructured background knowledge, the automatic extension of such knowledge, the uncovering of implicit content, and reasoning techniques suitable for informal, everyday argumentation.},
  langid = {english}
}

@thesis{Becker2023InstructiveArgumentMining,
  title = {Instructive {{Argument Mining}} with {{Large Language Models}}},
  author = {Becker, Michael and Nanyan, Karine},
  date = {2023-09-09},
  institution = {Trier University},
  location = {Trier, Germany},
  abstract = {Understanding the argumentative structure of texts is important for decision-making, problem-solving, and knowledge exchange across various fields. While manual argument extraction has been the traditional approach, computers have enabled argument mining: the automatic extraction of arguments. In our research, we focus on solving argument mining tasks using Large Language Models (LLMs) which are artificial intelligence algorithms trained on large amounts of data and can produce human-like responses to natural language queries. Specifically, we conduct a comparative analysis of state-of-the-art closed-source LLMs, OpenAI’s GPT-3.5 and GPT-4 and an open-source LLM, Meta’s Llama 2, which is fine-tuned on the task of argument mining, i.e., trained on specific argumentative datasets to enhance its performance in this task. Our evaluation involves the comparison of the performance of these LLMs against each other and against an existing end-to-end argument mining pipeline. Our findings reveal that LLMs perform impressively well and outperform the argument mining pipeline. This suggests that they can play a major role in advancing accurate argument mining and understanding of complex argumentative structures.},
  langid = {english}
}

@incollection{Beckwith1998DesignImplementationWordNet,
  title = {Design and {{Implementation}} of the {{WordNet Lexical Database}} and {{Searching Software}}},
  booktitle = {{{WordNet}}},
  author = {Beckwith, Richard and Miller, George A. and Tengi, Randee},
  editor = {Fellbaum, Christiane},
  date = {1998},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/7287.003.0009},
  url = {https://direct.mit.edu/books/book/1928/chapter/52969/design-and-implementation-of-the-wordnet-lexical},
  urldate = {2021-02-12},
  isbn = {978-0-262-27255-1},
  langid = {english}
}

@inproceedings{Behrendt2021ArgueBERTHowImprove,
  title = {{{ArgueBERT}}: {{How To Improve BERT Embeddings}} for {{Measuring}} the {{Similarity}} of {{Arguments}}},
  shorttitle = {{{ArgueBERT}}},
  booktitle = {Proceedings of the 17th {{Conference}} on {{Natural Language Processing}} ({{KONVENS}} 2021)},
  author = {Behrendt, Maike and Harmeling, Stefan},
  editor = {Evang, Kilian and Kallmeyer, Laura and Osswald, Rainer and Waszczuk, Jakub and Zesch, Torsten},
  date = {2021},
  pages = {28--36},
  publisher = {KONVENS 2021 Organizers},
  location = {Düsseldorf, Germany},
  url = {https://aclanthology.org/2021.konvens-1.3/},
  urldate = {2025-05-31},
  eventtitle = {{{KONVENS}} 2021}
}

@article{Bench-Capon2003PersuasionPracticalArgument,
  title = {Persuasion in {{Practical Argument Using Value-based Argumentation Frameworks}}},
  author = {Bench-Capon, Trevor J. M.},
  date = {2003-06-01},
  journaltitle = {Journal of Logic and Computation},
  shortjournal = {J Logic Computation},
  volume = {13},
  number = {3},
  pages = {429--448},
  issn = {0955-792X},
  doi = {10.1093/logcom/13.3.429},
  url = {https://doi.org/10.1093/logcom/13.3.429},
  urldate = {2025-09-09},
  abstract = {In many cases of disagreement, particularly in situations involving practical reasoning, it is impossible to demonstrate conclusively that either party is wrong. The role of argument in such cases is to persuade rather than to prove, demonstrate or refute. Following Perelman, we argue that persuasion in such cases relies on a recognition that the strength of an argument depends on the social values that it advances, and that whether the attack of one argument on another succeeds depends on the comparative strength of the values advanced by the arguments concerned. To model this we extend the standard notion of Argumentation Frameworks (AFs) to Value-based Argumentation Frameworks (VAFs). After defining VAFs we explore their properties, and show how they can provide a rational basis for the acceptance or rejection of arguments, even where this would appear to be a matter of choice in a standard AF. In particular we show that in a VAF certain arguments can be shown to be acceptable however the relative strengths of the values involved are assessed. This means that disputants can concur on the acceptance of arguments, even when they differ as to which values are more important, and hence that we can identify points for which persuasion should be possible. We illustrate the above using an example moral debate. We then show how factual considerations can be admitted to our framework and discuss the possibility of persuasion in the face of uncertainty and disagreement as to values.}
}

@article{Bench-Capon2007ArgumentationArtificialIntelligence,
  title = {Argumentation in {{Artificial Intelligence}}},
  author = {Bench-Capon, T. J. M. and Dunne, Paul E.},
  date = {2007-07-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  series = {Argumentation in {{Artificial Intelligence}}},
  volume = {171},
  number = {10},
  pages = {619--641},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2007.05.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370207000793},
  urldate = {2025-09-09},
  abstract = {Over the last ten years, argumentation has come to be increasingly central as a core study within Artificial Intelligence (AI). The articles forming this volume reflect a variety of important trends, developments, and applications covering a range of current topics relating to the theory and applications of argumentation. Our aims in this introduction are, firstly, to place these contributions in the context of the historical foundations of argumentation in AI and, subsequently, to discuss a number of themes that have emerged in recent years resulting in a significant broadening of the areas in which argumentation based methods are used. We begin by presenting a brief overview of the issues of interest within the classical study of argumentation: in particular, its relationship—in terms of both similarities and important differences—to traditional concepts of logical reasoning and mathematical proof. We continue by outlining how a number of foundational contributions provided the basis for the formulation of argumentation models and their promotion in AI related settings and then consider a number of new themes that have emerged in recent years, many of which provide the principal topics of the research presented in this volume.}
}

@article{Bench-Capon2020DungArgumentationAI,
  title = {Before and after {{Dung}}: {{Argumentation}} in {{AI}} and {{Law}}},
  shorttitle = {Before and after {{Dung}}},
  author = {Bench-Capon, T. J. M.},
  date = {2020-01-01},
  journaltitle = {Argument \& Computation},
  volume = {11},
  number = {1--2},
  pages = {221--238},
  publisher = {IOS Press},
  issn = {1946-2166},
  doi = {10.3233/AAC-190477},
  url = {https://content.iospress.com/articles/argument-and-computation/aac190477},
  urldate = {2021-01-17},
  abstract = {Dung’s abstract argumentation frameworks have had a very significant role in the rise in interest in argumentation throughout this century. In this paper we will explore the impact of this seminal idea on a specific application domain, AI and Law. Ar},
  langid = {english}
}

@article{Benett1954CommunicationsLimitedResponseQuestioning,
  title = {Communications {{Through Limited-Response Questioning}}*},
  author = {Benett, E. M. and Alpert, R. and Goldstein, A. C.},
  date = {1954-01-01},
  journaltitle = {Public Opinion Quarterly},
  shortjournal = {Public Opinion Quarterly},
  volume = {18},
  number = {3},
  pages = {303--308},
  issn = {0033-362X},
  doi = {10.1086/266520},
  url = {https://doi.org/10.1086/266520},
  urldate = {2021-03-07},
  abstract = {The extent of consistency between information from two methods of communication, the interview and the limited-response question, was investigated. Thirty questions showed consistencies greater than could be expected on the basis of chance. The questions were classified into four general categories, and the mean coefficients of consistency for these categories ranged from 0.46 to 1.00.}
}

@article{Bengio2003NeuralProbabilisticLanguage,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and {View Profile} and Ducharme, Réjean and {View Profile} and Vincent, Pascal and {View Profile} and Janvin, Christian and {View Profile}},
  date = {2003-03},
  journaltitle = {The Journal of Machine Learning Research},
  volume = {3},
  pages = {1137--1155},
  publisher = {JMLR.org},
  doi = {10.5555/944919.944966},
  url = {https://dl.acm.org/doi/10.5555/944919.944966},
  urldate = {2025-09-10},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.}
}

@article{Bentahar2010TaxonomyArgumentationModels,
  title = {A Taxonomy of Argumentation Models Used for Knowledge Representation},
  author = {Bentahar, Jamal and Moulin, Bernard and Bélanger, Micheline},
  date = {2010-03-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {33},
  number = {3},
  pages = {211--259},
  issn = {1573-7462},
  doi = {10.1007/s10462-010-9154-1},
  url = {https://doi.org/10.1007/s10462-010-9154-1},
  urldate = {2025-09-07},
  abstract = {Understanding argumentation and its role in human reasoning has been a continuous subject of investigation for scholars from the ancient Greek philosophers to current researchers in philosophy, logic and artificial intelligence. In recent years, argumentation models have been used in different areas such as knowledge representation, explanation, proof elaboration, commonsense reasoning, logic programming, legal reasoning, decision making, and negotiation. However, these models address quite specific needs and there is need for a conceptual framework that would organize and compare existing argumentation-based models and methods. Such a framework would be very useful especially for researchers and practitioners who want to select appropriate argumentation models or techniques to be incorporated in new software systems with argumentation capabilities. In this paper, we propose such a conceptual framework, based on taxonomy of the most important argumentation models, approaches and systems found in the literature. This framework highlights the similarities and differences between these argumentation models. As an illustration of the practical use of this framework, we present a case study which shows how we used this framework to select and enrich an argumentation model in a knowledge acquisition project which aimed at representing argumentative knowledge contained in texts critiquing military courses of action.},
  langid = {english}
}

@book{Bergmann2002ExperienceManagement,
  title = {Experience {{Management}}},
  editor = {Bergmann, Ralph},
  editora = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Carbonell, Jaime G. and Siekmann, Jörg},
  editoratype = {redactor},
  date = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {2432},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45759-3},
  url = {http://link.springer.com/10.1007/3-540-45759-3},
  urldate = {2024-04-01},
  isbn = {978-3-540-44191-5 978-3-540-45759-6}
}

@article{Bergmann2009CaseBasedReasoningIntroduction,
  title = {Case-{{Based Reasoning}} - {{Introduction}} and {{Recent Developments}}},
  author = {Bergmann, Ralph and Minor, Mirjam and Althoff, Klaus-Dieter and Reichle, Meike and Bach, Kerstin},
  date = {2009-01-01},
  journaltitle = {KI - Künstliche Intelligenz, German Journal on Artificial Intelligence - Organ des Fachbereiches Künstliche Intelligenz der Gesellschaft für Informatik e.V. KI},
  shortjournal = {KI - Künstliche Intelligenz, German Journal on Artificial Intelligence},
  volume = {1/2009},
  pages = {5--11},
  url = {http://www.wi2.uni-trier.de/publications/2009_KI_CBR.pdf},
  urldate = {2018-09-02},
  abstract = {Case-based reasoning (CBR) is a sub-field of Artificial Intelligence that deals with experience-based problem solving. CBR has its roots in different disciplines such as cognitive science, machine learning, and knowledge-based systems. Today, it is a well established research field of its own, which produced a rich variety of specific methods, as well as applications implementing those methods for particular tasks and domains. This paper gives a compact overview of CBR in general and further discusses recent advancements in selected topics.},
  issue = {Case-Based Reasoning}
}

@article{Bergmann2014SimilarityAssessmentEfficient,
  title = {Similarity Assessment and Efficient Retrieval of Semantic Workflows},
  author = {Bergmann, Ralph and Gil, Yolanda},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {115--127},
  issn = {0306-4379},
  doi = {10.1016/j.is.2012.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437912001020},
  urldate = {2025-05-13},
  abstract = {In the recent years, the use of workflows has significantly expanded from its original domain of business processes towards new areas. The increasing demand for individual and more flexible workflows asks for new methods that support domain experts to create, monitor, and adapt workflows. The emergent field of process-oriented case-based reasoning addresses this problem by proposing methods for reasoning with workflows based on experience. New workflows can be constructed by reuse of already available similar workflows from a repository. Hence, methods for the similarity assessment of workflows and for the efficient retrieval of similar workflows from a repository are of core importance. To this end, we describe a new generic model for representing workflows as semantically labeled graphs, together with a related model for knowledge intensive similarity measures. Further, new algorithms for workflow similarity computation, based on A⁎ search are described. A new retrieval algorithm is introduced that goes beyond traditional sequential retrieval for graphs, interweaving similarity computation with case selection. We describe the application of this model and several experimental evaluations of the algorithms in the domain of scientific workflows and in the domain of business workflows, thereby showing its broad applicability.}
}

@inproceedings{Bergmann2018ReCAPInformationRetrieval,
  title = {{{ReCAP}} - {{Information Retrieval}} and {{Case-Based Reasoning}} for {{Robust Deliberation}} and {{Synthesis}} of {{Arguments}} in the {{Political Discourse}}},
  booktitle = {Proceedings of the {{Conference}} "{{Lernen}}, {{Wissen}}, {{Daten}}, {{Analysen}}"},
  author = {Bergmann, Ralph and Schenkel, Ralf and Dumani, Lorik and Ollinger, Stefan},
  editor = {Gemulla, Rainer and Ponzetto, Simone Paolo and Bizer, Christian and Keuper, Margret and Stuckenschmidt, Heiner},
  date = {2018-08-22},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2191},
  pages = {49--60},
  publisher = {CEUR},
  location = {Mannheim, Germany},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-2191/#paper6},
  urldate = {2024-12-11},
  eventtitle = {Lernen, {{Wissen}}, {{Daten}}, {{Analysen}} 2018},
  langid = {english}
}

@incollection{Bergmann2018SimilarityBasedRetrievalAutomatic,
  title = {Similarity-{{Based Retrieval}} and {{Automatic Adaptation}} of {{Semantic Workflows}}},
  booktitle = {Synergies {{Between Knowledge Engineering}} and {{Software Engineering}}},
  author = {Bergmann, Ralph and Müller, Gilbert},
  editor = {Nalepa, Grzegorz J. and Baumeister, Joachim},
  date = {2018},
  volume = {626},
  pages = {31--54},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-64161-4_2},
  url = {http://link.springer.com/10.1007/978-3-319-64161-4_2},
  urldate = {2019-08-20},
  abstract = {The increasing demand for individual and more flexible process models and workflows asks for new intelligent process-oriented information systems. Such systems should, among other things, support domain experts in the creation and adaptation of process models or workflows. For this purpose, repositories of best practice workflows are an important means as they collect valuable experiential knowledge that can be reused in various ways. In this chapter we present process-oriented case-based reasoning (POCBR) as a method to support the creation and adaptation of workflows based on such knowledge. We provide a general introduction to process-oriented case-based reasoning and present a concise view of the POCBR methods we developed during the past ten years. This includes graph-based representation of semantic workflows, semantic workflow similarity, similarity-based retrieval, and workflow adaptation based on automatically learned adaptation knowledge. Finally, we sketch several application domains such as traditional business processes, social workflows, and cooking workflows.},
  isbn = {978-3-319-64160-7 978-3-319-64161-4},
  langid = {english}
}

@inproceedings{Bergmann2019ProCAKEProcessOrientedCaseBased,
  title = {{{ProCAKE}}: {{A Process-Oriented Case-Based Reasoning Framework}}},
  shorttitle = {{{ProCAKE}}},
  booktitle = {Workshops {{Proceedings}} for the {{Twenty-seventh International Conference}} on {{Case-Based Reasoning}}},
  author = {Bergmann, Ralph and Grumbach, Lisa and Malburg, Lukas and Zeyen, Christian},
  editor = {Kapetanakis, Stelios and Borck, Hayley},
  date = {2019-09-08},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2567},
  pages = {156--161},
  publisher = {CEUR},
  location = {Otzenhausen, Germany},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-2567/#paper14},
  urldate = {2024-04-01},
  eventtitle = {{{ICCBR}} 2019 {{Workshop Proceedings}}},
  langid = {english}
}

@inproceedings{Bergmann2019SimilarityMeasuresCaseBased,
  title = {Similarity {{Measures}} for {{Case-Based Retrieval}} of {{Natural Language Argument Graphs}} in {{Argumentation Machines}}},
  booktitle = {Proceedings of the {{Thirty-Second International Florida Artificial Intelligence Research Society Conference}}},
  author = {Bergmann, Ralph and Lenz, Mirko and Ollinger, Stefan and Pfister, Maximilian},
  editor = {Barták, Roman and Brawner, Keith W.},
  date = {2019-05-19},
  pages = {329--334},
  publisher = {AAAI Press},
  location = {Sarasota, Florida, USA},
  url = {https://aaai.org/ocs/index.php/FLAIRS/FLAIRS19/paper/view/18189},
  abstract = {In the field of argumentation, the vision of robust argumentation machines is investigated. They explore natural language arguments from information sources on the web and reason with them on the knowledge level to actively support the deliberation and synthesis of arguments for a particular user query. We aim at combining methods from case-based reasoning (CBR), information retrieval, and computational argumentation to contribute to the foundations of argumentation machines. In this paper, we focus on the retrieval phase of a CBR approach for an argumentation machine and propose similarity measures for arguments represented as argument graphs. We evaluate the similarity measures on a corpus of annotated micro texts and demonstrate the benefit of semantic similarity measures and the relevance of structural aspects.},
  eventtitle = {Thirty-{{Second International Florida Artificial Intelligence Research Society Conference}}}
}

@article{Bergmann2020ReCAPProject,
  title = {The {{ReCAP Project}}},
  author = {Bergmann, Ralph and Biertz, Manuel and Dumani, Lorik and Lenz, Mirko and Ludwig, Anna-Katharina and Neumann, Patrick J. and Ollinger, Stefan and Sahitaj, Premtim and Schenkel, Ralf and Witry, Alex},
  date = {2020-06-09},
  journaltitle = {Datenbank-Spektrum},
  shortjournal = {Datenbank Spektrum},
  volume = {20},
  pages = {93--98},
  issn = {1610-1995},
  doi = {10.1007/s13222-020-00340-0},
  url = {https://doi.org/10.1007/s13222-020-00340-0},
  abstract = {Argumentation Machines search for arguments in natural language from information sources on the Web and reason with them on the knowledge level to actively support the deliberation and synthesis of arguments for a~particular user query. The recap project is part of the Priority Program ratio and aims at novel contributions to and confluence of methods from information retrieval, knowledge representation, as well as case-based reasoning for the development of future argumentation machines. In this paper we summarise recent research results from the project. In particular, a~new German corpus of 100 semantically annotated argument graphs from the domain of education politics has been created and is made available to the argumentation research community. Further, we discuss a~comprehensive investigation in finding arguments and argument graphs. We introduce a~probabilistic ranking framework for argument retrieval, i.e. for finding good premises for a~designated claim. For finding argument graphs, we developed methods for case-based argument retrieval considering the graph structure of an argument together with textual and ontology-based similarity measures applied to claims, premises, and argument schemes.},
  langid = {english}
}

@inproceedings{Bergmann2025EXARUnifiedExperienceGrounded,
  title = {{{EXAR}}: {{A Unified Experience-Grounded Agentic Reasoning Architecture}}},
  shorttitle = {{{EXAR}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Bergmann, Ralph and Brand, Florian and Lenz, Mirko and Malburg, Lukas},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15662},
  pages = {3--17},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_1},
  abstract = {Current AI reasoning often relies on static pipelines (like the 4R cycle from Case-Based Reasoning (CBR) or standard Retrieval-Augmented Generation (RAG)) that limit adaptability. We argue it is time for a shift towards dynamic, experience-grounded agentic reasoning. This paper proposes EXAR, a new unified, experience-grounded architecture, conceptualizing reasoning not as a fixed sequence, but as a collaborative process orchestrated among specialized agents. EXAR integrates data and knowledge sources into a persistent Long-Term Memory utilized by diverse reasoning agents, which coordinate themselves via a Short-Term Memory. Governed by an Orchestrator and Meta Learner, EXAR enables flexible, context-aware reasoning strategies that adapt and improve over time, offering a blueprint for next-generation AI.},
  eventtitle = {{{ICCBR}} 2025},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Besnard2014ConstructingArgumentGraphs,
  title = {Constructing Argument Graphs with Deductive Arguments: A Tutorial},
  shorttitle = {Constructing Argument Graphs with Deductive Arguments},
  author = {Besnard, Philippe and Hunter, Anthony},
  date = {2014-01-01},
  journaltitle = {Argument \& Computation},
  volume = {5},
  number = {1},
  pages = {5--30},
  publisher = {IOS Press},
  issn = {1946-2166},
  doi = {10.1080/19462166.2013.869765},
  url = {https://content.iospress.com/articles/argument-and-computation/869765},
  urldate = {2020-04-27},
  abstract = {A deductive argument is a pair where the first item is a set of premises, the second item is a claim, and the premises entail the claim. This can be formalised by assuming a logical language for the premises and the claim, and logical entailment (or},
  langid = {english}
}

@article{Besnard2014IntroductionStructuredArgumentation,
  title = {Introduction to {{Structured Argumentation}}},
  author = {Besnard, Philippe and Garcia, Alejandro and Hunter, Anthony and Modgil, Sanjay and Prakken, Henry and Simari, Guillermo and Toni, Francesca},
  date = {2014-01-02},
  journaltitle = {Argument \& Computation},
  volume = {5},
  number = {1},
  pages = {1--4},
  publisher = {SAGE Publications},
  issn = {1946-2166},
  doi = {10.1080/19462166.2013.869764},
  url = {https://doi.org/10.1080/19462166.2013.869764},
  urldate = {2025-09-09},
  abstract = {In abstract argumentation, each argument is regarded as atomic. There is no internal structure to an argument. Also, there is no specification of what is an argument or an attack. They are assumed to be given. This abstract perspective provides many advantages for studying the nature of argumentation, but it does not cover all our needs for understanding argumentation or for building tools for supporting or undertaking argumentation. If we want a more detailed formalisation of arguments than is available with abstract argumentation, we can turn to structured argumentation, which is the topic of this special issue of Argument and Computation. In structured argumentation, we assume a formal language for representing knowledge, and specifying how arguments and counterarguments can be constructed from that knowledge. An argument is then said to be structured in the sense that normally the premises and claim of the argument are made explicit, and the relationship between the premises and claim is formally defined (for instance using logical entailment). In this introduction, we provide a brief overview of the approaches covered in this special issue on structured argumentation.},
  langid = {english}
}

@article{Besnard2020LogicalTheoriesAbstract,
  title = {Logical Theories and Abstract Argumentation: {{A}}~Survey of Existing Works},
  shorttitle = {Logical Theories and Abstract Argumentation},
  author = {Besnard, Philippe and Cayrol, Claudette and Lagasquie-Schiex, Marie-Christine},
  date = {2020-05-19},
  journaltitle = {Argument \& Computation},
  volume = {11},
  number = {1--2},
  pages = {41--102},
  publisher = {SAGE Publications},
  issn = {1946-2166},
  doi = {10.3233/AAC-190476},
  url = {https://doi.org/10.3233/AAC-190476},
  urldate = {2025-09-07},
  abstract = {In 1995, in his seminal paper introducing the abstract argumentation framework, Dung has also established the first relationship between this framework and a logical framework (in this case: logic programming). Since that time, a lot of work have pursued this path, proposing different definitions, uses and exhibiting distinct relationships between argumentation and logic. In this paper, we present a survey of existing works about this topic and more especially those that address the following question: “How logic has been used for capturing various aspects or parts of Dung’s argumentation”. This survey covers many different approaches but is not intended to be totally exhaustive due to the huge quantity of papers in this scope. Moreover, due to the fact that each approach has its own specificities, sometimes antagonistic with the other approaches, and is also justified by its own context of definition or use, the aim of this survey is not to identify one approach as being better than another.},
  langid = {english}
}

@inproceedings{Bex2010FormalAnalysisAIF,
  title = {A Formal Analysis of the {{AIF}} in Terms of the {{ASPIC}} Framework.},
  author = {Bex, Floris and Prakken, Henry and Reed, Chris},
  date = {2010-01-01},
  pages = {99--110},
  doi = {10.3233/978-1-60750-619-5-99},
  eventtitle = {{{COMMA}}}
}

@article{Bex2013ImplementingArgumentWeb,
  title = {Implementing the Argument Web},
  author = {Bex, Floris and Lawrence, John and Snaith, Mark and Reed, Chris},
  date = {2013-10-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {56},
  number = {10},
  pages = {66--73},
  issn = {0001-0782},
  doi = {10.1145/2500891},
  url = {https://doi.org/10.1145/2500891},
  urldate = {2022-04-21},
  abstract = {Improve online public discourse by connecting opinions across blogs, editorials, and social media.}
}

@article{Bex2013LogicalSpecificationsArgument,
  title = {On Logical Specifications of the {{Argument Interchange Format}}},
  author = {Bex, Floris and Modgil, Sanjay and Prakken, Henry and Reed, C},
  date = {2013-09-25},
  journaltitle = {Journal of Logic and Computation},
  shortjournal = {Journal of Logic and Computation},
  volume = {23},
  number = {5},
  pages = {951--989},
  doi = {10.1093/logcom/exs033},
  url = {https://academic.oup.com/logcom/article-lookup/doi/10.1093/logcom/exs033},
  urldate = {2018-09-01},
  abstract = {The Argument Interchange Format (AIF) has been devised in order to support the interchange of ideas and data between different projects and applications in the area of computational argumentation. In order to support such interchange, an abstract ontology for argumentation is presented, which serves as an interlingua between various more concrete argumentation languages. In this article, we aim to give what is essentially a logical specification of the AIF ontology by mapping the ontology onto the logical ASPIC+~…}
}

@article{Bex2014ArguBloggingApplicationArgument,
  title = {{{ArguBlogging}}: {{An}} Application for the {{Argument Web}}},
  shorttitle = {{{ArguBlogging}}},
  author = {Bex, Floris and Snaith, Mark and Lawrence, John and Reed, Chris},
  date = {2014-03-01},
  journaltitle = {Journal of Web Semantics},
  shortjournal = {Journal of Web Semantics},
  volume = {25},
  pages = {9--15},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2014.02.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826814000079},
  urldate = {2023-11-24},
  abstract = {In this paper, we present a software tool for ‘ArguBlogging’, which allows users to construct debate and discussions across blogs, linking existing and new online resources to form distributed, structured conversations. Arguments and counterarguments can be posed by giving opinions on one’s own blog and replying to other bloggers’ posts. The resulting argument structure is connected to the Argument Web, in which argumentative structures are made semantically explicit and machine-processable. We discuss the ArguBlogging tool and the underlying infrastructure and ontology of the Argument Web.}
}

@inproceedings{Beyer2025LexicalRecallLogical,
  title = {Lexical {{Recall}} or {{Logical Reasoning}}: {{Probing}} the {{Limits}} of {{Reasoning Abilities}} in {{Large Language Models}}},
  shorttitle = {Lexical {{Recall}} or {{Logical Reasoning}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Beyer, Henrike and Reed, Chris},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {13532--13557},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.664/},
  urldate = {2025-07-28},
  abstract = {Despite the increasing interest in the reasoning abilities of Large Language Models (LLMs), existing work shows limitations in assessing logic abilities independently from lexical memory. We address this gap with Mystery-Zebra. This robust two-part benchmark (4,290 puzzles) challenges the logic abstraction abilities of LLMs in two setups: (1) a lexical obfuscation setup tests the dependence of LLMs on lexical content based on two canonical grid puzzles widely spread on the Internet; (2) a set of new grid puzzles in 42 different sizes and 12 difficulty levels tests how the formal difficulty degree of a puzzle affects LLMs.We test open and closed-weight LLMs on both parts of the benchmark. The results on part two suggest that model sizes up to 70B parameters have only a minor influence when solving newly generated puzzles, while performance mainly relates to the number of items in the puzzle. The results on the first part of the benchmark suggest that the applied obfuscation strategies help to mitigate effects of logic puzzles being part of LLM training data, showing a drastic drop in performance for obfuscated versions of well-known puzzles. In addition we conduct a case-study on the first part of the benchmark predicting the position of single items, unveiling that the reasoning abilities of LLMs are mainly limited to a few consecutive steps of reasoning.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Bezou-Vrakatseli2025CanLargeLanguage,
  title = {Can {{Large Language Models Understand Argument Schemes}}?},
  author = {Bezou-Vrakatseli, Elfia and Cocarascu, Oana and Modgil, Sanjay},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {13666--13681},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.findings-acl.702},
  url = {https://aclanthology.org/2025.findings-acl.702/},
  urldate = {2025-08-21},
  abstract = {Argument schemes represent stereotypical patterns of reasoning that occur in everyday arguments. However, despite their usefulness, argument scheme classification, that is classifying natural language arguments according to the schemes they are instances of, is an under-explored task in NLP. In this paper we present a systematic evaluation of large language models (LLMs) for classifying argument schemes based on Walton's taxonomy. We experiment with seven LLMs in zero-shot, few-shot, and chain-of-thought prompting, and explore two strategies to enhance task instructions: employing formal definitions and LLM-generated descriptions. Our analysis on both manually annotated and automatically generated arguments, including enthymemes, indicates that while larger models exhibit satisfactory performance in identifying argument schemes, challenges remain for smaller models. Our work offers the first comprehensive assessment of LLMs in identifying argument schemes, and provides insights for advancing reasoning capabilities in computational argumentation.},
  eventtitle = {Findings 2025},
  isbn = {979-8-89176-256-5}
}

@unpublished{Bhagavatula2020AbductiveCommonsenseReasoning,
  title = {Abductive {{Commonsense Reasoning}}},
  author = {Bhagavatula, Chandra and Bras, Ronan Le and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-tau and Choi, Yejin},
  date = {2020-02-13},
  eprint = {1908.05739},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1908.05739},
  urldate = {2020-06-10},
  abstract = {Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks -- (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9\% accuracy, well below human performance of 91.4\%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform--despite their strong performance on the related but more narrowly defined task of entailment NLI--pointing to interesting avenues for future research.}
}

@online{Bharti2017AutomaticKeywordExtraction,
  title = {Automatic {{Keyword Extraction}} for {{Text Summarization}}: {{A Survey}}},
  shorttitle = {Automatic {{Keyword Extraction}} for {{Text Summarization}}},
  author = {Bharti, Santosh Kumar and Babu, Korra Sathya},
  date = {2017-04-11},
  eprint = {1704.03242},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.03242},
  url = {http://arxiv.org/abs/1704.03242},
  urldate = {2023-10-26},
  abstract = {In recent times, data is growing rapidly in every domain such as news, social media, banking, education, etc. Due to the excessiveness of data, there is a need of automatic summarizer which will be capable to summarize the data especially textual data in original document without losing any critical purposes. Text summarization is emerged as an important research area in recent past. In this regard, review of existing work on text summarization process is useful for carrying out further research. In this paper, recent literature on automatic keyword extraction and text summarization are presented since text summarization process is highly depend on keyword extraction. This literature includes the discussion about different methodology used for keyword extraction and text summarization. It also discusses about different databases used for text summarization in several domains along with evaluation matrices. Finally, it discusses briefly about issues and research challenges faced by researchers along with future direction.},
  pubstate = {prepublished}
}

@article{Bichindaritz2006MemoryOrganizationMissing,
  title = {Memory {{Organization}} as the {{Missing Link Between Case-Based Reasoning}} and {{Information Retrieval}} in {{Biomedicine}}},
  author = {Bichindaritz, Isabelle},
  date = {2006},
  journaltitle = {Computational Intelligence},
  volume = {22},
  number = {3--4},
  pages = {148--160},
  issn = {1467-8640},
  doi = {10.1111/j.1467-8640.2006.00280.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2006.00280.x},
  urldate = {2025-07-07},
  abstract = {Mémoire proposes a general framework for reasoning from cases in biology and medicine. Part of this project is to propose a memory organization capable of handling large cases and case bases as occur in biomedical domains. This article presents the essential principles for an efficient memory organization based on pertinent work in information retrieval (IR). IR systems have been able to scale up to terabytes of data taking advantage of large databases research to build Internet search engines. They search for pertinent documents to answer a query using term-based ranking and/or global ranking schemes. Similarly, case-based reasoning (CBR) systems search for pertinent cases using a scoring function for ranking the cases. Mémoire proposes a memory organization based on inverted indexes which may be powered by databases to search and rank efficiently through large case bases. It can be seen as a first step toward large-scale CBR systems, and in addition provides a framework for tight cooperation between CBR and IR.},
  langid = {english}
}

@book{Bichindaritz2025CaseBasedReasoningResearch,
  title = {Case-{{Based Reasoning Research}} and {{Development}}: 33rd {{International Conference}}, {{ICCBR}} 2025, {{Biarritz}}, {{France}}, {{June}} 30–{{July}} 3, 2025, {{Proceedings}}},
  shorttitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15662},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3},
  url = {https://link.springer.com/10.1007/978-3-031-96559-3},
  urldate = {2025-06-30},
  isbn = {978-3-031-96558-6 978-3-031-96559-3},
  langid = {english}
}

@inproceedings{Biertz2022QualiAssistantExtractingQualia,
  title = {{{QualiAssistant}}: {{Extracting Qualia Structures}} from {{Texts}}},
  shorttitle = {{{QualiAssistant}}},
  booktitle = {Proceedings of the 9th {{Workshop}} on {{Argument Mining}}},
  author = {Biertz, Manuel and Dumani, Lorik and Nilles, Markus and Metzler, Björn and Schenkel, Ralf},
  date = {2022-10},
  pages = {199--208},
  publisher = {International Conference on Computational Linguistics},
  location = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.argmining-1.19},
  urldate = {2022-11-16},
  abstract = {In this paper, we present QualiAssistant, a free and open-source system written in Java for identification and extraction of Qualia structures from any natural language texts having many application scenarios such as argument mining or creating dictionaries. It answers the call for a Qualia bootstrapping tool with a ready-to-use system that can be gradually filled by the community with patterns in multiple languages. Qualia structures express the meaning of lexical items. They describe, e.g., of what kind the item is (formal role), what it includes (constitutive role), how it is brought about (agentive role), and what it is used for (telic role). They are also valuable for various Information Retrieval and NLP tasks. Our application requires search patterns for Qualia structures consisting of POS tag sequences as well as the dataset the user wants to search for Qualias. Samples for both are provided alongside this paper. While samples are in German, QualiAssistant can process all languages for which constituency trees can be generated and patterns are available. Our provided patterns follow a high-precision low-recall design aiming to generate automatic annotations for text mining but can be exchanged easily for other purposes. Our evaluation shows that QualiAssistant is a valuable and reliable tool for finding Qualia structures in unstructured texts.},
  eventtitle = {{{ArgMining-COLING}} 2022}
}

@inproceedings{Bilu2016ClaimSynthesisPredicate,
  title = {Claim {{Synthesis}} via {{Predicate Recycling}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Bilu, Yonatan and Slonim, Noam},
  date = {2016},
  pages = {525--530},
  publisher = {Association for Computational Linguistics},
  location = {Berlin, Germany},
  doi = {10.18653/v1/P16-2085},
  url = {http://aclweb.org/anthology/P16-2085},
  urldate = {2019-08-20},
  abstract = {Computational Argumentation has two main goals - the detection and analysis of arguments on the one hand, and the synthesis of arguments on the other. Much attention has been given to the former, but considerably less to the latter.},
  eventtitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  langid = {english}
}

@inproceedings{BinShiha2025UniversityNewsNew,
  title = {University {{News}}: {{A New Data Source}} for {{NLP Bias Research}}},
  shorttitle = {University {{News}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Bin Shiha, Rawan and Atwell, Eric and Abbas, Noorhan},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  pages = {307--312},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77915-2_22},
  abstract = {This research explores the use of university news articles for Natural Language Processing (NLP) and gender bias detection. It emphasises the importance of ethical considerations in NLP, advocating for transparency and diversity in dataset selection to ensure fairness. Using techniques such as Sentiment Analysis (SA) and gender-specific language classification, the study reveals a bias towards male possessive terms, indicating gender imbalance in the content. While the Facebook BART-Large-Mnli model demonstrated strong accuracy, it struggled with neutral sentiment, suggesting areas for improvement. The study highlights university news as a valuable dataset for promoting equity and inclusivity in NLP tools, laying the foundation for fairer methodologies.},
  isbn = {978-3-031-77915-2},
  langid = {english}
}

@book{Bird2009NaturalLanguageProcessing,
  title = {Natural {{Language Processing}} with {{Python}}: {{Analyzing Text}} with the {{Natural Language Toolkit}}},
  shorttitle = {Natural {{Language Processing}} with {{Python}}},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  date = {2009-06-12},
  eprint = {KGIbfiiP1i4C},
  eprinttype = {googlebooks},
  publisher = {O'Reilly Media, Inc.},
  abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.Packed with examples and exercises, Natural Language Processing with Python will help you:Extract information from unstructured text, either to guess the topic or identify "named entities"Analyze linguistic structure in text, including parsing and semantic analysisAccess popular linguistic databases, including WordNet and treebanksIntegrate techniques drawn from fields as diverse as linguistics and artificial intelligenceThis book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.},
  isbn = {978-0-596-55571-9},
  langid = {english},
  pagetotal = {506}
}

@article{Bizer2009DBpediaCrystallizationPoint,
  title = {{{DBpedia}} - {{A}} Crystallization Point for the {{Web}} of {{Data}}},
  author = {Bizer, Christian and Lehmann, Jens and Kobilarov, Georgi and Auer, Sören and Becker, Christian and Cyganiak, Richard and Hellmann, Sebastian},
  date = {2009-09-01},
  journaltitle = {Journal of Web Semantics},
  shortjournal = {Journal of Web Semantics},
  series = {The {{Web}} of {{Data}}},
  volume = {7},
  number = {3},
  pages = {154--165},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2009.07.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826809000225},
  urldate = {2024-06-15},
  abstract = {The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of Data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia.}
}

@incollection{Blair2003RelationshipsLogicDialectic,
  title = {Relationships {{Among Logic}}, {{Dialectic}} and {{Rhetoric}}},
  booktitle = {Anyone {{Who Has}} a {{View}}: {{Theoretical Contributions}} to the {{Study}} of {{Argumentation}}},
  author = {Blair, J. Anthony},
  editor = {Van Eemeren, Frans H. and Blair, J. Anthony and Willard, Charles A. and Snoeck Henkemans, A. Francisca},
  date = {2003},
  pages = {91--107},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-007-1078-8_8},
  url = {https://doi.org/10.1007/978-94-007-1078-8_8},
  urldate = {2025-09-08},
  abstract = {A consideration of the relationship among logic, dialectic and rhetoric was found already in the work of Plato and Aristotle and others in the first golden age of Western philosophy, and this relationship has received attention down through Western history (see the historical observations in Krabbe, 2000, in Hohmann, 2000, and in Leff, 2000). The argumentation scholarly community was reminded of its salience in the late 20th century (see Wenzel, 1990) and has returned to its examination. In the last five years or so, a flurry of activity has raised the profile of these questions in this community, particularly with the focus on how dialectic and rhetoric and their relationships bear on the identification, interpretation and assessment of arguments and argumentation (see the special issues of Argumentation edited by Hansen and Tindale, 1998, and by van Eemeren and Houtlosser, 1998).},
  isbn = {978-94-007-1078-8},
  langid = {english}
}

@book{Blair2012GroundworkTheoryArgumentation,
  title = {Groundwork in the {{Theory}} of {{Argumentation}}},
  author = {Blair, J. Anthony},
  editor = {Tindale, Christopher W.},
  date = {2012},
  series = {Argumentation {{Library}}},
  volume = {21},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-007-2363-4},
  url = {http://link.springer.com/10.1007/978-94-007-2363-4},
  urldate = {2025-09-09},
  isbn = {978-94-007-2362-7 978-94-007-2363-4}
}

@article{Blei2003LatentDirichletAllocation,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003-03-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {3},
  pages = {993--1022},
  issn = {1532-4435},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  issue = {null}
}

@article{Blevins2006WordbasedMorphology,
  title = {Word-Based Morphology},
  author = {Blevins, James P.},
  date = {2006-11},
  journaltitle = {Journal of Linguistics},
  volume = {42},
  number = {3},
  pages = {531--573},
  publisher = {Cambridge University Press},
  issn = {1469-7742, 0022-2267},
  doi = {10.1017/S0022226706004191},
  url = {https://www.cambridge.org/core/journals/journal-of-linguistics/article/abs/wordbased-morphology/B6687AE06D3A068CD6ED011319B11ED9#access-block},
  urldate = {2021-03-08},
  abstract = {This paper examines two contrasting perspectives on morphological analysis, and considers inflectional patterns that bear on the choice between these alternatives. On what is termed an ABSTRACTIVE perspective, surface word forms are regarded as basic morphotactic units of a grammatical system, with roots, stems and exponents treated as abstractions over a lexicon of word forms. This traditional standpoint is contrasted with the more CONSTRUCTIVE perspective of post-Bloomfieldian models, in which surface word forms are ‘built’ from sub-word units. Part of the interest of this contrast is that it cuts across conventional divisions of morphological models. Thus, realization-based models are morphosyntactically ‘word-based’ in the sense that they regard words as the minimal meaningful units of a grammatical system. Yet morphotactically, these models tend to adopt a constructive ‘root-based’ or ‘stem-based’ perspective. An examination of some form-class patterns in Saami, Estonian and Georgian highlights advantages of an abstractive model, and suggests that these advantages derive from the fact that sets of words often predict other word forms and determine a morphotactic analysis of their parts, whereas sets of sub-word units are of limited predictive value and typically do not provide enough information to recover word forms.},
  langid = {english}
}

@article{Blinowski2022MonolithicVsMicroservice,
  title = {Monolithic vs. {{Microservice Architecture}}: {{A Performance}} and {{Scalability Evaluation}}},
  shorttitle = {Monolithic vs. {{Microservice Architecture}}},
  author = {Blinowski, Grzegorz and Ojdowska, Anna and Przybyłek, Adam},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {20357--20374},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3152803},
  url = {https://ieeexplore.ieee.org/document/9717259},
  urldate = {2023-11-21},
  abstract = {Context. Since its proclamation in 2012, microservices-based architecture has gained widespread popularity due to its advantages, such as improved availability, fault tolerance, and horizontal scalability, as well as greater software development agility. Motivation. Yet, refactoring a monolith to microservices by smaller businesses and expecting that the migration will bring benefits similar to those reported by top global companies, such as Netflix, Amazon, eBay, and Uber, might be an illusion. Indeed, for systems that do not have thousands of concurrent users and can be scaled vertically, the benefits of such migration have not been sufficiently investigated, while the existing evidence is inconsistent. Objective. The purpose of this paper is to compare the performance and scalability of monolithic and microservice architectures on a reference web application. Method. The application was implemented in four different versions, covering not only two different architectural styles (monolith vs. microservices) but also two different implementation technologies (Java vs. C\#.NET). Next, we conducted a series of controlled experiments in three different deployment environments (local, Azure Spring Cloud, and Azure App Service). Findings. The key lessons learned are as follows: (1) on a single machine, a monolith performs better than its microservice-based counterpart; (2) The Java platform makes better use of powerful machines in case of computation-intensive services when compared to.NET; the technology platform effect is reversed when non-computationally intensive services are run on machines with low computational capacity; (3) vertical scaling is more cost-effective than horizontal scaling in the Azure cloud; (4) scaling out beyond a certain number of instances degrades the application performance; (5) implementation technology (either Java or C\#.NET) does not have a noticeable impact on the scalability performance.},
  eventtitle = {{{IEEE Access}}}
}

@incollection{Block2019ClusteringArgumentGraphs,
  title = {Clustering of {{Argument Graphs Using Semantic Similarity Measures}}},
  booktitle = {{{KI}} 2019: {{Advances}} in {{Artificial Intelligence}}},
  author = {Block, Karsten and Trumm, Simon and Sahitaj, Premtim and Ollinger, Stefan and Bergmann, Ralph},
  editor = {Benzmüller, Christoph and Stuckenschmidt, Heiner},
  date = {2019},
  volume = {11793},
  pages = {101--114},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-30179-8_8},
  url = {http://link.springer.com/10.1007/978-3-030-30179-8_8},
  urldate = {2022-01-03},
  abstract = {Research on argumentation in Artificial Intelligence recently investigates new methods that contribute to the vision of developing robust argumentation machines. One line of research explores ways of reasoning with natural language arguments coming from information sources on the web as a foundation for the deliberation and synthesis of arguments in specific domains. This paper builds upon arguments represented as argument graphs in the standardized Argument Interchange Format. While previous work was focused on the development of semantic similarity measures used for the case-based retrieval of argument graphs, this paper addresses the problem of clustering argument graphs to explore structures that facilitate argumentation interpretation. We propose a k-medoid and an agglomerative clustering approach based on semantic similarity measures. We compare the clustering results based on a graph-based semantic measure that takes the structure of the argument into account with a semantic word2vec measure on the pure textual argument representation. Experiments based on the Microtext corpus show that the graph-based similarity is best on internal evaluation measures, while the pure textual measure performs very well for identifying topic-specific clusters.},
  isbn = {978-3-030-30178-1 978-3-030-30179-8},
  langid = {english}
}

@online{Bloem2019TransformersScratch,
  title = {Transformers {{From Scratch}}},
  author = {Bloem, Peter},
  date = {2019-08-18},
  url = {https://peterbloem.nl/blog/transformers},
  urldate = {2025-09-11}
}

@inproceedings{Blumenthal2017CorrectingSpeedingUpBounds,
  title = {Correcting and {{Speeding-Up Bounds}} for {{Non-Uniform Graph Edit Distance}}},
  booktitle = {2017 {{IEEE}} 33rd {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Blumenthal, David B. and Gamper, Johann},
  date = {2017-04},
  pages = {131--134},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2017.57},
  url = {https://ieeexplore.ieee.org/document/7929953},
  urldate = {2025-06-03},
  abstract = {The problem of deriving lower and upper bounds for the edit distance between labelled undirected graphs has recently received increasing attention. However, only one algorithm has been proposed that allegedly computes not only an upper but also a lower bound for non-uniform metric edit costs and incorporates information about both node and edge labels. In this paper, we show that this algorithm is incorrect in the sense that, in general, it does not compute a lower bound. We present BRANCH, a corrected version of the algorithm that runs in O(n5) time. We also develop a speed-up BRANCHFAST that runs in O(n4) time and computes a lower bound, which is only slightly less accurate than the one computed by BRANCH. An experimental evaluation shows that BRANCH and BRANCHFAST yield excellent runtime/accuracy-tradeoffs, as they outperform all existing competitors in terms of runtime or in terms of accuracy.},
  eventtitle = {2017 {{IEEE}} 33rd {{International Conference}} on {{Data Engineering}} ({{ICDE}})}
}

@article{Blumenthal2018ImprovedLowerBounds,
  title = {Improved {{Lower Bounds}} for {{Graph Edit Distance}}},
  author = {Blumenthal, David B. and Gamper, Johann},
  date = {2018-03},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {30},
  number = {3},
  pages = {503--516},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2017.2772243},
  url = {https://ieeexplore.ieee.org/document/8103805},
  urldate = {2025-06-03},
  abstract = {The problem of deriving lower and upper bounds for the edit distance between undirected, labeled graphs has recently received increasing attention. However, only one algorithm has been proposed that allegedly computes not only an upper but also a lower bound for non-uniform edit costs and incorporates information about both node and edge labels. In this paper, we demonstrate that this algorithm is incorrect. We present a corrected version BRANCH that runs in O(n2Δ3 + n3) time, where Δ is the maximum of the maximum degrees of input graphs G and H. We also develop a speed-up BRANCHFAST that runs in O(n2Δ2 + n3) time and computes an only slightly less accurate lower bound. The lower bounds produced by BRANCH and BRANCHFAST are shown to be pseudo-metrics on a collection of graphs. Finally, we suggest an anytime algorithm BRANCHTIGHT that iteratively improves BRANCH's lower bound. BRANCHTIGHT runs in O(n3Δ2 + I(n2Δ3 + n3)) time, where the number of iterations I is controlled by the user. A detailed experimental evaluation shows that all suggested algorithms are Pareto optimal, that they are very effective when used as filters for edit distance range queries, and that they perform excellently when used within classification frameworks.}
}

@article{Blumenthal2020ComparingHeuristicsGraph,
  title = {Comparing Heuristics for Graph Edit Distance Computation},
  author = {Blumenthal, David B. and Boria, Nicolas and Gamper, Johann and Bougleux, Sébastien and Brun, Luc},
  date = {2020-01-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {29},
  number = {1},
  pages = {419--458},
  issn = {0949-877X},
  doi = {10.1007/s00778-019-00544-1},
  url = {https://doi.org/10.1007/s00778-019-00544-1},
  urldate = {2025-06-02},
  abstract = {Because of its flexibility, intuitiveness, and expressivity, the graph edit distance (GED) is one of the most widely used distance measures for labeled graphs. Since exactly computing GED is NP-hard, over the past years, various heuristics have been proposed. They use techniques such as transformations to the linear sum assignment problem with error correction, local search, and linear programming to approximate GED via upper or lower bounds. In this paper, we provide a systematic overview of the most important heuristics. Moreover, we empirically evaluate all compared heuristics within an integrated implementation.},
  langid = {english}
}

@unpublished{Bogner2022TypeNotType,
  title = {To {{Type}} or {{Not}} to {{Type}}? {{A Systematic Comparison}} of the {{Software Quality}} of {{JavaScript}} and {{TypeScript Applications}} on {{GitHub}}},
  shorttitle = {To {{Type}} or {{Not}} to {{Type}}?},
  author = {Bogner, Justus and Merkel, Manuel},
  date = {2022-03-21},
  eprint = {2203.11115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.11115},
  urldate = {2022-05-03},
  abstract = {JavaScript (JS) is one of the most popular programming languages, and widely used for web apps and even backend development. Due to its dynamic nature, however, JS applications often have a reputation for poor software quality. As a type-safe superset of JavaScript, TypeScript (TS) offers features to address this. However, there is currently insufficient empirical evidence to broadly support the claim that TS apps exhibit better software quality than JS apps. We therefore conducted a repository mining study based on 604 GitHub projects (299 for JS, 305 for TS) with over 16M LoC and collected four facets of software quality: a) code quality (\# of code smells per LoC), b) code understandability (cognitive complexity per LoC), c) bug proneness (bug fix commit ratio), and d) bug resolution time (mean time a bug issue is open). For TS, we also collected how frequently the type-safety ignoring `any` type was used. The analysis indicates that TS apps exhibit significantly better code quality and understandability than JS apps. Contrary to expectations, however, bug proneness and bug resolution time of our TS sample were not significantly lower than for JS: mean bug fix commit ratio was more than 60\% larger (0.126 vs. 0.206), and TS projects needed on average more than an additional day to fix bugs (31.86 vs. 33.04 days). Furthermore, reducing the usage of the `any` type in TS apps was significantly correlated with all metrics except bug proneness (Spearman's rho between 0.17 and 0.26). Our results indicate that the perceived positive influence of TypeScript for avoiding bugs in comparison to JavaScript may be more complicated than assumed. While using TS seems to have benefits, it does not automatically lead to less and easier to fix bugs. However, more research is needed in this area, especially concerning the potential influence of project complexity and developer experience.}
}

@unpublished{Bojanowski2016EnrichingWordVectors,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  date = {2016-07-15},
  eprint = {1607.04606},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {https://arxiv.org/abs/1607.04606},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}

@article{Boller1990ConceptualizingArgumentQuality,
  title = {Conceptualizing {{Argument Quality Via Argument Structure}}},
  author = {Boller, Gregory W. and Swasy, John L. and Munch, James M.},
  date = {1990},
  journaltitle = {ACR North American Advances},
  volume = {NA-17},
  url = {http://acrwebsite.org/volumes/9831/volumes/v17/NA-17},
  urldate = {2019-09-04},
  langid = {english}
}

@inproceedings{Boltuzic2015IdentifyingProminentArguments,
  title = {Identifying {{Prominent Arguments}} in {{Online Debates Using Semantic Textual Similarity}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Argumentation Mining}}},
  author = {Boltužić, Filip and Šnajder, Jan},
  date = {2015-06},
  pages = {110--115},
  publisher = {Association for Computational Linguistics},
  location = {Denver, CO},
  doi = {10.3115/v1/W15-0514},
  url = {https://www.aclweb.org/anthology/W15-0514},
  urldate = {2019-11-18}
}

@article{Bonacich1987PowerCentralityFamily,
  title = {Power and {{Centrality}}: {{A Family}} of {{Measures}}},
  shorttitle = {Power and {{Centrality}}},
  author = {Bonacich, Phillip},
  date = {1987-03},
  journaltitle = {American Journal of Sociology},
  volume = {92},
  number = {5},
  pages = {1170--1182},
  publisher = {The University of Chicago Press},
  issn = {0002-9602},
  doi = {10.1086/228631},
  url = {https://www.journals.uchicago.edu/doi/10.1086/228631},
  urldate = {2025-02-20},
  abstract = {Although network centrality is generally assumed to produce power, recent research shows that this is not the case in exchange networks. This paper proposes a generalization of the concept of centrality that accounts for both the usual positive relationship between power and centrality and Cook et al.'s recent exceptional results.}
}

@book{Bonami2018LexemeDescriptiveTheoretical,
  title = {The Lexeme in Descriptive and Theoretical Morphology},
  author = {Bonami, Olivier},
  date = {2018-02-19},
  series = {Empirically {{Oriented Theoretical Morphology}} and {{Syntax}}},
  publisher = {Language Science Press},
  location = {Berlin},
  url = {https://langsci-press.org/catalog/book/165},
  langid = {english}
}

@inproceedings{Bond2013LinkingExtendingOpen,
  title = {Linking and {{Extending}} an {{Open Multilingual Wordnet}}},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Bond, Francis and Foster, Ryan},
  date = {2013-08},
  pages = {1352--1362},
  publisher = {Association for Computational Linguistics},
  location = {Sofia, Bulgaria},
  url = {https://www.aclweb.org/anthology/P13-1133},
  urldate = {2021-02-13},
  eventtitle = {{{ACL}} 2013}
}

@article{Bondarenko1993AssumptionbasedFrameworkNonmonotonic,
  title = {An {{Assumption-based Framework}} for {{Non-monotonic Reasoning}}},
  author = {Bondarenko, Andrei and Toni, Francesca and Kowalski, Robert A.},
  date = {1993-05-24},
  doi = {10.7551/mitpress/4307.003.0016},
  url = {https://direct.mit.edu/books/edited-volume/4498/chapter/192331/An-Assumption-based-Framework-for-Non-monotonic},
  urldate = {2025-09-09},
  langid = {english}
}

@inproceedings{Bondarenko2020ComparativeWebSearch,
  title = {Comparative {{Web Search Questions}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Bondarenko, Alexander and Braslavski, Pavel and Völske, Michael and Aly, Rami and Fröbe, Maik and Panchenko, Alexander and Biemann, Chris and Stein, Benno and Hagen, Matthias},
  date = {2020-01-20},
  series = {{{WSDM}} '20},
  pages = {52--60},
  publisher = {Association for Computing Machinery},
  location = {Houston, TX, USA},
  doi = {10.1145/3336191.3371848},
  url = {https://doi.org/10.1145/3336191.3371848},
  urldate = {2020-09-02},
  abstract = {\textbackslash beginabstract We analyze comparative questions, i.e., questions asking to compare different items, that were submitted to Yandex in 2012. Responses to such questions might be quite different from the simple "ten blue links'' and could, for example, aggregate pros and cons of the different options as direct answers. However, changing the result presentation is an intricate decision such that the classification of comparative questions forms a highly precision-oriented task. From a year-long Yandex log, we annotate a random sample of 50,000\textasciitilde questions; 2.8\%\textasciitilde of which are comparative. For these annotated questions, we develop a precision-oriented classifier by combining carefully hand-crafted lexico-syntactic rules with feature-based and neural approaches---achieving a recall of\textasciitilde 0.6 at a perfect precision of\textasciitilde 1.0. After running the classifier on the full year log (on average, there is at least one comparative question per second), we analyze 6,250\textasciitilde comparative questions using more fine-grained subclasses (e.g., should the answer be a "simple'' fact or rather a more verbose argument) for which individual classifiers are trained. An important insight is that more than 65\%\textasciitilde of the comparative questions demand argumentation and opinions, i.e., reliable direct answers to comparative questions require more than the facts from a search engine's knowledge graph. In addition, we present a qualitative analysis of the underlying comparative information needs (separated into 14\textasciitilde categories likeconsumer electronics orhealth ), their seasonal dynamics, and possible answers from community question answering platforms. \textbackslash endabstract},
  isbn = {978-1-4503-6822-3}
}

@inproceedings{Bondarenko2020ToucheFirstShared,
  title = {Touché: {{First Shared Task}} on {{Argument Retrieval}}},
  shorttitle = {Touché},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Bondarenko, Alexander and Hagen, Matthias and Potthast, Martin and Wachsmuth, Henning and Beloucif, Meriem and Biemann, Chris and Panchenko, Alexander and Stein, Benno},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {517--523},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-45442-5_67},
  abstract = {Technologies for argument mining and argumentation processing are maturing continuously, giving rise to the idea of retrieving arguments in search scenarios. We introduce Touché, the first lab on Argument Retrieval featuring two subtasks: (1) the retrieval of arguments from a focused debate collection to support argumentative conversations, and (2) the retrieval of arguments from a generic web crawl to answer comparative questions with argumentative results. The goal of this lab is to perform an evaluation of various strategies to retrieve argumentative information from the web content. In this paper, we describe the setting of each subtask: the motivation, the data, and the evaluation methodology.},
  isbn = {978-3-030-45442-5},
  langid = {english}
}

@inproceedings{Bondarenko2023OverviewTouche2023,
  title = {Overview of~{{Touché}} 2023: {{Argument}} and~{{Causal Retrieval}}},
  shorttitle = {Overview of~{{Touché}} 2023},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Bondarenko, Alexander and Fröbe, Maik and Kiesel, Johannes and Schlatt, Ferdinand and Barriere, Valentin and Ravenet, Brian and Hemamou, Léo and Luck, Simon and Reimer, Jan Heinrich and Stein, Benno and Potthast, Martin and Hagen, Matthias},
  editor = {Kamps, Jaap and Goeuriot, Lorraine and Crestani, Fabio and Maistro, Maria and Joho, Hideo and Davis, Brian and Gurrin, Cathal and Kruschwitz, Udo and Caputo, Annalina},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {527--535},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-28241-6_61},
  abstract = {The goal of Touché is to foster and support the development of technologies for argument and causal retrieval and analysis. For the fourth time, we organize the Touché lab featuring four shared tasks: (a)~argument retrieval for controversial topics, where participants retrieve web documents that contain high-quality argumentation and detect the argument stance, (b)~causal retrieval, where participants retrieve documents that contain causal statements from a generic web crawl and detect the causal stance, (c)~image retrieval for arguments, where participants retrieve images showing support or opposition to some stance from a focused web crawl, and (d)~intra-multilingual multi-target stance classification, where participants detect the stance of comments on proposals from the multilingual participatory democracy platform~CoFE. In this paper, we briefly summarize the results of Touché~2022 and describe the planned setup for the fourth lab edition at CLEF~2023.},
  isbn = {978-3-031-28241-6},
  langid = {english}
}

@article{Bonnici2013SubgraphIsomorphismAlgorithm,
  title = {A Subgraph Isomorphism Algorithm and Its Application to Biochemical Data},
  author = {Bonnici, Vincenzo and Giugno, Rosalba and Pulvirenti, Alfredo and Shasha, Dennis and Ferro, Alfredo},
  date = {2013-04-22},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {14},
  number = {7},
  pages = {S13},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-14-S7-S13},
  url = {https://doi.org/10.1186/1471-2105-14-S7-S13},
  urldate = {2019-01-07},
  abstract = {Graphs can represent biological networks at the molecular, protein, or species level. An important query is to find all matches of a pattern graph to a target graph. Accomplishing this is inherently difficult (NP-complete) and the efficiency of heuristic algorithms for the problem may depend upon the input graphs. The common aim of existing algorithms is to eliminate unsuccessful mappings as early as and as inexpensively as possible.}
}

@inproceedings{Bonzon2016ComparativeStudyRankingBased,
  title = {A {{Comparative Study}} of {{Ranking-Based Semantics}} for {{Abstract Argumentation}}},
  booktitle = {Proceedings of the {{Thirtieth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Bonzon, Elise and Delobelle, Jérôme and Konieczny, Sébastien and Maudet, Nicolas},
  date = {2016-02-21},
  volume = {30},
  pages = {914--920},
  doi = {10.1609/aaai.v30i1.10116},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/10116},
  urldate = {2025-09-09},
  abstract = {Argumentation is a process of evaluating and comparing a set of arguments. A way to compare them consists in using a ranking-based semantics which rank-order arguments from the most to the least acceptable ones. Recently, a number of such semantics have been pro- posed independently, often associated with some desirable properties. However, there is no comparative study which takes a broader perspective. This is what we propose in this work. We provide a general comparison of all these semantics with respect to the proposed proper- ties. That allows to underline the differences of behavior between the existing semantics.},
  eventtitle = {{{AAAI}} 2016},
  langid = {english}
}

@inproceedings{Bosc2016DARTDatasetArguments,
  title = {{{DART}}: A {{Dataset}} of {{Arguments}} and Their {{Relations}} on {{Twitter}}},
  shorttitle = {{{DART}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Bosc, Tom and Cabrio, Elena and Villata, Serena},
  date = {2016-05},
  pages = {1258--1263},
  publisher = {European Language Resources Association (ELRA)},
  location = {Portorož, Slovenia},
  url = {https://aclanthology.org/L16-1200},
  urldate = {2022-01-13},
  abstract = {The problem of understanding the stream of messages exchanged on social media such as Facebook and Twitter is becoming a major challenge for automated systems. The tremendous amount of data exchanged on these platforms as well as the specific form of language adopted by social media users constitute a new challenging context for existing argument mining techniques. In this paper, we describe a resource of natural language arguments called DART (Dataset of Arguments and their Relations on Twitter) where the complete argument mining pipeline over Twitter messages is considered: (i) we identify which tweets can be considered as arguments and which cannot, and (ii) we identify what is the relation, i.e., support or attack, linking such tweets to each other.},
  eventtitle = {{{LREC}} 2016}
}

@inproceedings{Bosc2016TweetiesSquabblingPositive,
  title = {Tweeties {{Squabbling}}: {{Positive}} and {{Negative Results}} in {{Applying Argument Mining}} on {{Social Media}}},
  shorttitle = {Tweeties {{Squabbling}}},
  booktitle = {Computational {{Models}} of {{Argument}}},
  author = {Bosc, Tom and Cabrio, Elena and Villata, Serena},
  date = {2016},
  pages = {21--32},
  publisher = {IOS Press},
  doi = {10.3233/978-1-61499-686-6-21},
  url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-686-6-21},
  urldate = {2023-10-20}
}

@inproceedings{Bosselut2019COMETCommonsenseTransformers,
  title = {{{COMET}}: {{Commonsense Transformers}} for {{Automatic Knowledge Graph Construction}}},
  shorttitle = {{{COMET}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  date = {2019-07},
  pages = {4762--4779},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/P19-1470},
  url = {https://www.aclweb.org/anthology/P19-1470},
  urldate = {2020-05-02},
  abstract = {We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5\% (ATOMIC) and 91.7\% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.},
  eventtitle = {{{ACL}} 2019}
}

@inproceedings{Boteanu2015SolvingExplainingAnalogy,
  title = {Solving and {{Explaining Analogy Questions Using Semantic Networks}}},
  booktitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Boteanu, Adrian and Chernova, Sonia},
  date = {2015-02-18},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9544},
  urldate = {2020-05-30},
  abstract = {Analogies are a fundamental human reasoning pattern that relies on relational similarity. Understanding how analogies are formed facilitates the transfer of knowledge between contexts. The approach presented in this work focuses on obtaining precise interpretations of analogies. We leverage noisy semantic networks to answer and explain a wide spectrum of analogy questions. The core of our contribution, the Semantic Similarity Engine, consists of methods for extracting and comparing graph-contexts that reveal the relational parallelism that analogies are based on, while mitigating uncertainty in the semantic network.We demonstrate these methods in two tasks: answering multiple choice analogy questions and generating human readable analogy explanations. We evaluate our approach on two datasets totaling 600 analogy questions. Our results show reliable performance and low false-positive rate in question answering; human evaluators agreed with 96\% of our analogy explanations.},
  eventtitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  langid = {english}
}

@article{Bougleux2017GraphEditDistance,
  title = {Graph Edit Distance as a Quadratic Assignment Problem},
  author = {Bougleux, Sébastien and Brun, Luc and Carletti, Vincenzo and Foggia, Pasquale and Gaüzère, Benoit and Vento, Mario},
  date = {2017-02-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  series = {Advances in {{Graph-based Pattern Recognition}}},
  volume = {87},
  pages = {38--46},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2016.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865516302665},
  urldate = {2025-06-02},
  abstract = {The Graph Edit Distance (GED) is a flexible measure of dissimilarity between graphs which arises in error-correcting graph matching. It is defined from an optimal sequence of edit operations (edit path) transforming one graph into another. Unfortunately, the exact computation of this measure is NP-hard. In the last decade, several approaches were proposed to approximate the GED in polynomial time, mainly by solving linear programming problems. Among them, the bipartite GED received much attention. It is deduced from a linear sum assignment of the nodes of the two graphs, which can be efficiently computed by Hungarian-type algorithms. However, edit operations on nodes and edges are not handled simultaneously, which limits the accuracy of the approximation. To overcome this limitation, we propose to extend the linear assignment model to a quadratic one. This is achieved through the definition of a family of edit paths induced by assignments between nodes. We formally show that the GED, restricted to the paths in this family, is equivalent to a quadratic assignment problem. Since this problem is NP-hard, we propose to compute an approximate solution by adapting two algorithms: Integer Projected Fixed Point method and Graduated Non Convexity and Concavity Procedure. Experiments show that the proposed approach is generally able to reach a more accurate approximation of the exact GED than the bipartite GED, with a computational cost that is still affordable for graphs of non trivial sizes.}
}

@inproceedings{Bowman2015LargeAnnotatedCorpus,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  date = {2015-09},
  pages = {632--642},
  publisher = {Association for Computational Linguistics},
  location = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1075},
  url = {https://www.aclweb.org/anthology/D15-1075},
  urldate = {2021-02-07},
  eventtitle = {{{EMNLP}} 2015}
}

@inproceedings{Boylan2024KGValidatorFrameworkAutomatic,
  title = {{{KGValidator}}: {{A Framework}} for {{Automatic Validation}} of {{Knowledge Graph Construction}}},
  shorttitle = {{{KGValidator}}},
  booktitle = {Joint Proceedings of the 3rd {{International}} Workshop One Knowledge Graph Generation from Text ({{TEXT2KG}}) and {{Data Quality}} Meets {{Machine Learning}} and {{Knowledge Graphs}} ({{DQMLKG}})},
  author = {Boylan, Jack and Gholipour-Ghalandari, Demian and Hokamp, Chris and Thorn, Dominic and Ghaffari, Parsa and Mangla, Shashank},
  editor = {Tiwari, Sanju and Mihindukulasooriya, Nandana and Osborne, Francesco and Kontokostas, Dimitris and D’Souza, Jennifer and Kejriwal, Mayank and Pellegrino, Maria Angela and Rula, Anisa and Gayo, Jose Emilio Labra and Cochez, Michael and Alam, Mehwish},
  date = {2024-05-26},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3747},
  pages = {23},
  publisher = {CEUR},
  location = {Hersonissos, Greece},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3747/#paper12},
  urldate = {2024-10-31},
  eventtitle = {3rd {{International}} Workshop One Knowledge Graph Generation from Text. {{Data Quality}} Meets {{Machine Learning}} and {{Knowledge Graphs}} 2024},
  langid = {english}
}

@book{Bramer2025ArtificialIntelligenceXLI,
  title = {Artificial {{Intelligence XLI}}: 44th {{SGAI International Conference}} on {{Artificial Intelligence}}, {{AI}} 2024, {{Cambridge}}, {{UK}}, {{December}} 17–19, 2024, {{Proceedings}}, {{Part II}}},
  shorttitle = {Artificial {{Intelligence XLI}}},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15447},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77918-3},
  url = {https://link.springer.com/10.1007/978-3-031-77918-3},
  urldate = {2025-06-08},
  isbn = {978-3-031-77917-6 978-3-031-77918-3},
  langid = {english}
}

@book{Bramer2025ArtificialIntelligenceXLIa,
  title = {Artificial {{Intelligence XLI}}: 44th {{SGAI International Conference}} on {{Artificial Intelligence}}, {{AI}} 2024, {{Cambridge}}, {{UK}}, {{December}} 17–19, 2024, {{Proceedings}}, {{Part I}}},
  shorttitle = {Artificial {{Intelligence XLI}}},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15446},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77915-2},
  url = {https://link.springer.com/10.1007/978-3-031-77915-2},
  urldate = {2025-06-08},
  isbn = {978-3-031-77914-5 978-3-031-77915-2},
  langid = {english}
}

@inproceedings{Brand2024LargeLanguageModels,
  title = {Large {{Language Models}} as {{Knowledge Engineers}}},
  booktitle = {Proceedings of the {{Workshops}} at the 32nd {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2024)},
  author = {Brand, Florian and Malburg, Lukas and Bergmann, Ralph},
  editor = {Malburg, Lukas},
  date = {2024-07-01},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3708},
  pages = {3--18},
  publisher = {CEUR},
  location = {Mérida, Mexico},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3708/#CBRLLM01},
  urldate = {2025-03-14},
  eventtitle = {{{ICCBR}} 2024 {{Workshop Proceedings}}},
  langid = {english}
}

@thesis{Brand2024ModelingAcquiringKnowledge,
  type = {mathesis},
  title = {Modeling and {{Acquiring Knowledge}} with {{Large Language Models}}: {{A Study}} in the {{Cyber-Physical Domain}}},
  shorttitle = {Modeling and {{Acquiring Knowledge}} with {{Large Language Models}}},
  author = {Brand, Florian},
  date = {2024-02-19},
  institution = {Trier University},
  location = {Trier},
  abstract = {Many Artificial Intelligence (AI) systems rely on human-engineered knowledge to solve new problems with the provided knowledge. These systems are deployed in a range of domains, from medicine over finance to the cyber-physical domain. However, the engineering and acquisition of knowledge for these systems is a cumbersome task, even for domain experts. There are various tools and methods proposed to lessen the burden on those experts. The recent advancements in Large Language Models (LLMs) show a promising way to utilize them in the knowledge creation process. This thesis applies LLMs as a tool for the engineering and acquisition of formal knowledge in the cyber-physical domain. It analyzes the characteristics and differences of LLMs and determines their suitability for this task. Additionally, the knowledge representations of the systems in the cyber-physical domain are researched and compared. Furthermore, a method is developed to use LLMs for the generation of formal knowledge in the cyber-physical domain. This method is implemented and evaluated, showing promising results across different setups. However, a domain expert is still needed in the process to guide the LLM and provide useful inputs.},
  langid = {english},
  pagetotal = {100}
}

@article{Branting2003ReductiongraphModelPrecedent,
  title = {A Reduction-Graph Model of Precedent in Legal Analysis},
  author = {Branting, L. Karl},
  date = {2003-11},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {150},
  number = {1--2},
  pages = {59--95},
  issn = {00043702},
  doi = {10.1016/S0004-3702(03)00102-4},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370203001024},
  urldate = {2019-08-20},
  abstract = {Legal analysis is a task underlying many forms of legal problem solving. In the Anglo-American legal system, legal analysis is based in part on legal precedents, previously decided cases. This paper describes a reduction-graph model of legal precedents that accounts for a key characteristic of legal precedents: a precedent’s relevance to subsequent cases is determined by the theory under which the precedent is decided. This paper identifies the implementation requirements for legal analysis using the reduction-graph model of legal precedents and describes GREBE, a program that satisfies these requirements.},
  langid = {english}
}

@book{Braun2026KI2025Advances,
  title = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}: 48th {{German Conference}} on {{AI}}, {{Potsdam}}, {{Germany}}, {{September}} 16–19, 2025, {{Proceedings}}},
  shorttitle = {{{KI}} 2025},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15956},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6},
  url = {https://link.springer.com/10.1007/978-3-032-02813-6},
  urldate = {2025-09-02},
  isbn = {978-3-032-02812-9 978-3-032-02813-6},
  langid = {english}
}

@inproceedings{Breen2004JMdictJapaneseMultilingualDictionary,
  title = {{{JMdict}}: A {{Japanese-Multilingual Dictionary}}},
  shorttitle = {{{JMdict}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Multilingual Linguistic Resources}}},
  author = {Breen, Jim},
  date = {2004-08-28},
  pages = {65--72},
  publisher = {COLING},
  location = {Geneva, Switzerland},
  url = {https://www.aclweb.org/anthology/W04-2209},
  urldate = {2021-02-13}
}

@inproceedings{Brincoveanu2026AugmentingSystematicLiterature,
  title = {Augmenting {{Systematic Literature Reviews}}: {{A Human-AI Collaborative Framework}}},
  shorttitle = {Augmenting {{Systematic Literature Reviews}}},
  booktitle = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}},
  author = {Brîncoveanu, Constantin and Carl, K. Valerie and Witzki, Aaron and Hinz, Oliver},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  pages = {3--17},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6_1},
  abstract = {While Systematic Literature Reviews (SLRs) are integral to research by synthesizing existing knowledge and guiding future inquiry, the exponential increase in academic publications presents significant challenges to traditional, manual review methods, notably regarding scalability, efficiency, and researcher workload. Recent advancements in Artificial Intelligence (AI), particularly Large Language Models (LLMs), offer promising avenues for augmenting the SLR process. Nonetheless, integrating AI into literature reviews introduces methodological complexities, including maintaining accuracy, minimizing biases, and preserving scholarly rigor. To address these challenges, this paper introduces a structured AI-augmented SLR framework, systematically integrating AI capabilities into Wolfswinkel et al.’s~[37] established Grounded Theory Literature Review Method. Our framework incorporates AI-driven relevance assessments, automated selection processes, and thematic content analysis, underpinned by rigorous human oversight to ensure reliability and interpretative validity. We empirically illustrate and evaluate our framework through a comparative study, replicating and extending a previously published human SLR. The evaluation assesses AI performance using key metrics such as type I and type II error rates across varying confidence thresholds. Results demonstrate substantial efficiency gains and effective accuracy in AI-assisted selection, highlighting the importance of carefully calibrated thresholds and continued human oversight. Our study contributes practical guidelines for effectively balancing AI automation with human scholarly judgment, offering a replicable methodological approach for researchers seeking to leverage AI capabilities without compromising methodological quality or academic integrity.},
  isbn = {978-3-032-02813-6},
  langid = {english}
}

@inproceedings{Brinner2024WeaklySupervisedClaim,
  title = {Weakly {{Supervised Claim Localization}} in~{{Scientific Abstracts}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Brinner, Marc and Zarrieß, Sina and Heger, Tina},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {20--38},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_2},
  abstract = {We explore the possibility of leveraging model explainability methods for weakly supervised claim localization in scientific abstracts. The resulting approaches require only abstract-level supervision, i.e., information about the general presence of a claim in a given abstract, to extract spans of text that indicate this specific claim. We evaluate our methods on the SciFact claim verification dataset, as well as on a newly created dataset that contains expert-annotated evidence for scientific hypotheses in paper abstracts from the field of invasion biology. Our results suggest that significant performance in the claim localization task can be achieved without any explicit supervision, which increases the transferability to new domains with limited data availability. In the course of our experiments, we additionally find that injecting information from human evidence annotations into the training of a neural network classifier can lead to a significant increase in classification performance.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Britner2023AQUAPLANEArgumentQuality,
  title = {{{AQUAPLANE}}: {{The Argument Quality Explainer App}}},
  shorttitle = {{{AQUAPLANE}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Britner, Sebastian and Dumani, Lorik and Schenkel, Ralf},
  date = {2023-10-21},
  series = {{{CIKM}} '23},
  pages = {5015--5020},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3583780.3614733},
  url = {https://dl.acm.org/doi/10.1145/3583780.3614733},
  urldate = {2023-11-24},
  abstract = {In computational argumentation, so-called quality dimensions such as coherence or rhetoric are often used for ranking arguments. However, the literature often only predicts which argument is more persuasive, but not why this is the case. In this paper, we introduce AQUAPLANE, a transparent and easy-to-extend application that not only decides for a pair of arguments which one is more convincing with respect to a statement, but also provides an explanation.},
  isbn = {979-8-4007-0124-5}
}

@inproceedings{Brown2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {https://dl.acm.org/doi/abs/10.5555/3495724.3495883},
  urldate = {2024-06-21},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.}
}

@inproceedings{Bryant2018LanguageModelBased,
  title = {Language {{Model Based Grammatical Error Correction}} without {{Annotated Training Data}}},
  booktitle = {Proceedings of the {{Thirteenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  author = {Bryant, Christopher and Briscoe, Ted},
  date = {2018-06},
  pages = {247--253},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans, Louisiana},
  doi = {10.18653/v1/W18-0529},
  url = {https://www.aclweb.org/anthology/W18-0529},
  urldate = {2020-05-20},
  abstract = {Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.}
}

@article{Budan2017ApproachCharacterizeGraded,
  title = {An Approach to Characterize Graded Entailment of Arguments through a Label-Based Framework},
  author = {Budán, Maximiliano C. D. and Simari, Gerardo I. and Viglizzo, Ignacio and Simari, Guillermo R.},
  date = {2017-03-01},
  journaltitle = {International Journal of Approximate Reasoning},
  shortjournal = {International Journal of Approximate Reasoning},
  volume = {82},
  pages = {242--269},
  issn = {0888-613X},
  doi = {10.1016/j.ijar.2016.12.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0888613X16303474},
  urldate = {2023-10-20},
  abstract = {Argumentation theory is a powerful paradigm that formalizes a type of commonsense reasoning that aims to simulate the human ability to resolve a specific problem in an intelligent manner. A classical argumentation process takes into account only the properties related to the intrinsic logical soundness of an argument in order to determine its acceptability status. However, these properties are not always the only ones that matter to establish the argument's acceptability—there exist other qualities, such as strength, weight, social votes, trust degree, relevance level, and certainty degree, among others. In this work, we redefine the argumentative process to improve the analysis of arguments by considering their special features in order to obtain more refined results. Towards this end, we propose adding meta-level information to the arguments in the form of labels representing quantifiable data ranking over a range of fuzzy valuations. These labels are propagated through an argumentative graph according to the relations of support, conflict, and aggregation between arguments. Through this process we obtain final labels that are useful in determining argument acceptability.}
}

@inproceedings{Budzynska2019AdvancesArgumentMining,
  title = {Advances in {{Argument Mining}}},
  author = {Budzynska, Katarzyna and Reed, Chris},
  date = {2019-07},
  pages = {39--42},
  url = {https://www.aclweb.org/anthology/P19-4008/},
  urldate = {2019-08-25},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Tutorial Abstracts}}},
  langid = {american}
}

@article{Bunke1983InexactGraphMatching,
  title = {Inexact Graph Matching for Structural Pattern Recognition},
  author = {Bunke, H and Allermann, G},
  date = {1983-05-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {1},
  number = {4},
  pages = {245--253},
  issn = {0167-8655},
  doi = {10.1016/0167-8655(83)90033-8},
  url = {https://www.sciencedirect.com/science/article/pii/0167865583900338},
  urldate = {2025-05-16},
  abstract = {This paper is concerned with the inexact matching of attributed, relational graphs for structural pattern recognition. The matching procedure is based on a state space search utilizing heuristic information. Some experimental results are reported.}
}

@article{Bunke1997RelationGraphEdit,
  title = {On a Relation between Graph Edit Distance and Maximum Common Subgraph},
  author = {Bunke, H.},
  date = {1997-08-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {18},
  number = {8},
  pages = {689--694},
  issn = {0167-8655},
  doi = {10.1016/S0167-8655(97)00060-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865597000603},
  urldate = {2025-05-20},
  abstract = {In approximate, or error-correcting, graph matching one considers a set of graph edit operations, and defines the edit distance of two graphs g1 and g2 as the shortest (or least cost) sequence of edit operations that transform g1 into g2. A maximum common subgraph of two graphs g1 and g2 is a subgraph of both g1 and g2 such that there is no other subgraph of g1 and g2 with more nodes. Graph edit distance and maximum common subgraph are well known concepts that have various applications in pattern recognition and machine vision. In this paper a particular cost function for graph edit distance is introduced, and it is shown that under this cost function graph edit distance computation is equivalent to the maximum common subgraph problem.}
}

@inproceedings{Bunke1998ErrortolerantGraphMatching,
  title = {Error-Tolerant Graph Matching: {{A}} Formal Framework and Algorithms},
  shorttitle = {Error-Tolerant Graph Matching},
  booktitle = {Advances in {{Pattern Recognition}}},
  author = {Bunke, H.},
  editor = {Amin, Adnan and Dori, Dov and Pudil, Pavel and Freeman, Herbert},
  date = {1998},
  pages = {1--14},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/BFb0033223},
  abstract = {This paper first reviews some theoretical results in error-tolerant graph matching that were obtained recently. The results include a new metric for error-tolerant graph matching based on maximum common subgraph, a relation between maximum common subgraph and graph edit distance, and the existence of classes of cost functions for error-tolerant graph matching. Then some new optimal algorithms for error-tolerant graph matching are discussed. Under specific conditions, the new algorithms may be significantly more efficient than traditional methods.},
  isbn = {978-3-540-68526-5},
  langid = {english}
}

@thesis{Burkert2023MultitaskTransformerModel,
  title = {A Multi-Task Transformer Model for Argument Graph Construction},
  author = {Burkert, Nico},
  date = {2023-03-25},
  institution = {Hochschule RheinMain},
  location = {Wiesbaden, Germany}
}

@incollection{Burkhard2001NotionSimilarityCase,
  title = {On the {{Notion}} of {{Similarity}} in {{Case Based Reasoning}} and {{Fuzzy Theory}}},
  booktitle = {Soft {{Computing}} in {{Case Based Reasoning}}},
  author = {Burkhard, Hans-Dieter and Richter, Michael M.},
  editor = {Pal, Sankar K. and Dillon, Tharam S. and Yeung, Daniel S.},
  date = {2001},
  pages = {29--45},
  publisher = {Springer London},
  location = {London},
  doi = {10.1007/978-1-4471-0687-6_2},
  abstract = {Notions of similarity and neighborhood play an important role in informatics. Different disciplines have developed their own treatment of related measures. We consider this problem under the viewpoint of case based reasoning and fuzzy theory. While distance and similarity can be considered to be formally equivalent, there exist some differences concerning their intuitive use which have impact on the composition of global measures from local ones.},
  isbn = {978-1-4471-0687-6},
  langid = {english}
}

@article{Busse2015ActuallyWhatDoes,
  title = {Actually, {{What Does}} "{{Ontology}}" {{Mean}}? {{A Term Coined}} by {{Philosophy}} in the {{Light}} of {{Different Scientific Disciplines}}},
  author = {Busse, Johannes and Humm, Bernhard and Lübbert, Christoph and Moelter, Frank and Reibold, Anatol and Rewald, Matthias and Schlüter, Veronika and Seiler, Bernhard and Tegtmeier, Erwin and Zeh, Thomas},
  date = {2015-03-01},
  journaltitle = {Journal of Computing and Information Technology},
  shortjournal = {CIT},
  volume = {23},
  number = {1},
  pages = {29--41},
  doi = {10.2498/cit.1002508},
  url = {http://cit.srce.unizg.hr/index.php/CIT/article/view/2508},
  urldate = {2018-09-01},
  abstract = {This article is a fictitious, moderated dialogue between an information scientist, a philosopher, and a psychologist. They explore the term “ontology” from the point of view of their own discipline, with the object of learning from each other. The target audience of this article are laypersons with respect to the specific disciplines – but who have a scientific background. The authors work in the fields of computer science, knowledge engineering, electrical engineering, mathematics, neurobiology, philosophy, and psychology.}
}

@inproceedings{Cabrio2012CombiningTextualEntailment,
  title = {Combining {{Textual Entailment}} and {{Argumentation Theory}} for {{Supporting Online Debates Interactions}}},
  booktitle = {Proceedings of the 50th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Cabrio, Elena and Villata, Serena},
  date = {2012-07},
  pages = {208--212},
  publisher = {Association for Computational Linguistics},
  location = {Jeju Island, Korea},
  url = {https://www.aclweb.org/anthology/P12-2041},
  urldate = {2019-11-18},
  eventtitle = {{{ACL}} 2012}
}

@inproceedings{Cabrio2013DetectingBipolarSemantic,
  title = {Detecting {{Bipolar Semantic Relations}} among {{Natural Language Arguments}} with {{Textual Entailment}}: A {{Study}}.},
  shorttitle = {Detecting {{Bipolar Semantic Relations}} among {{Natural Language Arguments}} with {{Textual Entailment}}},
  booktitle = {Proceedings of the {{Joint Symposium}} on {{Semantic Processing}}. {{Textual Inference}} and {{Structures}} in {{Corpora}}},
  author = {Cabrio, Elena and Villata, Serena},
  date = {2013-11},
  pages = {24--32},
  location = {Trento, Italy},
  url = {https://aclanthology.org/W13-3815},
  urldate = {2023-10-20}
}

@article{Cabrio2013NaturalLanguageBipolar,
  title = {A Natural Language Bipolar Argumentation Approach to Support Users in Online Debate Interactions},
  author = {Cabrio, Elena and Villata, Serena},
  date = {2013-09},
  journaltitle = {Argument \& Computation},
  shortjournal = {Argument \& Computation},
  volume = {4},
  number = {3},
  pages = {209--230},
  issn = {1946-2166, 1946-2174},
  doi = {10.1080/19462166.2013.862303},
  url = {http://content.iospress.com/doi/10.1080/19462166.2013.862303},
  urldate = {2023-10-20},
  langid = {english}
}

@inproceedings{Cabrio2018FiveYearsArgument,
  title = {Five Years of Argument Mining: A Data-Driven Analysis},
  shorttitle = {Five Years of Argument Mining},
  booktitle = {Proceedings of the 27th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Cabrio, Elena and Villata, Serena},
  date = {2018-07-13},
  series = {{{IJCAI}}'18},
  pages = {5427--5433},
  publisher = {AAAI Press},
  location = {Stockholm, Sweden},
  doi = {10.24963/ijcai.2018/766},
  abstract = {Argument mining is the research area aiming at extracting natural language arguments and their relations from text, with the final goal of providing machine-processable structured data for computational models of argument. This research topic has started to attract the attention of a small community of researchers around 2014, and it is nowadays counted as one of the most promising research areas in Artificial Intelligence in terms of growing of the community, funded projects, and involvement of companies. In this paper, we present the argument mining tasks, and we discuss the obtained results in the area from a data-driven perspective. An open discussion highlights the main weaknesses suffered by the existing work in the literature, and proposes open challenges to be faced in the future.},
  isbn = {978-0-9992411-2-7}
}

@article{Cai2018ComprehensiveSurveyGraph,
  title = {A {{Comprehensive Survey}} of {{Graph Embedding}}: {{Problems}}, {{Techniques}}, and {{Applications}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Graph Embedding}}},
  author = {Cai, HongYun and Zheng, Vincent W. and Chang, Kevin Chen-Chuan},
  date = {2018-09},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {30},
  number = {9},
  pages = {1616--1637},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2018.2807452},
  url = {https://ieeexplore.ieee.org/document/8294302},
  urldate = {2025-05-15},
  abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios.}
}

@inproceedings{Campello2013DensityBasedClusteringBased,
  title = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {160--172},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37456-2_14},
  abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a “flat” partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
  isbn = {978-3-642-37456-2},
  langid = {english}
}

@inproceedings{Campos2018TextFeatureBased,
  title = {A {{Text Feature Based Automatic Keyword Extraction Method}} for {{Single Documents}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Campos, Ricardo and Mangaravite, Vítor and Pasquali, Arian and Jorge, Alípio Mário and Nunes, Célia and Jatowt, Adam},
  editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {684--691},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-76941-7_63},
  abstract = {In this work, we propose a lightweight approach for keyword extraction and ranking based on an unsupervised methodology to select the most important keywords of a single document. To understand the merits of our proposal, we compare it against RAKE, TextRank and SingleRank methods (three well-known unsupervised approaches) and the baseline TF.IDF, over four different collections to illustrate the generality of our approach. The experimental results suggest that extracting keywords from documents using our method results in a superior effectiveness when compared to similar approaches.},
  isbn = {978-3-319-76941-7},
  langid = {english}
}

@inproceedings{Campos2018YAKECollectionIndependentAutomatic,
  title = {{{YAKE}}! {{Collection-Independent Automatic Keyword Extractor}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Campos, Ricardo and Mangaravite, Vítor and Pasquali, Arian and Jorge, Alípio Mário and Nunes, Célia and Jatowt, Adam},
  editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {806--810},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-76941-7_80},
  abstract = {In this paper, we present YAKE!, a novel feature-based system for multi-lingual keyword extraction from single documents, which supports texts of different sizes, domains or languages. Unlike most systems, YAKE! does not rely on dictionaries or thesauri, neither it is trained against any corpora. Instead, we follow an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in many different languages without the need for external knowledge. This can be beneficial for a large number of tasks and a plethora of situations where the access to training corpora is either limited or restricted. In this demo, we offer an easy to use, interactive session, where users from both academia and industry can try our system, either by using a sample document or by introducing their own text. As an add-on, we compare our extracted keywords against the output produced by the IBM Natural Language Understanding (IBM NLU) and Rake system. YAKE! demo is available at http://bit.ly/YakeDemoECIR2018. A python implementation of YAKE! is also available at PyPi repository (https://pypi.python.org/pypi/yake/).},
  isbn = {978-3-319-76941-7},
  langid = {english}
}

@article{Campos2020YAKEKeywordExtraction,
  title = {{{YAKE}}! {{Keyword}} Extraction from Single Documents Using Multiple Local Features},
  author = {Campos, Ricardo and Mangaravite, Vítor and Pasquali, Arian and Jorge, Alípio and Nunes, Célia and Jatowt, Adam},
  date = {2020-01-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {509},
  pages = {257--289},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.09.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025519308588},
  urldate = {2020-04-27},
  abstract = {As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains.},
  langid = {english}
}

@misc{CanonicalDebateLab2020ArgumentTechnologies,
  title = {Argument {{Technologies}}},
  author = {{Canonical Debate Lab}},
  date = {2020},
  url = {https://docs.google.com/spreadsheets/d/1wQShF0J3lGmIFACTvt5FLQWnKc1YdRaaCEDvRJlcUT8},
  urldate = {2023-10-18}
}

@inproceedings{Cao2023AutoAMEndToEndNeural,
  title = {{{AutoAM}}: {{An End-To-End Neural Model}} for~{{Automatic}} and~{{Universal Argument Mining}}},
  shorttitle = {{{AutoAM}}},
  booktitle = {Advanced {{Data Mining}} and {{Applications}}},
  author = {Cao, Lang},
  editor = {Yang, Xiaochun and Suhartanto, Heru and Wang, Guoren and Wang, Bin and Jiang, Jing and Li, Bing and Zhu, Huaijie and Cui, Ningning},
  date = {2023},
  pages = {517--531},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-46674-8_36},
  abstract = {Argument mining is to analyze argument structure and extract important argument information from unstructured text. An argument mining system can help people automatically gain causal and logical information behind the text. As argumentative corpus gradually increases, like more people begin to argue and debate on social media, argument mining from them is becoming increasingly critical. However, argument mining is still a big challenge in natural language tasks due to its difficulty, and relative techniques are not mature. For example, research on non-tree argument mining needs to be done more. Most works just focus on extracting tree structure argument information. Moreover, current methods cannot accurately describe and capture argument relations and do not predict their types. In this paper, we propose a novel neural model called AutoAM to solve these problems. We first introduce the argument component attention mechanism in our model. It can capture the relevant information between argument components, so our model can better perform argument mining. Our model is a universal end-to-end framework, which can analyze argument structure without constraints like tree structure and complete three subtasks of argument mining in one model. The experiment results show that our model outperforms the existing works on several metrics in two public datasets.},
  isbn = {978-3-031-46674-8},
  langid = {english}
}

@inproceedings{Carbonell1983DerivationalAnalogyIts,
  title = {Derivational Analogy and Its Role in Problem Solving},
  booktitle = {Proceedings of the {{Third AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Carbonell, Jaime G.},
  date = {1983-08-22},
  series = {{{AAAI}}'83},
  pages = {64--69},
  publisher = {AAAI Press},
  location = {Washington, D.C.},
  url = {https://www.aaai.org/Papers/AAAI/1983/AAAI83-021.pdf},
  urldate = {2020-05-26},
  abstract = {Derivational analogy, a method of solving problems based upon the transfer of past experience to new problem situations, is discussed in the context of other general approaches to problem solving. The experience transfer process consists of recreating lines of reasoning, including decision sequences and accompanying justifications, that proved effective in solving particular problems requiring similar initial analysis. The derivational analogy approach is advocated as a means of implementing reasoning from individual cases in expert systems.}
}

@incollection{Carbonell1983LearningAnalogyFormulating,
  title = {Learning by {{Analogy}}: {{Formulating}} and {{Generalizing Plans}} from {{Past Experience}}},
  shorttitle = {Learning by {{Analogy}}},
  booktitle = {Machine {{Learning}}: {{An Artificial Intelligence Approach}}},
  author = {Carbonell, Jaime G.},
  editor = {Michalski, Ryszard S. and Carbonell, Jaime G. and Mitchell, Tom M.},
  date = {1983},
  series = {Symbolic {{Computation}}},
  volume = {1},
  pages = {137--161},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-12405-5_5},
  url = {https://doi.org/10.1007/978-3-662-12405-5_5},
  urldate = {2020-05-26},
  abstract = {Analogical reasoning is a powerful mechanism for exploiting past experience in planning and problem solving. This chapter outlines a theory of analogical problem solving based on an extension to means-ends analysis. An analogical transformation process is developed to extract knowledge from past successful problem-solving situations that bear a strong similarity to the current problem. Then, the investigation focuses on exploiting and extending the analogical reasoning model to generate useful exemplary solutions to related problems from which more general plans can be induced and refined. Starting with a general analogical inference engine, problem-solving experience is, in essence, compiled incrementally into effective procedures that solve various classes of problems in an increasingly reliable and direct manner.},
  isbn = {978-3-662-12405-5},
  langid = {english}
}

@incollection{Carbonell1986DerivationalAnalogyTheory,
  title = {Derivational {{Analogy}}: {{A}} Theory of Reconstructive Problem Solving and Expertise Acquisition},
  shorttitle = {Derivational {{Analogy}}},
  booktitle = {Machine {{Learning}}: {{An Artificial Intelligence Approach}}},
  author = {Carbonell, Jaime G.},
  editor = {Michalski, Ryszard S. and Carbonell, Jaime G. and Mitchell, Tom M.},
  date = {1986},
  series = {Symbolic {{Computation}}},
  volume = {2},
  pages = {26},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  abstract = {Derivational analogy, a method of solving problems based on the transfer of past experience to new probiem situations, is discussed in the context of other general approaches to problem solving. The experience transfer process consists of recreating lines of reasoning, including decision sequences and accompanying justifications, that proved effective in solving particular problems requiring similar initial analysis. The role of derivational analogy in case-based reasoning and in automated expertise acquisition is discussed.},
  langid = {english}
}

@article{Cardoso2023ArgumentationModelsTheir,
  title = {Argumentation Models and Their Use in Corpus Annotation: {{Practice}}, Prospects, and Challenges},
  shorttitle = {Argumentation Models and Their Use in Corpus Annotation},
  author = {Cardoso, Henrique Lopes and Sousa-Silva, Rui and Carvalho, Paula and Martins, Bruno},
  date = {2023-07},
  journaltitle = {Natural Language Engineering},
  volume = {29},
  number = {4},
  pages = {1150--1187},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324923000062},
  url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/argumentation-models-and-their-use-in-corpus-annotation-practice-prospects-and-challenges/6C529558653EE306B4B896D31EB3D224},
  urldate = {2025-09-07},
  abstract = {The study of argumentation is transversal to several research domains, from philosophy to linguistics, from the law to computer science and artificial intelligence. In discourse analysis, several distinct models have been proposed to harness argumentation, each with a different focus or aim. To analyze the use of argumentation in natural language, several corpora annotation efforts have been carried out, with a more or less explicit grounding on one of such theoretical argumentation models. In fact, given the recent growing interest in argument mining applications, argument-annotated corpora are crucial to train machine learning models in a supervised way. However, the proliferation of such corpora has led to a wide disparity in the granularity of the argument annotations employed. In this paper, we review the most relevant theoretical argumentation models, after which we survey argument annotation projects closely following those theoretical models. We also highlight the main simplifications that are often introduced in practice. Furthermore, we glimpse other annotation efforts that are not so theoretically grounded but instead follow a shallower approach. It turns out that most argument annotation projects make their own assumptions and simplifications, both in terms of the textual genre they focus on and in terms of adapting the adopted theoretical argumentation model for their own agenda. Issues of compatibility among argument-annotated corpora are discussed by looking at the problem from a syntactical, semantic, and practical perspective. Finally, we discuss current and prospective applications of models that take advantage of argument-annotated corpora.},
  langid = {english}
}

@inproceedings{Caro-Martinez2024UseCaseSpecificReuse,
  title = {Use {{Case-Specific Reuse}} of~{{XAI Strategies}}: {{Design}} and~{{Analysis Through}} an~{{Evaluation Metrics Library}}},
  shorttitle = {Use {{Case-Specific Reuse}} of~{{XAI Strategies}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Caro-Martínez, Marta and Darias, Jesús M. and Díaz-Agudo, Belén and Recio-García, Juan A.},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {81--95},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_6},
  abstract = {Nowadays, we have access to a good number of eXplainable Artificial Intelligence libraries and techniques aimed at providing explanations for users to comprehend black-box intelligent systems. However, this presents a double-edged sword: while we can access a wide catalogue of explanation possibilities, determining the most suitable explanation method for a specific situation remains a challenging decision-making task. The iSee project was conceived with the primary goal of constructing a platform where users can share their own experiences with explanations and their successful explanation strategies. Through this CBR platform, other users can leverage these solutions for their own explanation needs, obtaining the most suitable explanation solutions regarding their requirements. In this paper, our focus lies on the reuse step of the CBR cycle. We have developed and implemented constructive reuse approaches, consisting of various methods to assist design users in adapting their solutions to specific use cases and end users. We have validated the applicability of the resulting solutions and introduced an evaluation metrics library designed to assess explanation strategies. Using this library, we have evaluated system solutions based on various key features including computational complexity, popularity, uniformity, diversity, serendipity, and granularity.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Carvallo2019UseNontechnicalRequirements,
  title = {On the {{Use}} of {{Non-technical Requirements}} for the {{Evaluation}} of {{FOSS Software Components}}},
  booktitle = {Quality of {{Information}} and {{Communications Technology}}},
  author = {Carvallo, Juan Pablo and Carvajal, Fabián and Crespo, Esteban and Mendez, Lucia and Torres, María José and Vintimilla, Rosalva},
  editor = {Piattini, Mario and Rupino da Cunha, Paulo and García Rodríguez de Guzmán, Ignacio and Pérez-Castillo, Ricardo},
  date = {2019},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {64--78},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-29238-6_5},
  abstract = {Modern enterprises rely on Information Systems specifically designed to manage the increasing complexity of their operation. In the usual case, they are built as hybrid systems which integrate several software components of different nature and origins e.g.; legacy systems, web services, commercial components (typically referred as COTS) and, Free and/or Open Source Software (FOSS). The evaluation of individual software components is highly relevant in this kind of system and is usually conducted with the support of software Quality Models. However, these artifacts usually consider only the evaluation of technical quality requirements, in detriment of non-technical ones (e.g. costs, legal and quality of suppliers) which can be just as critical, particularly in the selection of COTS and FOSS. In this paper, we propose an extension to preexisting software Quality Models, intended to deal with technical and non-technical quality requirements in a homogeneous and holistic way. The relevance of the approach is illustrated in relation to four industrial FOSS adoption processes.},
  isbn = {978-3-030-29238-6},
  langid = {english}
}

@unpublished{Castrillo2018DynamicStructuralSimilarity,
  title = {Dynamic {{Structural Similarity}} on {{Graphs}}},
  author = {Castrillo, Eduar and León, Elizabeth and Gómez, Jonatan},
  date = {2018-05-03},
  eprint = {1805.01419},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {One way of characterizing the topological and structural properties of vertices and edges in a graph is by using structural similarity measures. Measures like Cosine, Jaccard and Dice compute the similarities restricted to the immediate neighborhood of the vertices, bypassing important structural properties beyond the locality. Others measures, such as the generalized edge clustering coefficient, go beyond the locality but with high computational complexity, making them impractical in large-scale scenarios. In this paper we propose a novel similarity measure that determines the structural similarity by dynamically diffusing and capturing information beyond the locality. This new similarity is modeled as an iterated function that can be solved by fixed point iteration in super-linear time and memory complexity, so it is able to analyze large-scale graphs. In order to show the advantages of the proposed similarity in the community detection task, we replace the local structural similarity used in the SCAN algorithm with the proposed similarity measure, improving the quality of the detected community structure and also reducing the sensitivity to the parameter \$\textbackslash epsilon\$ of the SCAN algorithm.}
}

@book{Caumanns1999FastSimpleStemming,
  title = {A {{Fast}} and {{Simple Stemming Algorithm}} for {{German Words}}},
  author = {Caumanns, Jörg},
  date = {1999},
  eprint = {iF3cGwAACAAJ},
  eprinttype = {googlebooks},
  publisher = {Freie Univ., Fachbereich Mathematik und Informatik},
  location = {Berlin},
  abstract = {In this paper I present a stemming algorithm for morphological complex languages like German or Dutch. The main idea is not to use stems as common forms in order to make the algorithm simple and fast. The algorithm consists of two steps: First certain characters and/or character sequences are substituted. This step takes linguistic rules and statistical heuristics into account. In a second step a very simple, context free suffix-stripping algorithm is applied. Three variations of the algorithm are described in this paper. The simplest one can easily be implemented with 50 lines of C++ code while the most complex one requires about 100 lines of code and a small wordlist. The algorithm is scalable in a way that linguistic rules and statistical heuristics can be added.},
  langid = {english},
  pagetotal = {10}
}

@inproceedings{Cavnar1994NGramBasedTextCategorization,
  title = {N-{{Gram-Based Text Categorization}}},
  booktitle = {In {{Proceedings}} of {{SDAIR-94}}, 3rd {{Annual Symposium}} on {{Document Analysis}} and {{Information Retrieval}}},
  author = {Cavnar, William and Trenkle, John M.},
  date = {1994},
  pages = {161--175},
  abstract = {Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8\% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80\% correct classification...}
}

@inproceedings{Cayrol2005AcceptabilityArgumentsBipolar,
  title = {On the {{Acceptability}} of {{Arguments}} in {{Bipolar Argumentation Frameworks}}},
  booktitle = {Symbolic and {{Quantitative Approaches}} to {{Reasoning}} with {{Uncertainty}}},
  author = {Cayrol, C. and Lagasquie-Schiex, M. C.},
  editor = {Godo, Lluís},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {378--389},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11518655_33},
  abstract = {In this paper, we extend the basic abstract argumentation framework proposed by Dung, by taking into account two independent kinds of interaction between arguments: a defeat relation and a support relation. In that new framework, called a bipolar argumentation framework, we focus on the concept of acceptability and propose new semantics defined from characteristic properties that a set of arguments must satisfy in order to be an output of the argumentation process. We generalize the well-known stable and preferred semantics by enforcing the coherence requirement for an acceptable set of arguments.},
  isbn = {978-3-540-31888-0},
  langid = {english}
}

@article{Cayrol2013BipolarityArgumentationGraphs,
  title = {Bipolarity in Argumentation Graphs: {{Towards}} a Better Understanding},
  shorttitle = {Bipolarity in Argumentation Graphs},
  author = {Cayrol, Claudette and Lagasquie-Schiex, Marie-Christine},
  date = {2013-09-01},
  journaltitle = {International Journal of Approximate Reasoning},
  shortjournal = {International Journal of Approximate Reasoning},
  series = {Special Issue: {{Uncertainty}} in {{Artificial Intelligence}} and {{Databases}}},
  volume = {54},
  number = {7},
  pages = {876--899},
  issn = {0888-613X},
  doi = {10.1016/j.ijar.2013.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0888613X13000509},
  urldate = {2023-10-25},
  abstract = {Different abstract argumentation frameworks have been used for various applications within multi-agents systems. Among them, bipolar frameworks make use of both attack and support relations between arguments. However, there is no single interpretation of the support, and the handling of bipolarity cannot avoid a deeper analysis of the notion of support. In this paper we consider three recent proposals for specializing the support relation in abstract argumentation: the deductive support, the necessary support and the evidential support. These proposals have been developed independently within different frameworks. We restate these proposals in a common setting, which enables us to undertake a comparative study of the modellings obtained for the three variants of the support. We highlight relationships and differences between these variants, namely a kind of duality between the deductive and the necessary interpretations of the support.}
}

@misc{CentreForArgumentTechnology2017QuickStartGuide,
  title = {A {{Quick Start Guide}} to {{Inference Anchoring Theory}} ({{IAT}})},
  author = {{Centre for Argument Technology}},
  date = {2017-10-01},
  url = {www.arg-tech.org},
  urldate = {2018-09-01}
}

@misc{CentreForArgumentTechnology2022IATAnnotationGuidelines,
  title = {{{IAT}} Annotation Guidelines for {{QT30}}},
  author = {{Centre for Argument Technology}},
  date = {2022-05-02},
  url = {https://www.arg.tech/f/IATannotationguidelines-2022-05.pdf},
  urldate = {2024-09-16}
}

@misc{CentreForArgumentTechnology2023QuickStartGuide,
  title = {A {{Quick Start Guide}} to {{Inference Anchoring Theory}} ({{IAT}})},
  author = {{Centre for Argument Technology}},
  date = {2023-09-25},
  url = {https://www.arg.tech/f/IAT_guidelines_and_tutorials-2023-10.pdf},
  urldate = {2024-09-16}
}

@article{Cer2017SemEval2017TaskSemantic,
  title = {{{SemEval-2017 Task}} 1 - {{Semantic Textual Similarity}} - {{Multilingual}} and {{Cross-lingual Focused Evaluation}}.},
  author = {Cer, Daniel M and Diab, Mona T and Agirre, Eneko and Lopez-Gazpio, Iñigo and Specia, Lucia},
  date = {2017-01-01},
  journaltitle = {CoRR}
}

@unpublished{Cer2018UniversalSentenceEncoder,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  date = {2018-04-12},
  eprint = {1803.11175},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.11175},
  urldate = {2020-08-02},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.}
}

@inproceedings{Chakrabarty2019AMPERSANDArgumentMining,
  title = {{{AMPERSAND}}: {{Argument Mining}} for {{PERSuAsive oNline Discussions}}},
  shorttitle = {{{AMPERSAND}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Chakrabarty, Tuhin and Hidey, Christopher and Muresan, Smaranda and McKeown, Kathy and Hwang, Alyssa},
  date = {2019-11},
  pages = {2933--2943},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-1291},
  url = {https://www.aclweb.org/anthology/D19-1291},
  urldate = {2020-09-02},
  abstract = {Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macro-level (argument as process) models of argumentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contextual information in terms of fine-tuning a pre-trained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of one's argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-the-art approaches using pointer networks and a pre-trained language model.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019}
}

@inproceedings{Chalaguine2020PersuasiveChatbotUsing,
  title = {A {{Persuasive Chatbot Using}} a {{CrowdSourced Argument Graph}} and {{Concerns}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Computational Models}} of {{Argument}}},
  author = {Chalaguine, Lisa A. and Hunter, Anthony},
  editor = {Prakken, Henry and Bistarelli, Stefano and Santini, Francesco and Taticchi, Carlo},
  date = {2020},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {326},
  pages = {9--20},
  publisher = {IOS Press},
  location = {Perugia, Italy},
  doi = {10.3233/FAIA200487},
  abstract = {Chatbots are versatile tools that have the potential of being used for computational persuasion where the chatbot acts as the persuader and the human agent as the persuadee. To allow the user to type his or her arguments, as opposed to selecting them from a menu, the chatbot needs a sufficiently large knowledge base of arguments and counterarguments. And in order to make the user change their current stance on a subject, the chatbot needs a method to select persuasive counterarguments. To address this, we present a chatbot that is equipped with an argument graph and the ability to identify the concerns of the user argument in order to select appropriate counterarguments. We evaluate the bot in a study with participants and show how using our method can make the chatbot more persuasive.},
  eventtitle = {Computational {{Models}} of {{Argument}}}
}

@inproceedings{Chen2019TabFactLargescaleDataset,
  title = {{{TabFact}}: {{A Large-scale Dataset}} for {{Table-based Fact Verification}}},
  shorttitle = {{{TabFact}}},
  author = {Chen, Wenhu and Wang, Hongmin and Chen, Jianshu and Zhang, Yunkai and Wang, Hong and Li, Shiyang and Zhou, Xiyou and Wang, William Yang},
  date = {2019-09-23},
  url = {https://openreview.net/forum?id=rkeJRhNYDH},
  urldate = {2025-07-15},
  abstract = {The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains unexplored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into LISP-like programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{Chen2020BigSelfsupervisedModels,
  title = {Big Self-Supervised Models Are Strong Semi-Supervised Learners},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-12-06},
  series = {{{NIPS}} '20},
  pages = {22243--22255},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (≤13 labeled images per class) using ResNet-50, a 10 x improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  isbn = {978-1-7138-2954-6}
}

@inproceedings{Chen2020SimpleFrameworkContrastive,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-11-21},
  pages = {1597--1607},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/chen20j.html},
  urldate = {2025-07-15},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{Chen2021Election2020FirstPublic,
  title = {\#{{Election2020}}: The First Public {{Twitter}} Dataset on the 2020 {{US Presidential}} Election},
  shorttitle = {\#{{Election2020}}},
  author = {Chen, Emily and Deb, Ashok and Ferrara, Emilio},
  date = {2021-04-02},
  journaltitle = {Journal of Computational Social Science},
  shortjournal = {J Comput Soc Sc},
  issn = {2432-2725},
  doi = {10.1007/s42001-021-00117-9},
  url = {https://doi.org/10.1007/s42001-021-00117-9},
  urldate = {2022-02-06},
  abstract = {Credible evidence-based political discourse is a critical pillar of democracy and is at the core of guaranteeing free and fair elections. The study of online chatter is paramount, especially in the wake of important voting events like the recent November 3, 2020 U.S. Presidential election and the inauguration on January 21, 2021. Limited access to social media data is often the primary obstacle that limits our abilities to study and understand online political discourse. To mitigate this impediment and empower the Computational Social Science research community, we are publicly releasing a massive-scale, longitudinal dataset of U.S. politics- and election-related tweets. This multilingual dataset encompasses over 1.2 billion tweets and tracks all salient U.S. political trends, actors, and events from 2019 to the time of this writing. It predates and spans the entire period of the Republican and Democratic primaries, with real-time tracking of all presidential contenders on both sides of the aisle. The dataset also focuses on presidential and vice-presidential candidates, the presidential elections and the transition from the Trump administration to the Biden administration. Our dataset release is curated, documented, and will continue to track relevant events. We hope that the academic community, computational journalists, and research practitioners alike will all take advantage of our dataset to study relevant scientific and social issues, including problems like misinformation, information manipulation, conspiracies, and the distortion of online political discourse that has been prevalent in the context of recent election events in the United States. Our dataset is available at: https://github.com/echen102/us-pres-elections-2020.},
  langid = {english}
}

@online{Chen2023ExploringPotentialLarge,
  title = {Exploring the {{Potential}} of {{Large Language Models}} in {{Computational Argumentation}}},
  author = {Chen, Guizhen and Cheng, Liying and Tuan, Luu Anh and Bing, Lidong},
  date = {2023-11-15},
  eprint = {2311.09022},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09022},
  url = {http://arxiv.org/abs/2311.09022},
  urldate = {2024-02-10},
  abstract = {Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing (NLP) that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into 6 main classes and standardise the format of 14 open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of these datasets, demonstrating their capabilities in the field of argumentation. We also highlight the limitations in evaluating computational argumentation and provide suggestions for future research directions in this field.},
  pubstate = {prepublished}
}

@inproceedings{Cheng2010PredictingPartialOrders,
  title = {Predicting {{Partial Orders}}: {{Ranking}} with {{Abstention}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Cheng, Weiwei and Rademaker, Michaël and De Baets, Bernard and Hüllermeier, Eyke},
  date = {2010-09-20},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {215--230},
  publisher = {Springer},
  location = {Barcelona, Spain},
  doi = {10.1007/978-3-642-15880-3_20},
  abstract = {The prediction of structured outputs in general and rankings in particular has attracted considerable attention in machine learning in recent years, and different types of ranking problems have...},
  eventtitle = {{{ECML PKDD}}}
}

@inproceedings{Chernodub2019TARGERNeuralArgument,
  title = {{{TARGER}}: {{Neural Argument Mining}} at {{Your Fingertips}}},
  shorttitle = {{{TARGER}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Chernodub, Artem and Oliynyk, Oleksiy and Heidenreich, Philipp and Bondarenko, Alexander and Hagen, Matthias and Biemann, Chris and Panchenko, Alexander},
  date = {2019-07},
  pages = {195--200},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/P19-3031},
  url = {https://www.aclweb.org/anthology/P19-3031},
  urldate = {2020-05-24},
  abstract = {We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user's side. The open source code ensures portability to other domains and use cases.}
}

@article{Chesnevar2006ArgumentInterchangeFormat,
  title = {Towards an Argument Interchange Format.},
  author = {Chesñevar, Carlos Iván and McGinnis, Jarred and Modgil, Sanjay and Rahwan, Iyad and Reed, Chris and Simari, Guillermo Ricardo and South, Matthew and Vreeswijk, Gerard and Willmott, Steven},
  date = {2006-01-01},
  journaltitle = {The Knowledge Engineering Review},
  shortjournal = {The Knowledge Engineering Review},
  volume = {21},
  number = {04},
  pages = {293},
  doi = {10.1017/S0269888906001044},
  url = {http://www.journals.cambridge.org/abstract_S0269888906001044},
  urldate = {2018-09-01},
  abstract = {The theory of argumentation is a rich, interdisciplinary area of research straddling the fields of artificial intelligence, philosophy, communication studies, linguistics and psychology. In the last few years, significant progress has been made in understanding the theoretical properties of different argumentation logics. However, one major barrier to the development and practical deployment of argumentation systems is the lack of a shared, agreed notation or ‘interchange format’ for argumentation and arguments. In this paper, we describe a draft specification for an argument interchange format (AIF) intended for representation and exchange of data between various argumentation tools and agent-based applications. It represents a consensus ‘abstract model’ established by researchers across fields of argumentation, artificial intelligence and multi-agent systems. In its current form, this specification is intended as a starting point for further discussion and elaboration by the community, rather than an attempt at a definitive, all-encompassing model. However, to demonstrate proof of concept, a use case scenario is briefly described. Moreover, three concrete realizations or ‘reifications’ of the abstract model are illustrated.}
}

@inproceedings{Cho2010ReweightedRandomWalks,
  title = {Reweighted {{Random Walks}} for {{Graph Matching}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2010},
  author = {Cho, Minsu and Lee, Jungmin and Lee, Kyoung Mu},
  editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {492--505},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-15555-0_36},
  abstract = {Graph matching is an essential problem in computer vision and machine learning. In this paper, we introduce a random walk view on the problem and propose a robust graph matching algorithm against outliers and deformation. Matching between two graphs is formulated as node selection on an association graph whose nodes represent candidate correspondences between the two graphs. The solution is obtained by simulating random walks with reweighting jumps enforcing the matching constraints on the association graph. Our algorithm achieves noise-robust graph matching by iteratively updating and exploiting the confidences of candidate correspondences. In a practical sense, our work is of particular importance since the real-world matching problem is made difficult by the presence of noise and outliers. Extensive and comparative experiments demonstrate that it outperforms the state-of-the-art graph matching algorithms especially in the presence of outliers and deformation.},
  isbn = {978-3-642-15555-0},
  langid = {english}
}

@unpublished{Chollampatt2018MultilayerConvolutionalEncoderDecoder,
  title = {A {{Multilayer Convolutional Encoder-Decoder Neural Network}} for {{Grammatical Error Correction}}},
  author = {Chollampatt, Shamil and Ng, Hwee Tou},
  date = {2018-01-26},
  eprint = {1801.08831},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.08831},
  urldate = {2020-08-03},
  abstract = {We improve automatic correction of grammatical, orthographic, and collocation errors in text using a multilayer convolutional encoder-decoder neural network. The network is initialized with embeddings that make use of character N-gram information to better suit this task. When evaluated on common benchmark test data sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior neural approaches on this task as well as strong statistical machine translation-based systems with neural and task-specific features trained on the same data. Our analysis shows the superiority of convolutional neural networks over recurrent neural networks such as long short-term memory (LSTM) networks in capturing the local context via attention, and thereby improving the coverage in correcting grammatical errors. By ensembling multiple models, and incorporating an N-gram language model and edit features via rescoring, our novel method becomes the first neural approach to outperform the current state-of-the-art statistical machine translation-based approach, both in terms of grammaticality and fluency.}
}

@article{Chomsky1956ThreeModelsDescription,
  title = {Three Models for the Description of Language},
  author = {Chomsky, N.},
  date = {1956-09},
  journaltitle = {IRE Transactions on Information Theory},
  volume = {2},
  number = {3},
  pages = {113--124},
  issn = {2168-2712},
  doi = {10.1109/TIT.1956.1056813},
  url = {https://ieeexplore.ieee.org/document/1056813},
  urldate = {2025-09-10},
  abstract = {We investigate several conceptions of linguistic structure to determine whether or not they can provide simple and "revealing" grammars that generate all of the sentences of English and only these. We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar. Furthermore, the particular subclass of such processes that producen-order statistical approximations to English do not come closer, with increasingn, to matching the output of an English grammar. We formalize-the notions of "phrase structure" and show that this gives us a method for describing language which is essentially more powerful, though still representable as a rather elementary type of finite-state process. Nevertheless, it is successful only when limited to a small subset of simple sentences. We study the formal properties of a set of grammatical transformations that carry sentences with phrase structure into new sentences with derived phrase structure, showing that transformational grammars are processes of the same elementary type as phrase-structure grammars; that the grammar of English is materially simplified if phrase structure description is limited to a kernel of simple sentences from which all other sentences are constructed by repeated transformations; and that this view of linguistic structure gives a certain insight into the use and understanding of language.}
}

@book{Choudhary1993ElementsComplexAnalysis,
  title = {The {{Elements}} of {{Complex Analysis}}},
  author = {Choudhary, B.},
  date = {1993},
  eprint = {5K9i2YwgTjYC},
  eprinttype = {googlebooks},
  publisher = {New Age International},
  abstract = {This Book Is Intended To Be A Simple And Easy Introduction To The Subject. It Is Meant As A Textbook For A Course In Complex Analysis At Postgraduate Level Of Indian Universities.Some Of The Welcome Features Of The Book Are: Proofs And Motivation For The Theory: Examples Are Provided To Illustrate The Concepts; Exercises Of Various Levels Of Difficulty Are Given At The End Of Every Chapter: Keeping In View The Applied Nature Of The Subject, Ordinary Linear Homogeneous Differential Equations Of The Second Order And Conformal Mapping And Its Applications Are Given More Attention Than Most Other Books: Uniform Approximation And Elliptic Functions Are Treated In Great Detail; There Is Also A Detailed Treatment Of Harmonic Functions, Weierstrass Approximation Theorem, Analytic Continuation, Riemann Mapping Theorem, Homological Version OfCauchys Theorem And Its Applications; Diagrams Are Provided Whenever Feasible To Help The Reader Develop Skill In Using Imagination To Visualise Abstract Ideas; Solutions To Some Selected Exercises Which Involve Lot Of New Ideas And Theoretical Considerations Have Been Provided At The End.},
  isbn = {978-81-224-0399-2},
  langid = {english},
  pagetotal = {352}
}

@inproceedings{Christiano2017DeepReinforcementLearning,
  title = {Deep Reinforcement Learning from Human Preferences},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017-12-04},
  series = {{{NIPS}}'17},
  pages = {4302--4310},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
  isbn = {978-1-5108-6096-4}
}

@book{Cimiano2024RobustArgumentationMachines,
  title = {Robust {{Argumentation Machines}}: {{First International Conference}}, {{RATIO}} 2024, {{Bielefeld}}, {{Germany}}, {{June}} 5–7, 2024, {{Proceedings}}},
  shorttitle = {Robust {{Argumentation Machines}}},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14638},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6},
  url = {https://link.springer.com/10.1007/978-3-031-63536-6},
  urldate = {2024-07-18},
  isbn = {978-3-031-63535-9 978-3-031-63536-6},
  langid = {english}
}

@online{Clavie2025ItsAllMASK,
  title = {It's {{All}} in {{The}} [{{MASK}}]: {{Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers}}},
  shorttitle = {It's {{All}} in {{The}} [{{MASK}}]},
  author = {Clavié, Benjamin and Cooper, Nathan and Warner, Benjamin},
  date = {2025-02-06},
  eprint = {2502.03793},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.03793},
  url = {http://arxiv.org/abs/2502.03793},
  urldate = {2025-02-10},
  abstract = {While encoder-only models such as BERT and ModernBERT are ubiquitous in real-world NLP applications, their conventional reliance on task-specific classification heads can limit their applicability compared to decoder-based large language models (LLMs). In this work, we introduce ModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its masked language modelling (MLM) head for generative classification. Our approach employs an intentionally simple training loop and inference mechanism that requires no heavy pre-processing, heavily engineered prompting, or architectural modifications. ModernBERT-Large-Instruct exhibits strong zero-shot performance on both classification and knowledge-based tasks, outperforming similarly sized LLMs on MMLU and achieving 93\% of Llama3-1B's MMLU performance with 60\% less parameters. We also demonstrate that, when fine-tuned, the generative approach using the MLM head matches or even surpasses traditional classification-head methods across diverse NLU tasks.This capability emerges specifically in models trained on contemporary, diverse data mixes, with models trained on lower volume, less-diverse data yielding considerably weaker performance. Although preliminary, these results demonstrate the potential of using the original generative masked language modelling head over traditional task-specific heads for downstream tasks. Our work suggests that further exploration into this area is warranted, highlighting many avenues for future improvements.},
  pubstate = {prepublished}
}

@inproceedings{Cocarascu2017IdentifyingAttackSupport,
  title = {Identifying Attack and Support Argumentative Relations Using Deep Learning},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Cocarascu, Oana and Toni, Francesca},
  date = {2017-09},
  pages = {1374--1379},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1144},
  url = {https://aclanthology.org/D17-1144},
  urldate = {2022-01-13},
  abstract = {We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.},
  eventtitle = {{{EMNLP}} 2017}
}

@inproceedings{Cocarascu2020DatasetIndependentBaselines,
  title = {Dataset {{Independent Baselines}} for {{Relation Prediction}} in {{Argument Mining}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Computational Models}} of {{Argument}}},
  author = {Cocarascu, Oana and Cabrio, Elena and Villata, Serena and Toni, Christopher},
  editor = {Prakken, Henry and Bistarelli, Stefano and Santini, Francesco and Taticchi, Carlo},
  date = {2020},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {326},
  pages = {45--52},
  publisher = {IOS Press},
  location = {Perugia, Italy},
  doi = {10.3233/FAIA200490},
  abstract = {Argument(ation) Mining (AM) is the research area which aims at extracting argument components and predicting argumentative relations (i.e., support and attack) from text. In particular, numerous approaches have been proposed in the literature to predict the relations holding between arguments, and application-specific annotated resources were built for this purpose. Despite the fact that these resources were created to experiment on the same task, the definition of a single relation prediction method to be successfully applied to a significant portion of these datasets is an open research problem in AM. This means that none of the methods proposed in the literature can be easily ported from one resource to another. In this paper, we address this problem by proposing a set of dataset independent strong neural baselines which obtain homogeneous results on all the datasets proposed in the literature for the argumentative relation prediction task in AM. Thus, our baselines can be employed by the AM community to compare more effectively how well a method performs on the argumentative relation prediction task.},
  eventtitle = {Computational {{Models}} of {{Argument}}}
}

@online{Cocarascu2020DatasetIndependentSet,
  title = {A {{Dataset Independent Set}} of {{Baselines}} for {{Relation Prediction}} in {{Argument Mining}}},
  author = {Cocarascu, Oana and Cabrio, Elena and Villata, Serena and Toni, Francesca},
  date = {2020-02-14},
  eprint = {2003.04970},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.04970},
  url = {http://arxiv.org/abs/2003.04970},
  urldate = {2023-10-20},
  abstract = {Argument Mining is the research area which aims at extracting argument components and predicting argumentative relations (i.e.,support and attack) from text. In particular, numerous approaches have been proposed in the literature to predict the relations holding between the arguments, and application-specific annotated resources were built for this purpose. Despite the fact that these resources have been created to experiment on the same task, the definition of a single relation prediction method to be successfully applied to a significant portion of these datasets is an open research problem in Argument Mining. This means that none of the methods proposed in the literature can be easily ported from one resource to another. In this paper, we address this problem by proposing a set of dataset independent strong neural baselines which obtain homogeneous results on all the datasets proposed in the literature for the argumentative relation prediction task. Thus, our baselines can be employed by the Argument Mining community to compare more effectively how well a method performs on the argumentative relation prediction task.},
  pubstate = {prepublished}
}

@article{Cohen1960CoefficientAgreementNominal,
  title = {A {{Coefficient}} of {{Agreement}} for {{Nominal Scales}}},
  author = {Cohen, Jacob},
  date = {1960-04-01},
  journaltitle = {Educational and Psychological Measurement},
  shortjournal = {Educational and Psychological Measurement},
  volume = {20},
  number = {1},
  pages = {37--46},
  publisher = {SAGE Publications Inc},
  issn = {0013-1644},
  doi = {10.1177/001316446002000104},
  url = {https://doi.org/10.1177/001316446002000104},
  urldate = {2021-03-07},
  langid = {english}
}

@article{Cohen1968WeightedKappaNominal,
  title = {Weighted Kappa: {{Nominal}} Scale Agreement Provision for Scaled Disagreement or Partial Credit.},
  shorttitle = {Weighted Kappa},
  author = {Cohen, Jacob},
  date = {1968},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {70},
  number = {4},
  pages = {213--220},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/h0026256},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0026256},
  urldate = {2021-03-07},
  langid = {english}
}

@inproceedings{Collins2002DiscriminativeTrainingMethods,
  title = {Discriminative {{Training Methods}} for {{Hidden Markov Models}}: {{Theory}} and {{Experiments}} with {{Perceptron Algorithms}}},
  shorttitle = {Discriminative {{Training Methods}} for {{Hidden Markov Models}}},
  booktitle = {Proceedings of the 2002 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}} 2002)},
  author = {Collins, Michael},
  date = {2002-07},
  pages = {1--8},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/1118693.1118694},
  url = {https://www.aclweb.org/anthology/W02-1001},
  urldate = {2021-02-09}
}

@inproceedings{Collins2019ArgumentationbasedApproachExplainable,
  title = {Towards an Argumentation-Based Approach to Explainable Planning},
  booktitle = {Proceedings of the {{Second ICAPS Workshop}} on {{Explainable Planning}}},
  author = {Collins, Anna and Magazzeni, Daniele and Parsons, Simon},
  date = {2019-04-16},
  location = {Berkeley, CA},
  url = {https://openreview.net/forum?id=Byef4anQcE},
  urldate = {2019-09-12},
  abstract = {Providing transparency of AI planning systems is crucial for their success in practical applications. In order to create a transparent system, a user must be able to query it for explanations about its outputs. We argue that a key underlying principle for this is the use of causality within a planning model, and that argumentation frameworks provide an intuitive representation of such causality. In this paper, we discuss how argumentation can aid in extracting causalities from plans and models, and how they can create explanations from them.},
  eventtitle = {{{XAIP-2019}}}
}

@article{Colom2010HumanIntelligenceBrain,
  title = {Human Intelligence and Brain Networks},
  author = {Colom, Roberto and Karama, Sherif and Jung, Rex E. and Haier, Richard J.},
  date = {2010-12},
  journaltitle = {Dialogues in Clinical Neuroscience},
  shortjournal = {Dialogues Clin Neurosci},
  volume = {12},
  number = {4},
  eprint = {21319494},
  eprinttype = {pubmed},
  pages = {489--501},
  issn = {1294-8322},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3181994/},
  urldate = {2021-01-16},
  abstract = {Intelligence can be defined as a general mental ability for reasoning, problem solving, and learning. Because of its general nature, intelligence integrates cognitive functions such as perception, attention, memory, language, or planning. On the basis of this definition, intelligence can be reliably measured by standardized tests with obtained scores predicting several broad social outcomes such as educational achievement, job performance, health, and longevity. A detailed understanding of the brain mechanisms underlying this general mental ability could provide significant individual and societal benefits. Structural and functional neuroimaging studies have generally supported a frontoparietal network relevant for intelligence. This same network has also been found to underlie cognitive functions related to perception, short-term memory storage, and language. The distributed nature of this network and its involvement in a wide range of cognitive functions fits well with the integrative nature of intelligence. A new key phase of research is beginning to investigate how functional networks relate to structural networks, with emphasis on how distributed brain areas communicate with each other.},
  pmcid = {PMC3181994}
}

@inproceedings{Compton1990KnowledgeContextStrategy,
  title = {Knowledge in Context: {{A}} Strategy for Expert System Maintenance},
  shorttitle = {Knowledge in Context},
  booktitle = {{{AI}} '88},
  author = {Compton, P. and Jansen, R.},
  editor = {Barter, Christopher J. and Brooks, Michael J.},
  date = {1990},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {292--306},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-52062-7_86},
  abstract = {Knowledge engineering, obtaining knowledge from experts and incorporating it into expert systems is difficult and time consuming. We suggest that these difficulties arise because experts never report on how they reach a decision, rather they justify why the decision is correct. These justifications vary markedly with the context in which they are required, but in context they are accurate and adequate; the difficulties in knowledge engineering arise from taking the justification out of context. We therefore hypothesise that knowledge engineering may be obviated, particularly in the long term maintenance of expert systems, if the rules experts provide are used in the context in which they are given. This paper describes work in progress to test this hypothesis.},
  isbn = {978-3-540-46875-2},
  langid = {english}
}

@inproceedings{Conneau2017SupervisedLearningUniversal,
  title = {Supervised {{Learning}} of {{Universal Sentence Representations}} from {{Natural Language Inference Data}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loïc and Bordes, Antoine},
  date = {2017-09},
  pages = {670--680},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1070},
  url = {https://www.aclweb.org/anthology/D17-1070},
  urldate = {2020-10-20},
  abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
  eventtitle = {{{EMNLP}} 2017}
}

@inproceedings{Cook2025EfficientAnnotatorReliability,
  title = {Efficient {{Annotator Reliability Assessment}} with {{EffiARA}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Cook, Owen and Vasilakes, Jake A and Roberts, Ian and Song, Xingyi},
  editor = {Mishra, Pushkar and Muresan, Smaranda and Yu, Tao},
  date = {2025-07},
  pages = {542--550},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-demo.52/},
  urldate = {2025-07-29},
  abstract = {Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft-label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk.},
  isbn = {979-8-89176-253-4}
}

@article{Cordella2004SubgraphIsomorphismAlgorithm,
  title = {A (Sub)Graph Isomorphism Algorithm for Matching Large Graphs},
  author = {Cordella, L.P. and Foggia, P. and Sansone, C. and Vento, M.},
  date = {2004-10},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {26},
  number = {10},
  pages = {1367--1372},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2004.75},
  url = {https://ieeexplore.ieee.org/document/1323804},
  urldate = {2025-06-11},
  abstract = {We present an algorithm for graph isomorphism and subgraph isomorphism suited for dealing with large graphs. A first version of the algorithm has been presented in a previous paper, where we examined its performance for the isomorphism of small and medium size graphs. The algorithm is improved here to reduce its spatial complexity and to achieve a better performance on large graphs; its features are analyzed in detail with special reference to time and memory requirements. The results of a testing performed on a publicly available database of synthetically generated graphs and on graphs relative to a real application dealing with technical drawings are presented, confirming the effectiveness of the approach, especially when working with large graphs.}
}

@inproceedings{Cortes2016RelevanceLocalNeighbourhoods,
  title = {On the {{Relevance}} of {{Local Neighbourhoods}} for {{Greedy Graph Edit Distance}}},
  booktitle = {Structural, {{Syntactic}}, and {{Statistical Pattern Recognition}}},
  author = {Cortés, Xavier and Serratosa, Francesc and Riesen, Kaspar},
  editor = {Robles-Kelly, Antonio and Loog, Marco and Biggio, Battista and Escolano, Francisco and Wilson, Richard},
  date = {2016},
  pages = {121--131},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-49055-7_11},
  abstract = {Approximation of graph edit distance based on bipartite graph matching emerged to an important model for distance based graph classification. However, one of the drawbacks of this particular approximation is its cubic runtime with respect to the number of nodes of the graphs. In fact, this runtime restricts the applicability of bipartite graph matching to graphs of rather small size. Recently, a new approximation for graph edit distance using greedy algorithms (rather than optimal bipartite algorithms) has been proposed. This novel algorithm reduces the computational complexity to quadratic order. In another line of research it has been shown that the definition of local neighbourhoods plays a crucial role in bipartite graph matching. These neighbourhoods define the local substructures of the graphs which are eventually assigned to each other. In the present paper we demonstrate that the type of local neighbourhood and in particular the distance model defined on them is also highly relevant for graph classification using greedy graph edit distance.},
  isbn = {978-3-319-49055-7},
  langid = {english}
}

@article{Craven2016ArgumentGraphsAssumptionbased,
  title = {Argument Graphs and Assumption-Based Argumentation},
  author = {Craven, Robert and Toni, Francesca},
  date = {2016-04-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {233},
  pages = {1--59},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2015.12.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370215001800},
  urldate = {2020-04-27},
  abstract = {Arguments in structured argumentation are usually defined as trees, and extensions as sets of such tree-based arguments with various properties depending on the particular argumentation semantics. However, these arguments and extensions may have redundancies as well as circularities, which are conceptually and computationally undesirable. Focusing on the specific case of Assumption-Based Argumentation (ABA), we propose novel notions of arguments and admissible/grounded extensions, both defined in terms of graphs. We show that this avoids the redundancies and circularities of standard accounts, and set out the relationship to standard tree-based arguments and admissible/grounded extensions (as sets of arguments). We also define new notions of graph-based admissible/grounded dispute derivations for ABA, for determining whether specific sentences hold under the admissible/grounded semantics. We show that these new derivations are superior with respect to standard dispute derivations in that they are complete in general, rather than solely for restricted classes of ABA frameworks. Finally, we present several experiments comparing the implementation of graph-based admissible/grounded dispute derivations with implementations of standard dispute derivations, suggesting that the graph-based approach is computationally advantageous.},
  langid = {english}
}

@inproceedings{Croitoru2025ChildRobotInteractionExperiment,
  title = {A {{Child-Robot Interaction Experiment}} to~{{Analyze Gender Stereotypes}} in~the~{{Perception}} of~{{Mathematical Abilities}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Croitoru, Madalina and Laviron, Pablo and Bando, Sio and Gilles, Eric and Miled, Amine and Anders, Royce and Blanc, Nathalie and Ganesh, Gowrishankar and Brigaud, Emmanuelle},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  pages = {232--237},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77918-3_17},
  abstract = {This study examines the use of social robotics in education, focusing on reducing the gender biases child students may have in the perception of their mathematical ability and potential. Our initial pilot with twenty 7-year-olds provides insights into the potential of combining AI with educational strategies. We discuss our findings, and experimental setup involving ChatGPT4, and future research directions.},
  isbn = {978-3-031-77918-3},
  langid = {english}
}

@article{Cui2019Integrative3CEvaluation,
  title = {An {{Integrative 3C}} Evaluation Framework for {{Explainable Artificial Intelligence}}},
  author = {Cui, Xiaocong and Lee, Jung Min and Hsieh, J. Po-An},
  date = {2019-07-06},
  journaltitle = {AMCIS 2019 Proceedings},
  url = {https://aisel.aisnet.org/amcis2019/ai_semantic_for_intelligent_info_systems/ai_semantic_for_intelligent_info_systems/10}
}

@inproceedings{Cyras2016AbstractArgumentationCasebased,
  title = {Abstract Argumentation for Case-Based Reasoning},
  booktitle = {Proceedings of the {{Fifteenth International Conference}} on {{Principles}} of {{Knowledge Representation}} and {{Reasoning}}},
  author = {Čyras, Kristijonas and Satoh, Ken and Toni, Francesca},
  date = {2016-04-25},
  series = {{{KR}}'16},
  pages = {549--552},
  publisher = {AAAI Press},
  location = {Cape Town, South Africa},
  abstract = {We investigate case-based reasoning (CBR) problems where cases are represented by abstract factors and (positive or negative) outcomes, and an outcome for a new case, represented by abstract factors, needs to be established. To this end, we employ abstract argumentation (AA) and propose a novel methodology for CBR, called AA-CBR. The argumentative formulation naturally allows to characterise the computation of an outcome as a dialogical process between a proponent and an opponent, and can also be used to extract explanations for why an outcome for a new case is (not) computed.}
}

@inproceedings{Cyras2016ExplanationCaseBasedReasoning,
  title = {Explanation for {{Case-Based Reasoning}} via {{Abstract Argumentation}}},
  booktitle = {Computational {{Models}} of {{Argument}}},
  author = {Čyras, Kristijonas and Satoh, Ken and Toni, Francesca},
  date = {2016},
  pages = {243--254},
  publisher = {IOS Press},
  doi = {10.3233/978-1-61499-686-6-243},
  url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-686-6-243},
  urldate = {2023-07-26}
}

@inproceedings{Dabbish2012SocialCodingGitHub,
  title = {Social Coding in {{GitHub}}: Transparency and Collaboration in an Open Software Repository},
  shorttitle = {Social Coding in {{GitHub}}},
  booktitle = {Proceedings of the {{ACM}} 2012 Conference on {{Computer Supported Cooperative Work}}},
  author = {Dabbish, Laura and Stuart, Colleen and Tsay, Jason and Herbsleb, Jim},
  date = {2012-02-11},
  series = {{{CSCW}} '12},
  pages = {1277--1286},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2145204.2145396},
  url = {https://doi.org/10.1145/2145204.2145396},
  urldate = {2022-05-03},
  abstract = {Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation.},
  isbn = {978-1-4503-1086-4}
}

@dataset{Dai2017NewsArticles,
  title = {News {{Articles}}},
  author = {Dai, Tianru},
  date = {2017-03-31},
  publisher = {Harvard Dataverse},
  doi = {10.7910/DVN/GMFCTR},
  url = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GMFCTR},
  urldate = {2024-02-10},
  abstract = {This is a reusable publicly-available dataset for “media bias” studies. The content of this dataset is publish date, title, subtitle and text for 3824 news articles. These articles are collected by a project within 3 months from December of 2016 to march 2017. The source of these news articles are from ABC News, CNN news, The Huffington Post, BBC News, DW News, TASS News, Al Jazeera News, China Daily and RTE News. All of them are collected by using RSS feeds of each news sites. (2017-3-31)},
  langid = {english}
}

@online{Darcet2024VisionTransformersNeed,
  title = {Vision {{Transformers Need Registers}}},
  author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  date = {2024-04-12},
  eprint = {2309.16588},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16588},
  url = {http://arxiv.org/abs/2309.16588},
  urldate = {2024-05-12},
  abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
  pubstate = {prepublished}
}

@online{Das2024EndtoEndArgumentMining,
  title = {End-to-{{End Argument Mining}} as {{Augmented Natural Language Generation}}},
  author = {Das, Nilmadhab and Choudhary, Vishal and Saradhi, V. Vijaya and Anand, Ashish},
  date = {2024-06-12},
  eprint = {2406.08606},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2406.08606},
  urldate = {2024-06-24},
  abstract = {Argument Mining (AM) is a crucial aspect of computational argumentation, which deals with the identification and extraction of Argumentative Components (ACs) and their corresponding Argumentative Relations (ARs). Most prior works have solved these problems by dividing them into multiple subtasks. And the available end-to-end setups are mostly based on the dependency parsing approach. This work proposes a unified end-to-end framework based on a generative paradigm, in which the argumentative structures are framed into label-augmented text, called Augmented Natural Language (ANL). Additionally, we explore the role of different types of markers in solving AM tasks. Through different marker-based fine-tuning strategies, we present an extensive study by integrating marker knowledge into our generative model. The proposed framework achieves competitive results to the state-of-the-art (SoTA) model and outperforms several baselines.},
  pubstate = {prepublished}
}

@article{Davis2015CommonsenseReasoningCommonsense,
  title = {Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence},
  author = {Davis, Ernest and Marcus, Gary},
  date = {2015-08-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {58},
  number = {9},
  pages = {92--103},
  issn = {0001-0782},
  doi = {10.1145/2701413},
  url = {https://doi.org/10.1145/2701413},
  urldate = {2020-06-07},
  abstract = {AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.}
}

@article{Daxenberger2020ArgumenTextArgumentClassification,
  title = {{{ArgumenText}}: {{Argument Classification}} and {{Clustering}} in a~{{Generalized Search Scenario}}},
  shorttitle = {{{ArgumenText}}},
  author = {Daxenberger, Johannes and Schiller, Benjamin and Stahlhut, Chris and Kaiser, Erik and Gurevych, Iryna},
  date = {2020-06-16},
  journaltitle = {Datenbank-Spektrum},
  shortjournal = {Datenbank Spektrum},
  issn = {1610-1995},
  doi = {10.1007/s13222-020-00347-7},
  url = {https://doi.org/10.1007/s13222-020-00347-7},
  urldate = {2020-07-09},
  abstract = {The ArgumenText project creates argument mining technology for big and heterogeneous data and aims to evaluate its use in real-world applications. The technology mines and clusters arguments from a~variety of textual sources for a~large range of topics and in multiple languages. Its main strength is its generalization to very different textual sources including web crawls, news data, or customer reviews. We validated the technology with a~focus on supporting decisions in innovation management as well as customer feedback analysis. Along with its public argument search engine and API, ArgumenText has released multiple datasets for argument classification and clustering. This contribution outlines the major technology-related challenges and proposed solutions for the tasks of argument extraction from heterogeneous sources and argument clustering. It also lays out exemplary industry applications and remaining challenges.},
  langid = {english}
}

@online{DeepSeek-AI2025DeepSeekR1IncentivizingReasoning,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  date = {2025-01-22},
  eprint = {2501.12948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.12948},
  url = {http://arxiv.org/abs/2501.12948},
  urldate = {2025-01-26},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  pubstate = {prepublished}
}

@inproceedings{Defourneaux1997AnalogyAbductionAutomated,
  title = {Analogy and {{Abduction}} in {{Automated Deduction}}},
  booktitle = {Proc. {{IJCAI-97}}},
  author = {Defourneaux, Gilles and Peltier, Nicolas},
  date = {1997},
  pages = {216--221},
  publisher = {Morgan Kaufmann},
  abstract = {A method is presented for analogical reasoning in Automated Deduction. We focus on the abductive aspects of analogy and give a unified treatment for theorems and non-theorems. Abduction allows to deal with partial analogies thus strongly increasing the application field of the method. It also allows to detect \&quot;bad analogies\&quot; in several cases. Explanatory examples as well as more realistic examples quantifying the effects of using analogy (for theorem-proving and for counter-example building) are given. 1}
}

@inproceedings{DelCorro2013ClausIEClausebasedOpen,
  title = {{{ClausIE}}: Clause-Based Open Information Extraction},
  shorttitle = {{{ClausIE}}},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}}},
  author = {Del Corro, Luciano and Gemulla, Rainer},
  date = {2013-05-13},
  series = {{{WWW}} '13},
  pages = {355--366},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2488388.2488420},
  url = {https://dl.acm.org/doi/10.1145/2488388.2488420},
  urldate = {2023-07-26},
  abstract = {We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of ``useful'' pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of each clause according to the grammatical function of its constituents. Based on this information, ClausIE is able to generate high-precision extractions; the representation of these extractions can be flexibly customized to the underlying application. ClausIE is based on dependency parsing and a small set of domain-independent lexica, operates sentence by sentence without any post-processing, and requires no training data (whether labeled or unlabeled). Our experimental study on various real-world datasets suggests that ClausIE obtains higher recall and higher precision than existing approaches, both on high-quality text as well as on noisy text as found in the web.},
  isbn = {978-1-4503-2035-1}
}

@inproceedings{DeLiddo2021UnderstandingFailuresPotentials,
  title = {Understanding {{Failures}} and {{Potentials}} of {{Argumentation Tools}} for {{Public Deliberation}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Communities}} \& {{Technologies}} - {{Wicked Problems}} in the {{Age}} of {{Tech}}},
  author = {De Liddo, Anna and Strube, Rosa},
  date = {2021-06-21},
  series = {C\&amp;{{T}} '21},
  pages = {75--88},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3461564.3461584},
  url = {https://doi.org/10.1145/3461564.3461584},
  urldate = {2023-09-21},
  abstract = {Argument mapping technologies have been historically proposed in Computer Supported Cooperative Work (CSCW) and Computer Supported Argument Visualisation (CSAV) research to improve online deliberation practices, but they have hardly overcome issues of non–expert users’ uptake. This paper presents LiteMap a novel online collaborative argument mapping tool which combines web annotation with light-weight argument mapping features to enable visual summarisation and sensemaking of public online debates. Lessons learned from a user study indicate that LiteMap can be effectively used by untrained users to collectively summarize online discussion forum conversations. LiteMap is reported to improve self-reflection on both, group performance and assessment of the quality of an online conversation. The use of the argument mapping tool has also enhanced the mappers’ understanding of the nature of the debated issues. Nonetheless, map creators have reported the need for parallel negotiation channels to disambiguate meaning and manage disagreement while co-creating an argument map.},
  isbn = {978-1-4503-9056-9}
}

@thesis{Dellaiera2024ReproducibilitySoftwareEngineering,
  title = {Reproducibility in {{Software Engineering}}},
  author = {Dellaiera, Pol},
  date = {2024-08-05},
  doi = {10.5281/zenodo.13208605},
  url = {https://zenodo.org/records/13208605},
  urldate = {2024-08-28},
  abstract = {The concept of reproducibility has long been a cornerstone in scientific research, ensuring that results are robust, repeatable, and can be independently verified. This concept has been extended to computer science, focusing on the ability to recreate identical software artefacts. However, the importance of reproducibility in software engineering is often overlooked, leading to challenges in the validation, security, and reliability of software products. This master's thesis aims to investigate the current state of reproducibility in software engineering, exploring both the barriers and potential solutions to making software more reproducible and raising awareness. It identifies key factors that impede reproducibility such as inconsistent environments, lack of standardisation, and incomplete documentation. To tackle these issues, I propose an empirical comparison of tools facilitating software reproducibility. To provide a comprehensive assessment of reproducibility in software engineering, this study adopts a methodology that involves a hands-on evaluation of four different methods and tools. Through a systematic evaluation of these tools, this research seeks to determine their effectiveness in establishing and maintaining identical software environments and builds. This study contributes to academic knowledge and offers practical insights that could influence future software development protocols and standards.},
  langid = {english}
}

@inproceedings{Deng2009ImageNetLargescaleHierarchical,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2025-07-15},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{Deng2023ResolutionAnalogiesStrings,
  title = {Resolution of {{Analogies Between Strings}} in the {{Case}} of {{Multiple Solutions}}},
  booktitle = {Proceedings of the {{Workshops}} at the 31st {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2023)},
  author = {Deng, Xulin and Lepage, Yves},
  editor = {Malburg, Lukas and Verma, Deepika},
  date = {2023-07-17},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3438},
  pages = {3--14},
  publisher = {CEUR},
  location = {Aberdeen, Scotland},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3438/#paper_01},
  urldate = {2023-07-31},
  eventtitle = {{{ICCBR}} 2023 {{Workshop Proceedings}}},
  langid = {english}
}

@article{Devlin2004MorphologyInternalStructure,
  title = {Morphology and the Internal Structure of Words},
  author = {Devlin, Joseph T. and Jamison, Helen L. and Matthews, Paul M. and Gonnerman, Laura M.},
  date = {2004-10-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {101},
  number = {41},
  eprint = {15358857},
  eprinttype = {pubmed},
  pages = {14984--14988},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0403766101},
  url = {https://www.pnas.org/content/101/41/14984},
  urldate = {2021-03-08},
  abstract = {Morphology is the aspect of language concerned with the internal structure of words, and languages vary in the extent to which they rely on morphological structure. Consequently, it is not clear whether morphology is a basic element of a linguistic structure or whether it emerges from systematic regularities between the form and meaning of words. Here, we looked for evidence of morphological structure at a neural systems level by using a visual masked priming paradigm and functional MRI. Form and meaning relations were manipulated in a 2 × 2 design to identify reductions in blood oxygenation level-dependent signal related to shared form (e.g., corner-corn), shared meaning (e.g., idea-notion), and shared morphemes (e.g., boldly-bold, which overlapped in both form and meaning). Relative to unrelated pairs (e.g., ozone-hero), morphologically related items reduced blood oxygenation level-dependent signal in the posterior angular gyrus bilaterally, left occipitotemporal cortex, and left middle temporal gyrus. In the posterior angular gyrus, a neural priming effect was observed for all three priming conditions, possibly reflecting reduced attentional demands rather than overlapping linguistic representations per se. In contrast, the reductions seen in the left occipitotemporal cortex and left middle temporal gyrus corresponded, respectively, to main effects of orthographic and semantic overlap. As neural regions sensitive to morphological structure overlapped almost entirely with regions sensitive to orthographic and semantic relatedness, our results suggest that morphology emerges from the convergence of form and meaning.},
  langid = {english}
}

@inproceedings{Devlin2019BERTPretrainingDeep,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423},
  urldate = {2024-07-15},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019}
}

@inproceedings{DeWetering2020SpaceReclaimingIciclePlots,
  title = {Space-{{Reclaiming Icicle Plots}}},
  booktitle = {2020 {{IEEE Pacific Visualization Symposium}} ({{PacificVis}})},
  author = {family=Wetering, given=Huub, prefix=van, useprefix=true and Klaassen, Nico and Burch, Michael},
  date = {2020-06},
  pages = {121--130},
  issn = {2165-8773},
  doi = {10.1109/PacificVis48177.2020.4908},
  url = {https://ieeexplore.ieee.org/document/9086292},
  urldate = {2025-02-13},
  abstract = {This paper describes the space-reclaiming icicle plots, hierarchy visualizations based on the visual metaphor of icicles. As a novelty, our approach tries to reclaim empty space in all hierarchy levels. This reclaiming results in an improved visibility of the hierarchy elements especially those in deeper levels. We implemented an algorithm that is capable of producing more space-reclaiming icicle plot variants. Several visual parameters can be tweaked to change the visual appearance and readability of the plots: among others, a space-reclaiming parameter, an empty space shrinking parameter, and a gap size. To illustrate the usefulness of the novel visualization technique we applied it, among others, to an NCBI taxonomy dataset consisting of more than 300,000 elements and with maximum depth 42. Moreover, we explore the parameter and design space by applying several values for the visual parameters. We also conducted a controlled user study with 17 participants and received qualitative feedback from 112 students from a visualization course.},
  eventtitle = {2020 {{IEEE Pacific Visualization Symposium}} ({{PacificVis}})}
}

@online{DeWynter2024HaveArgumentPlease,
  title = {"{{I}}'d {{Like}} to {{Have}} an {{Argument}}, {{Please}}": {{Argumentative Reasoning}} in {{Large Language Models}}},
  shorttitle = {"{{I}}'d {{Like}} to {{Have}} an {{Argument}}, {{Please}}"},
  author = {family=Wynter, given=Adrian, prefix=de, useprefix=true and Yuan, Tangming},
  date = {2024-06-10},
  eprint = {2309.16938},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16938},
  url = {http://arxiv.org/abs/2309.16938},
  urldate = {2024-06-21},
  abstract = {We evaluate two large language models (LLMs) ability to perform argumentative reasoning. We experiment with argument mining (AM) and argument pair extraction (APE), and evaluate the LLMs' ability to recognize arguments under progressively more abstract input and output (I/O) representations (e.g., arbitrary label sets, graphs, etc.). Unlike the well-known evaluation of prompt phrasings, abstraction evaluation retains the prompt's phrasing but tests reasoning capabilities. We find that scoring-wise the LLMs match or surpass the SOTA in AM and APE, and under certain I/O abstractions LLMs perform well, even beating chain-of-thought--we call this symbolic prompting. However, statistical analysis on the LLMs outputs when subject to small, yet still human-readable, alterations in the I/O representations (e.g., asking for BIO tags as opposed to line numbers) showed that the models are not performing reasoning. This suggests that LLM applications to some tasks, such as data labelling and paper reviewing, must be done with care.},
  pubstate = {prepublished}
}

@inproceedings{Diallo2019LearningAnalogyPreservingSentence,
  title = {Learning {{Analogy-Preserving Sentence Embeddings}} for {{Answer Selection}}},
  booktitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Diallo, Aïssatou and Zopf, Markus and Fürnkranz, Johannes},
  date = {2019-11},
  pages = {910--919},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/K19-1085},
  url = {https://www.aclweb.org/anthology/K19-1085},
  urldate = {2020-08-03},
  abstract = {Answer selection aims at identifying the correct answer for a given question from a set of potentially correct answers. Contrary to previous works, which typically focus on the semantic similarity between a question and its answer, our hypothesis is that question-answer pairs are often in analogical relation to each other. Using analogical inference as our use case, we propose a framework and a neural network architecture for learning dedicated sentence embeddings that preserve analogical properties in the semantic space. We evaluate the proposed method on benchmark datasets for answer selection and demonstrate that our sentence embeddings indeed capture analogical properties better than conventional embeddings, and that analogy-based question answering outperforms a comparable similarity-based technique.},
  eventtitle = {{{CoNLL}} 2019}
}

@inproceedings{Diebold2024OlaaafGeneralAdaptation,
  title = {Olaaaf: {{A General Adaptation Prototype}}},
  shorttitle = {Olaaaf},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Diebold, Erwan and Kabrit, Yan and Kril, Axel and Lieber, Jean and Malvaud, Paul and Nauer, Emmanuel and Sipp, Jules},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {223--239},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_15},
  abstract = {Adaptation is often considered a complex issue during the design of a case-based reasoning system. Various approaches can be found in the literature, but their scopes are often limited to a relatively narrow range of applications.However, a general approach to adaptation based on belief revision has been developed over the years and applied in several formalisms, these formalisms being chosen for particular needs. This article presents the first version of \$\$\textbackslash text \{Olaaaf\}\$\$Olaaaf, a general adaptation prototype based on belief revision whose long-term objective is to cover a wide range of adaptation processes. It is based on a formalism that covers both attribute-value pairs (often used for representing cases) and taxonomies (often used for representing domain knowledge). It is shown through an example how this system works, and it is discussed how it can be used for other complex adaptations.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Dijkman2009GraphMatchingAlgorithms,
  title = {Graph {{Matching Algorithms}} for {{Business Process Model Similarity Search}}},
  booktitle = {Business {{Process Management}}},
  author = {Dijkman, Remco and Dumas, Marlon and García-Bañuelos, Luciano},
  editor = {Dayal, Umeshwar and Eder, Johann and Koehler, Jana and Reijers, Hajo A.},
  date = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {48--63},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-03848-8_5},
  abstract = {We investigate the problem of ranking all process models in a repository according to their similarity with respect to a given process model. We focus specifically on the application of graph matching algorithms to this similarity search problem. Since the corresponding graph matching problem is NP-complete, we seek to find a compromise between computational complexity and quality of the computed ranking. Using a repository of 100 process models, we evaluate four graph matching algorithms, ranging from a greedy one to a relatively exhaustive one. The results show that the mean average precision obtained by a fast greedy algorithm is close to that obtained with the most exhaustive algorithm.},
  isbn = {978-3-642-03848-8},
  langid = {english}
}

@article{Dijkstra1959NoteTwoProblems,
  title = {A Note on Two Problems in Connexion with Graphs},
  author = {Dijkstra, E. W.},
  date = {1959-12-01},
  journaltitle = {Numerische Mathematik},
  shortjournal = {Numer. Math.},
  volume = {1},
  number = {1},
  pages = {269--271},
  issn = {0945-3245},
  doi = {10.1007/BF01386390},
  url = {https://doi.org/10.1007/BF01386390},
  urldate = {2021-03-17},
  langid = {english}
}

@inproceedings{Ding2025RetrievalLearnedSimilarities,
  title = {Retrieval with {{Learned Similarities}}},
  booktitle = {Proceedings of the {{ACM}} on {{Web Conference}} 2025},
  author = {Ding, Bailu and Zhai, Jiaqi},
  date = {2025-04-22},
  series = {{{WWW}} '25},
  pages = {1626--1637},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3696410.3714822},
  url = {https://dl.acm.org/doi/10.1145/3696410.3714822},
  urldate = {2025-09-01},
  abstract = {Retrieval plays a fundamental role in recommendation systems, search, and natural language processing (NLP) by efficiently finding relevant items from a large corpus given a query. Dot products have been widely used as the similarity function in such tasks, enabled by Maximum Inner Product Search (MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval algorithms have migrated to learned similarities. These advanced approaches encompass multiple query embeddings, complex neural networks, direct item ID decoding via beam search, and hybrid solutions. Unfortunately, we lack efficient solutions for retrieval in these state-of-the-art setups. Our work addresses this gap by investigating efficient retrieval techniques with expressive learned similarity functions. We establish Mixture-of-Logits (MoL) as a universal approximator of similarity functions, demonstrate that MoL's expressiveness can be realized empirically to achieve superior performance on diverse retrieval scenarios, and propose techniques to retrieve the approximate top-k results using MoL with tight error bounds. Through extensive experimentation, we show that MoL, enhanced by our proposed mutual information-based load balancing loss, sets new state-of-the-art results across heterogeneous scenarios, including sequential retrieval models in recommendation systems and finetuning language models for question answering; and our approximate top-k algorithms outperform baselines by up to 66× in latency while achieving \&gt;.99 recall rate compared to exact algorithms.},
  isbn = {979-8-4007-1274-6}
}

@thesis{Dolstra2006PurelyFunctionalSoftware,
  type = {phdthesis},
  title = {The {{Purely Functional Software Deployment Model}}},
  author = {Dolstra, Eelco},
  date = {2006},
  institution = {Utrecht University},
  location = {Utrecht, The Netherlands},
  url = {https://edolstra.github.io/pubs/phd-thesis.pdf},
  langid = {english},
  pagetotal = {281}
}

@inproceedings{Dolstra2008NixOSPurelyFunctional,
  title = {{{NixOS}}: {{A}} Purely Functional {{Linux}} Distribution},
  shorttitle = {{{NixOS}}},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Dolstra, Eelco and Löh, Andres},
  date = {2008-09-20},
  series = {{{ICFP}} '08},
  pages = {367--378},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1411204.1411255},
  url = {https://doi.org/10.1145/1411204.1411255},
  urldate = {2024-07-19},
  abstract = {Existing package and system configuration management tools suffer from an imperative model, where system administration actions such as upgrading packages or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to run multiple versions of a package side-by-side, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogously to a heap in a purely function language. We have implemented this model in NixOS, a non-trivial Linux distribution that uses the Nix package manager to build the entire system configuration from a purely functional specification.},
  isbn = {978-1-59593-919-7}
}

@article{Dolstra2010NixOSPurelyFunctional,
  title = {{{NixOS}}: {{A}} Purely Functional {{Linux}} Distribution},
  shorttitle = {{{NixOS}}},
  author = {Dolstra, Eelco and Löh, Andres and Pierron, Nicolas},
  date = {2010-11},
  journaltitle = {Journal of Functional Programming},
  volume = {20},
  number = {5--6},
  pages = {577--615},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796810000195},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/nixos-a-purely-functional-linux-distribution/C1ACBA2A51D2E5466820F5B5086EA2CE},
  urldate = {2024-07-19},
  abstract = {Existing package and system configuration management tools suffer from an imperative model, where system administration actions such as package upgrades or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to deploy multiple versions of a package side-by-side, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogous to a heap in a purely functional language. We have implemented this model in NixOS, a non-trivial Linux distribution that uses the Nix package manager to build the entire system configuration from a modular, purely functional specification.},
  langid = {english}
}

@inproceedings{Don-Yehiya2025ShareLMCollectionPlugin,
  title = {The {{ShareLM Collection}} and {{Plugin}}: {{Contributing Human-Model Chats}} for the {{Benefit}} of the {{Community}}},
  shorttitle = {The {{ShareLM Collection}} and {{Plugin}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Don-Yehiya, Shachar and Choshen, Leshem and Abend, Omri},
  editor = {Mishra, Pushkar and Muresan, Smaranda and Yu, Tao},
  date = {2025-07},
  pages = {167--177},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-demo.17/},
  urldate = {2025-07-29},
  abstract = {Human-model conversations provide a window into users' real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research. While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind.We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations. Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms. The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user's local storage.},
  isbn = {979-8-89176-253-4}
}

@online{Dong2024CanLLMBe,
  title = {Can {{LLM}} Be a {{Personalized Judge}}?},
  author = {Dong, Yijiang River and Hu, Tiancheng and Collier, Nigel},
  date = {2024-06-17},
  eprint = {2406.11657},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.11657},
  url = {http://arxiv.org/abs/2406.11657},
  urldate = {2024-09-23},
  abstract = {Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80\%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.},
  pubstate = {prepublished}
}

@inproceedings{Dore2025LeveragingGraphStructural,
  title = {Leveraging {{Graph Structural Knowledge}} to {{Improve Argument Relation Prediction}} in {{Political Debates}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Dore, Deborah and Faralli, Stefano and Villata, Serena},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {74--86},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.7/},
  urldate = {2025-07-28},
  abstract = {Argument Mining (AM) aims at detecting argumentation structures (i.e., premises and claims linked by attack and support relations) in text. A natural application domain is political debates, where uncovering the hidden dynamics of a politician's argumentation strategies can help the public to identify fallacious and propagandist arguments. Despite the few approaches proposed in the literature to apply AM to political debates, this application scenario is still challenging, and, more precisely, concerning the task of predicting the relation holding between two argument components. Most of AM relation prediction approaches only consider the textual content of the argument component to identify and classify the argumentative relation holding among them (i.e., support, attack), and they mostly ignore the structural knowledge that arises from the overall argumentation graph. In this paper, we propose to address the relation prediction task in AM by combining the structural knowledge provided by a Knowledge Graph Embedding Model with the contextual knowledge provided by a fine-tuned Language Model. Our experimental setting is grounded on a standard AM benchmark of televised political debates of the US presidential campaigns from 1960 to 2020. Our extensive experimental setting demonstrates that integrating these two distinct forms of knowledge (i.e., the textual content of the argument component and the structural knowledge of the argumentation graph) leads to novel pathways that outperform existing approaches in the literature on this benchmark and enhance the accuracy of the predictions.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Dosovitskiy2021ImageWorth16x16,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-01-12},
  url = {https://openreview.net/forum?id=YicbFdNTTy},
  urldate = {2025-02-10},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{DosSantos2018ImpactsCodingPractices,
  title = {Impacts of Coding Practices on Readability},
  booktitle = {Proceedings of the 26th {{Conference}} on {{Program Comprehension}}},
  author = {family=Santos, given=Rodrigo Magalhães, prefix=dos, useprefix=true and Gerosa, Marco Aurélio},
  date = {2018-05-28},
  series = {{{ICPC}} '18},
  pages = {277--285},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3196321.3196342},
  url = {https://doi.org/10.1145/3196321.3196342},
  urldate = {2022-05-03},
  abstract = {Several conventions and standards aim to improve maintainability of software code. However, low levels of code readability perceived by developers still represent a barrier to their daily work. In this paper, we describe a survey that assessed the impact of a set of Java coding practices on the readability perceived by software developers. While some practices promoted an enhancement of readability, others did not show statistically significant effects. Interestingly, one of the practices worsened the readability. Our results may help to identify coding conventions with a positive impact on readability and, thus, guide the creation of coding standards.},
  isbn = {978-1-4503-5714-2}
}

@inproceedings{Douglas2017MonkeypuzzleNextGeneration,
  title = {Monkeypuzzle - {{Towards Next Generation}}, {{Free}} \& {{Open-Source}}, {{Argument Analysis Tools}}},
  booktitle = {{{CMNA}}@{{ICAIL}}},
  author = {Douglas, John and Wells, Simon},
  date = {2017},
  abstract = {A new, free, open-source, web-based argument analysis tool called Monkeypuzzle is introduced, designed to provide both a foundation for creating and visualising reproducible argument analyses as well as a flexible framework for investigating new and extending existing argument analysis techniques. We introduce a new, free, open-source, web-based argument analysis tool called Monkeypuzzle. This is designed to provide both a foundation for creating and visualising reproducible argument analyses as well as a flexible framework for investigating new and extending existing argument analysis techniques.}
}

@inproceedings{Du2019ValidationGrowingKnowledge,
  title = {Validation of {{Growing Knowledge Graphs}} by {{Abductive Text Evidences}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Du, Jianfeng and Pan, Jeff Z. and Wang, Sylvia and Qi, Kunxun and Shen, Yuming and Deng, Yu},
  date = {2019-07-17},
  volume = {33},
  pages = {2784--2791},
  doi = {10.1609/aaai.v33i01.33012784},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4130},
  urldate = {2024-10-31},
  abstract = {This paper proposes a validation mechanism for newly added triples in a growing knowledge graph. Given a logical theory, a knowledge graph, a text corpus, and a new triple to be validated, this mechanism computes a sorted list of explanations for the new triple to facilitate the validation of it, where an explanation, called an abductive text evidence, is a set of pairs of the form (triple, window) where appending the set of triples on the left to the knowledge graph enforces entailment of the new triple under the logical theory, while every sentence window on the right which is contained in the text corpus explains to some degree why the triple on the left is true. From the angle of practice, a special class of abductive text evidences called TEP-based abductive text evidence is proposed, which is constructed from explanation patterns seen before in the knowledge graph. Accordingly, a method for computing the complete set of TEP-based abductive text evidences is proposed. Moreover, a method for sorting abductive text evidences based on distantly supervised learning is proposed. To evaluate the proposed validation mechanism, four knowledge graphs with logical theories are constructed from the four great classical masterpieces of Chinese literature. Experimental results on these datasets demonstrate the efficiency and effectiveness of the proposed mechanism.}
}

@article{Dudley2018ReviewUserInterface,
  title = {A {{Review}} of {{User Interface Design}} for {{Interactive Machine Learning}}},
  author = {Dudley, John J. and Kristensson, Per Ola},
  date = {2018-06-13},
  journaltitle = {ACM Transactions on Interactive Intelligent Systems},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  volume = {8},
  number = {2},
  pages = {8:1--8:37},
  issn = {2160-6455},
  doi = {10.1145/3185517},
  url = {https://doi.org/10.1145/3185517},
  urldate = {2022-05-03},
  abstract = {Interactive Machine Learning (IML) seeks to complement human perception and intelligence by tightly integrating these strengths with the computational power and speed of computers. The interactive process is designed to involve input from the user but does not require the background knowledge or experience that might be necessary to work with more traditional machine learning techniques. Under the IML process, non-experts can apply their domain knowledge and insight over otherwise unwieldy datasets to find patterns of interest or develop complex data-driven applications. This process is co-adaptive in nature and relies on careful management of the interaction between human and machine. User interface design is fundamental to the success of this approach, yet there is a lack of consolidated principles on how such an interface should be implemented. This article presents a detailed review and characterisation of Interactive Machine Learning from an interactive systems perspective. We propose and describe a structural and behavioural model of a generalised IML system and identify solution principles for building effective interfaces for IML. Where possible, these emergent solution principles are contextualised by reference to the broader human-computer interaction literature. Finally, we identify strands of user interface research key to unlocking more efficient and productive non-expert interactive machine learning applications.}
}

@inproceedings{Dufour-Lussier2010TextAdaptationUsing,
  title = {Text {{Adaptation Using Formal Concept Analysis}}},
  booktitle = {Case-{{Based Reasoning}}. {{Research}} and {{Development}}},
  author = {Dufour-Lussier, Valmi and Lieber, Jean and Nauer, Emmanuel and Toussaint, Yannick},
  editor = {Bichindaritz, Isabelle and Montani, Stefania},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {96--110},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14274-1_9},
  abstract = {This paper addresses the issue of adapting cases represented by plain text with the help of formal concept analysis and natural language processing technologies. The actual cases represent recipes in which we classify ingredients according to culinary techniques applied to them. The complex nature of linguistic anaphoras in recipe texts make usual text mining techniques inefficient so a stronger approach, using syntactic and dynamic semantic analysis to build a formal representation of a recipe, had to be used. This representation is useful for various applications but, in this paper, we show how one can extract ingredient–action relations from it in order to use formal concept analysis and select an appropriate replacement sequence of culinary actions to use in adapting the recipe text.},
  isbn = {978-3-642-14274-1},
  langid = {english}
}

@article{Dufour-Lussier2014AutomaticCaseAcquisition,
  title = {Automatic Case Acquisition from Texts for Process-Oriented Case-Based Reasoning},
  author = {Dufour-Lussier, Valmi and Le Ber, Florence and Lieber, Jean and Nauer, Emmanuel},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {153--167},
  issn = {0306-4379},
  doi = {10.1016/j.is.2012.11.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437912001573},
  urldate = {2025-05-13},
  abstract = {This paper introduces a method for the automatic acquisition of a rich case representation from free text for process-oriented case-based reasoning. Case engineering is among the most complicated and costly tasks in implementing a case-based reasoning system. This is especially so for process-oriented case-based reasoning, where more expressive case representations are generally used and, in our opinion, actually required for satisfactory case adaptation. In this context, the ability to acquire cases automatically from procedural texts is a major step forward in order to reason on processes. We therefore detail a methodology that makes case acquisition from processes described as free text possible, with special attention given to assembly instruction texts. This methodology extends the techniques we used to extract actions from cooking recipes. We argue that techniques taken from natural language processing are required for this task, and that they give satisfactory results. An evaluation based on our implemented prototype extracting workflows from recipe texts is provided.}
}

@thesis{Dumani2018ReconstructingVersionHistory,
  type = {mathesis},
  title = {Reconstructing the Version History of {{Stack Overflow}} Posts},
  author = {Dumani, Lorik},
  date = {2018-01-21},
  institution = {University of Trier},
  pagetotal = {1}
}

@inproceedings{Dumani2019GoodPremisesRetrieval,
  title = {Good {{Premises Retrieval}} via a {{Two-Stage Argument Retrieval Model}}},
  booktitle = {Proceedings of the 31st {{GI-Workshop Grundlagen}} von {{Datenbanken}}},
  author = {Dumani, Lorik},
  editor = {Schenkel, Ralf},
  date = {2019-06-11},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2367},
  pages = {3--8},
  publisher = {CEUR},
  location = {Saarburg, Germany},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-2367/#paper_13},
  urldate = {2024-12-11},
  eventtitle = {Grundlagen von {{Datenbanken}}},
  langid = {english}
}

@inproceedings{Dumani2019SystematicComparisonMethods,
  title = {A {{Systematic Comparison}} of {{Methods}} for {{Finding Good Premises}} for {{Claims}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Dumani, Lorik and Schenkel, Ralf},
  date = {2019-07-18},
  series = {{{SIGIR}}'19},
  pages = {957--960},
  publisher = {Association for Computing Machinery},
  location = {Paris, France},
  doi = {10.1145/3331184.3331282},
  url = {https://doi.org/10.1145/3331184.3331282},
  urldate = {2020-08-22},
  abstract = {Research on computational argumentation has recently become very popular. An argument consists of a claim that is supported or attacked by at least one premise. Its intention is the persuasion of others. An important problem in this field is retrieving good premises for a designated claim from a corpus of arguments. Given a claim, oftentimes existing approaches' first step is finding textually similar claims. In this paper we compare 196 methods systematically for determining similar claims by textual similarity, using a large corpus of (claim, premise) pairs crawled from debate portals. We also evaluate how well textual similarity of claims can predict relevance of the associated premises.},
  isbn = {978-1-4503-6172-9}
}

@inproceedings{Dumani2020FrameworkArgumentRetrieval,
  title = {A {{Framework}} for {{Argument Retrieval}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Dumani, Lorik and Neumann, Patrick J. and Schenkel, Ralf},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {431--445},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-45439-5_29},
  abstract = {Computational argumentation has recently become a fast growing field of research. An argument consists of a claim, such as “We should abandon fossil fuels”, which is supported or attacked by at least one premise, for example “Burning fossil fuels is one cause for global warming”. From an information retrieval perspective, an interesting task within this setting is finding the best supporting and attacking premises for a given query claim from a large corpus of arguments. Since the same logical premise can be formulated differently, the system needs to avoid retrieving duplicate results and thus needs to use some form of clustering. In this paper we propose a principled probabilistic ranking framework for premises based on the idea of tf-idf that, given a query claim, first identifies highly similar claims in the corpus, and then clusters and ranks their premises, taking clusters of claims as well as the stances of query and premises into account. We compare our approach to a baseline system that uses BM25F which we outperform even with a primitive implementation of our framework utilising BERT.},
  isbn = {978-3-030-45439-5},
  langid = {english}
}

@inproceedings{Dumani2020QualityAwareRankingArguments,
  title = {Quality-{{Aware Ranking}} of {{Arguments}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Dumani, Lorik and Schenkel, Ralf},
  date = {2020-10-19},
  series = {{{CIKM}} '20},
  pages = {335--344},
  publisher = {Association for Computing Machinery},
  location = {Virtual Event, Ireland},
  doi = {10.1145/3340531.3411960},
  url = {https://doi.org/10.1145/3340531.3411960},
  urldate = {2020-10-24},
  abstract = {Argument search engines identify, extract, and rank the most important arguments for and against a given controversial topic. A number of such systems have recently been developed, usually focusing on classic information retrieval ranking methods that are based on frequency information. An important aspect that has been ignored so far by search engines is the quality of arguments. We present a quality-aware ranking framework for arguments already extracted from texts and represented as argument graphs, considering multiple established quality measures. An extensive evaluation with a standard benchmark collection demonstrates that taking quality into account significantly helps to improve retrieval quality for argument search. We also publish a dataset in which arguments with respect to topics were tediously annotated by humans with three widely accepted argument quality dimensions.},
  isbn = {978-1-4503-6859-9}
}

@inproceedings{Dumani2020RankingArgumentsCombining,
  title = {Ranking {{Arguments}} by {{Combining Claim Similarity}} and {{Argument Quality Dimensions}}},
  booktitle = {Working {{Notes}} of {{CLEF}} 2020 - {{Conference}} and {{Labs}} of the {{Evaluation Forum}}},
  author = {Dumani, Lorik and Schenkel, Ralf},
  editor = {Cappellato, Linda and Eickhoff, Carsten and Ferro, Nicola and Névéol, Aurélie},
  date = {2020-09-22},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2696},
  publisher = {CEUR},
  location = {Thessaloniki, Greece},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-2696/#paper_174},
  urldate = {2024-12-11},
  eventtitle = {{{CLEF}} 2020 {{Working Notes}}},
  langid = {english}
}

@inproceedings{Dumani2020SegmentingClusteringNoisy,
  title = {Segmenting and {{Clustering Noisy Arguments}}},
  booktitle = {Proceedings of the {{Conference}} "{{Lernen}}, {{Wissen}}, {{Daten}}, {{Analysen}}"},
  author = {Dumani, Lorik and Kreutz, Christin Katharina and Biertz, Manuel and Witry, Alex and Schenkel, Ralf},
  editor = {Trabold, Daniel and Welke, Pascal and Piatkowski, Nico},
  date = {2020-09-09},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2738},
  pages = {23--34},
  publisher = {CEUR},
  location = {Online},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-2738/#paper24},
  urldate = {2023-11-27},
  abstract = {Automated argument retrieval for queries is desirable, e.g., as it helps in decision making or convincing others of certain actions. An argument consists of a claim supported or attacked by at least one premise. The claim describes a controversial viewpoint that should not be accepted without evidence given by premises. Premises are composed of Elementary Discourse Units (EDUs) which are their smallest contextual components. Oftentimes argument search engines find similar claims to a query first before returning their premises. Due to heterogeneous data sources, premises often appear repeatedly in different syntactic forms. From an information retrieval perspective, it is essential to rank premises relevant for a query claim highly in a duplicate-free manner. The main challenge in clustering them is to avoid redundancies as premises frequently address various aspects, i.e., consist of multiple EDUs. So, two tasks can be defined: segmentation of premises in EDUs and clustering of similar EDUs. In this paper we make two contributions: Our first contribution is the introduction of a noisy dataset with 480 premises for 30 queries crawled from debate portals which serves as a gold standard for the segmentation of premises into EDUs and the clustering of EDUs. Our second contribution consists of first baselines for the two mentioned tasks, for which we evaluated various methods. Our results show that an uncurated dataset is a major challenge and that clustering EDUs is only reasonable with premises as context information.},
  eventtitle = {Lernen, {{Wissen}}, {{Daten}}, {{Analysen}} 2020},
  langid = {english}
}

@inproceedings{Dumani2021FineCoarseGranular,
  title = {Fine and {{Coarse Granular Argument Classification}} before {{Clustering}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Dumani, Lorik and Wiesenfeldt, Tobias and Schenkel, Ralf},
  date = {2021-10-26},
  series = {{{CIKM}} '21},
  pages = {422--432},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3459637.3482431},
  url = {https://doi.org/10.1145/3459637.3482431},
  urldate = {2022-09-26},
  abstract = {Computational argumentation and especially argument mining together with retrieval enjoys increasing popularity. In contrast to standard search engines that focus on finding documents relevant to a query, argument retrieval aims at finding the best supporting and attacking premises given a query claim, e.g., from a predefined collection of arguments. Here, a claim is the central part of an argument representing the standpoint of a speaker with the goal to persuade the audience, and a premise serves as evidence to the claim. In addition to the actual retrieval process, existing work has focused on (1) classifying polarities of arguments into supporting or opposing, (2) classifying arguments by their frames (such as economic or environmental), and (3) clustering similar arguments by their meaning to avoid repetitions in the result list. For experiments, either hand-made argument collections or arguments extracted from debate portals were used. In this paper, we extend existing work on argument clustering, making the following contributions: First, we introduce a novel pipeline for clustering arguments. While previous work classified arguments either by polarity, frame, or meaning, our pipeline incorporates these three, allowing a more systematic presentation of arguments. Second, we introduce a new dataset consisting of 365 argument graphs accompanying more than 11,000 high-quality arguments that, contrary to previous datasets, have been generated, displayed, and verified by journalists and were published in newspapers. A thorough evaluation with this dataset provides a first baseline for future work.},
  isbn = {978-1-4503-8446-9}
}

@inproceedings{Dumani2021ReCAPCorpusCorpus,
  title = {The {{ReCAP Corpus}}: {{A Corpus}} of {{Complex Argument Graphs}} on {{German Education Politics}}},
  shorttitle = {The {{ReCAP Corpus}}},
  booktitle = {{{IEEE Proceedings}} of the 15th {{International Conference}} on {{Semantic Computing}} ({{ICSC}})},
  author = {Dumani, Lorik and Biertz, Manuel and Witry, Alex and Ludwig, Anna-Katharina and Lenz, Mirko and Ollinger, Stefan and Bergmann, Ralph and Schenkel, Ralf},
  date = {2021-01},
  pages = {248--255},
  publisher = {IEEE},
  location = {Laguna Hills, CA, USA},
  issn = {2325-6516},
  doi = {10.1109/ICSC50631.2021.00083},
  abstract = {The automatic extraction of arguments from natural language texts is a highly researched area and more important than ever today, as it is nearly impossible to manually capture all arguments on a controversial topic in a reasonable amount of time. For testing different algorithms such as the retrieval of the best arguments, which are still in their infancy, gold standards must exist. An argument consists of a claim or standpoint that is supported or opposed by at least one premise. The generic term for a claim or premise is Argumentative Discourse Unit (ADU). The relationships between ADUs can be specified by argument schemes and can lead to large graphs. This paper presents a corpus of 100 argument graphs with about 2,500 ADUs in German, which is unique in its size and the utilisation of argument schemes. The corpus is built from natural language texts like party press releases and parliamentary motions on education policies in the German federal states. Each high-quality text is presented by an argument graph and created by the use of a modified version of the annotation tool OVA. The final argument graphs resulted by merging two previously independently annotated graphs based on detailed discussions.},
  eventtitle = {{{IEEE}} 15th {{International Conference}} on {{Semantic Computing}} ({{ICSC}})},
  langid = {english}
}

@thesis{Dumani2022FrequencyQualityDrivenMethods,
  type = {phdthesis},
  title = {Frequency- and {{Quality-Driven Methods}} for {{Ranking}} and {{Clustering Arguments}}},
  author = {Dumani, Lorik},
  date = {2022-09-23},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {english},
  pagetotal = {164}
}

@article{Dung1995AcceptabilityArgumentsIts,
  title = {On the Acceptability of Arguments and Its Fundamental Role in Nonmonotonic Reasoning, Logic Programming and n-Person Games},
  author = {Dung, Phan Minh},
  date = {1995-09-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {77},
  number = {2},
  pages = {321--357},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(94)00041-X},
  url = {http://www.sciencedirect.com/science/article/pii/000437029400041X},
  urldate = {2020-07-23},
  abstract = {The purpose of this paper is to study the fundamental mechanism, humans use in argumentation, and to explore ways to implement this mechanism on computers. We do so by first developing a theory for argumentation whose central notion is the acceptability of arguments. Then we argue for the “correctness” or “appropriateness” of our theory with two strong arguments. The first one shows that most of the major approaches to nonmonotonic reasoning in AI and logic programming are special forms of our theory of argumentation. The second argument illustrates how our theory can be used to investigate the logical structure of many practical problems. This argument is based on a result showing that our theory captures naturally the solutions of the theory of n-person games and of the well-known stable marriage problem. By showing that argumentation can be viewed as a special form of logic programming with negation as failure, we introduce a general logic-programming-based method for generating meta-interpreters for argumentation systems, a method very much similar to the compiler-compiler idea in conventional programming.},
  langid = {english}
}

@inproceedings{Dusmanu2017ArgumentMiningTwitter,
  title = {Argument {{Mining}} on {{Twitter}}: {{Arguments}}, {{Facts}} and {{Sources}}},
  shorttitle = {Argument {{Mining}} on {{Twitter}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Dusmanu, Mihai and Cabrio, Elena and Villata, Serena},
  date = {2017-09},
  pages = {2317--2322},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1245},
  url = {https://aclanthology.org/D17-1245},
  urldate = {2023-10-20},
  abstract = {Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in. Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on social media. In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification. In this paper, we apply supervised classification to identify arguments on Twitter, and we present two new tasks for argument mining, namely facts recognition and source identification. We study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexit news topics.},
  eventtitle = {{{EMNLP}} 2017}
}

@article{Duthie2020NavigatingArgumentsHypotheses,
  title = {Navigating {{Arguments}} and {{Hypotheses}} at {{Scale}}},
  author = {Duthie, Rory and Lawrence, John and Reed, Chris and Visser, Jacky and Zografistou, Dimitra},
  date = {2020},
  journaltitle = {Computational Models of Argument},
  pages = {459--460},
  publisher = {IOS Press},
  doi = {10.3233/FAIA200533},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200533},
  urldate = {2022-04-21}
}

@article{Dwivedi2018ErrortolerantGraphMatching,
  title = {Error-Tolerant Graph Matching Using Node Contraction},
  author = {Dwivedi, Shri Prakash and Singh, Ravi Shankar},
  date = {2018-12-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {116},
  pages = {58--64},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2018.09.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865518305981},
  urldate = {2025-05-16},
  abstract = {Graph matching is the task of computing the similarity between two graphs. It is widely used in graph-based structural pattern recognition. Error-tolerant graph matching is a type of graph matching, in which a similarity between two graphs is computed based on some tolerance value whereas in exact graph matching a strict one-to-one correspondence is required between two graphs. In this paper, we present an approach to error-tolerant graph matching using node contraction where the given graph is transformed into another graph by contracting smaller degree nodes. We use this scheme to extend the notion of graph edit distance, which can be used as a trade-off between execution time and accuracy requirements of various graph matching applications.}
}

@article{Dwivedi2020ErrortolerantApproximateGraph,
  title = {Error-Tolerant Approximate Graph Matching Utilizing Node Centrality Information},
  author = {Dwivedi, Shri Prakash and Singh, Ravi Shankar},
  date = {2020-05-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {133},
  pages = {313--319},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2020.03.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865520300970},
  urldate = {2025-05-14},
  abstract = {Graph matching is the task of finding the similarity between the two graphs. Error-tolerant graph matching is the process of computing the similarity between the two graphs, where some flexibility to noise or error is allowed to approximate the value of graph matching. In this paper, we present a framework for graph matching by utilizing the centrality measures to ignore the least central nodes of the graphs. Experimental evaluation shows that this approach can be useful to reduce the overall matching time and it can provide time versus accuracy trade-off.}
}

@inproceedings{Dwork2001RankAggregationMethods,
  title = {Rank Aggregation Methods for the {{Web}}},
  booktitle = {Proceedings of the 10th International Conference on {{World Wide Web}}},
  author = {Dwork, Cynthia and Kumar, Ravi and Naor, Moni and Sivakumar, D.},
  date = {2001-04-01},
  series = {{{WWW}} '01},
  pages = {613--622},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/371920.372165},
  url = {https://dl.acm.org/doi/10.1145/371920.372165},
  urldate = {2025-07-07},
  isbn = {978-1-58113-348-6}
}

@article{Dykes2020ReconstructingArgumentsNoisy,
  title = {Reconstructing {{Arguments}} from {{Noisy Text}}},
  author = {Dykes, Natalie and Evert, Stefan and Göttlinger, Merlin and Heinrich, Philipp and Schröder, Lutz},
  date = {2020-07-01},
  journaltitle = {Datenbank-Spektrum},
  shortjournal = {Datenbank Spektrum},
  volume = {20},
  number = {2},
  pages = {123--129},
  issn = {1610-1995},
  doi = {10.1007/s13222-020-00342-y},
  url = {https://doi.org/10.1007/s13222-020-00342-y},
  urldate = {2023-07-26},
  abstract = {Social media are of paramount importance to public discourse. RANT aims to contribute methods and formalisms for extracting, representing, and processing arguments from noisy text found in social media discussions, using a~large corpus of pre-referendum Brexit tweets as a~running case study. We identify recurring linguistic argumentation patterns in a~corpus-linguistic analysis and formulate corresponding corpus queries to extract arguments automatically. Given the huge amount of social media data available, our approach aims at high precision at the possible price of low recall. Argumentation patterns are directly associated with logical patterns in a~dedicated formalism and accordingly, individual arguments are directly parsed as logical formulae. The logical formalism for argument representation features a~broad range of modalities capturing real-life modes of expression. We cast this formalism as a~family of instance logics in the generic framework of coalgebraic logic and complement it by a~flexible framework to represent relationships between arguments; including standard relations like attack and support but also relations extracted from metadata. Some relations are inferred from the logical content of individual arguments. We are in the process of developing suitable generalizations of various extension semantics for argumentation frameworks combined with corresponding algorithmic methods to allow for the automated retrieval of large-scale argumentative positions.},
  langid = {english}
}

@inproceedings{Dykes2024FindingArgumentFragments,
  title = {Finding {{Argument Fragments}} on~{{Social Media}} with~{{Corpus Queries}} and~{{LLMs}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Dykes, Nathan and Evert, Stephanie and Heinrich, Philipp and Humml, Merlin and Schröder, Lutz},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {163--181},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_10},
  abstract = {We are concerned with extracting argumentative fragments from social media, exemplified with a case study on a large corpus of English tweets about the UK Brexit referendum in 2016. Our overall approach is to parse the corpus using dedicated corpus queries that fill designated slots in predefined logical patterns. We present an inventory of logical patterns and corresponding queries, which have been carefully designed and refined. While a gold standard of substantial size is difficult to obtain by manual annotation, our queries can retrieve hundreds of thousands of examples with high precision. We show how queries can be combined to extract complex nested statements relevant to argumentation. We also show how to proceed for applications needing higher recall: high-precision query matches can be used as training data for an LLM classifier, and the trade-off between precision and recall can be freely adjusted with its cutoff threshold.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Eckle-Kohler2015RoleDiscourseMarkers,
  title = {On the {{Role}} of {{Discourse Markers}} for {{Discriminating Claims}} and {{Premises}} in {{Argumentative Discourse}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Eckle-Kohler, Judith and Kluge, Roland and Gurevych, Iryna},
  date = {2015-09},
  pages = {2249--2255},
  publisher = {Association for Computational Linguistics},
  location = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1267},
  url = {http://aclweb.org/anthology/D/D15/D15-1267.pdf},
  urldate = {2018-09-01},
  eventtitle = {{{EMNLP}}}
}

@inproceedings{Eden2023WelcomeRealWorld,
  title = {Welcome to the {{Real World}}: {{Efficient}}, {{Incremental}} and {{Scalable Key Point Analysis}}},
  shorttitle = {Welcome to the {{Real World}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Industry Track}}},
  author = {Eden, Lilach and Kantor, Yoav and Orbach, Matan and Katz, Yoav and Slonim, Noam and Bar-Haim, Roy},
  editor = {Wang, Mingxuan and Zitouni, Imed},
  date = {2023-12},
  pages = {483--491},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.emnlp-industry.46},
  url = {https://aclanthology.org/2023.emnlp-industry.46},
  urldate = {2024-04-09},
  abstract = {Key Point Analysis (KPA) is an emerging summarization framework, which extracts the main points from a collection of opinions, and quantifies their prevalence. It has been successfully applied to diverse types of data, including arguments, user reviews and survey responses. Despite the growing academic interest in KPA, little attention has been given to the practical challenges of implementing a KPA system in production. This work presents a deployed KPA system, which regularly serves multiple teams in our organization. We discuss the main challenges we faced while building a real-world KPA system, as well as the architecture and algorithmic improvements we developed to address these challenges. Specifically, we focus on efficient matching of sentences to key points, incremental processing, scalability and resiliency. The value of our contributions is demonstrated in an extensive set of experiments, over five existing and novel datasets. Finally, we describe several use cases of the deployed system, which illustrate its practical value.},
  eventtitle = {{{EMNLP}} 2023}
}

@inproceedings{Eger2018CrosslingualArgumentationMining,
  title = {Cross-Lingual {{Argumentation Mining}}: {{Machine Translation}} (and a Bit of {{Projection}}) Is {{All You Need}}!},
  shorttitle = {Cross-Lingual {{Argumentation Mining}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Eger, Steffen and Daxenberger, Johannes and Stab, Christian and Gurevych, Iryna},
  date = {2018-08},
  pages = {831--844},
  publisher = {Association for Computational Linguistics},
  location = {Santa Fe, New Mexico, USA},
  url = {https://www.aclweb.org/anthology/C18-1071},
  urldate = {2020-03-31},
  abstract = {Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into German, French, Spanish, and Chinese. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/coling2018-xling\_argument\_mining.},
  eventtitle = {{{COLING}} 2018}
}

@inproceedings{Eisenstadt2024AutocompletionArchitecturalSpatial,
  title = {Autocompletion of {{Architectural Spatial Configurations Using Case-Based Reasoning}}, {{Graph Clustering}}, and {{Deep Learning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Eisenstadt, Viktor and Langenhan, Christoph and Bielski, Jessica and Bergmann, Ralph and Althoff, Klaus-Dieter},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {321--337},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_21},
  abstract = {This paper presents an approach for autocompletion of architectural building designs in the form of graph-based floor plans during the early design phase. We utilize established case-based reasoning methods, such as subgraph matching and transformational adaptation, further we employ supervised and unsupervised machine learning techniques, such as graph clustering and graph neural networks. Combining those methods into a single approach, the goal is to predict possibly missing spaces in architectural designs of housing buildings, supporting the acceleration of the early design process of architects to make it more sustainable, while enriching it with the recent developments of artificial intelligence. The approach was validated by a performance evaluation and a user study with participation of representatives of the architecture domain.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@book{Eisenstein2019IntroductionNaturalLanguage,
  title = {Introduction to {{Natural Language Processing}}},
  author = {Eisenstein, Jacob},
  date = {2019-10-01},
  eprint = {72yuDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {MIT Press},
  abstract = {A survey of computational methods for understanding, generating, and manipulating human language, which offers a synthesis of classical representations and algorithms with contemporary machine learning techniques.This textbook provides a technical perspective on natural language processing—methods for building computer software that understands, generates, and manipulates human language. It emphasizes contemporary data-driven approaches, focusing on techniques from supervised and unsupervised machine learning. The first section establishes a foundation in machine learning by building a set of tools that will be used throughout the book and applying them to word-based textual analysis. The second section introduces structured representations of language, including sequences, trees, and graphs. The third section explores different approaches to the representation and analysis of linguistic meaning, ranging from formal logic to neural word embeddings. The final section offers chapter-length treatments of three transformative applications of natural language processing: information extraction, machine translation, and text generation. End-of-chapter exercises include both paper-and-pencil analysis and software implementation.The text synthesizes and distills a broad and diverse research literature, linking contemporary machine learning techniques with the field's linguistic and computational foundations. It is suitable for use in advanced undergraduate and graduate-level courses and as a reference for software engineers and data scientists. Readers should have a background in computer programming and college-level mathematics. After mastering the material presented, students will have the technical skill to build and analyze novel natural language processing systems and to understand the latest research in the field.},
  isbn = {978-0-262-04284-0},
  langid = {english},
  pagetotal = {535}
}

@online{El-Nouby2021TrainingVisionTransformers,
  title = {Training {{Vision Transformers}} for {{Image Retrieval}}},
  author = {El-Nouby, Alaaeldin and Neverova, Natalia and Laptev, Ivan and Jégou, Hervé},
  date = {2021-02-10},
  eprint = {2102.05644},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.05644},
  url = {http://arxiv.org/abs/2102.05644},
  urldate = {2025-07-15},
  abstract = {Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.},
  pubstate = {prepublished}
}

@online{EskandariMiandoab2024LetsArgueBoth,
  title = {“{{Let}}’s {{Argue Both Sides}}”: {{Argument Generation Can Force Small Models}} to {{Utilize Previously Inaccessible Reasoning Capabilities}}},
  author = {Eskandari Miandoab, Kaveh and Sarathy, Vasanth},
  date = {2024-10-27},
  number = {8UN60R},
  eprint = {8UN60R},
  eprinttype = {Qeios},
  doi = {10.32388/8UN60R},
  pubstate = {prepublished}
}

@article{Etaiwi2023SemanticGraph2VecSemanticGraph,
  title = {{{SemanticGraph2Vec}}: {{Semantic}} Graph Embedding for Text Representation},
  shorttitle = {{{SemanticGraph2Vec}}},
  author = {Etaiwi, Wael and Awajan, Arafat},
  date = {2023-03-01},
  journaltitle = {Array},
  shortjournal = {Array},
  volume = {17},
  pages = {100276},
  issn = {2590-0056},
  doi = {10.1016/j.array.2023.100276},
  url = {https://www.sciencedirect.com/science/article/pii/S2590005623000012},
  urldate = {2025-05-15},
  abstract = {Graph embedding is an important representational technique that aims to maintain the structure of a graph while learning low-dimensional representations of its vertices. Semantic relationships between vertices contain essential information regarding the meaning of the represented graph. However, most graph embedding methods do not consider the semantic relationships during the learning process. In this paper, we propose a novel semantic graph embedding approach, called SemanticGraph2Vec. SemanticGraph2Vec learns mappings of vertices into low-dimensional feature spaces that consider the most important semantic relationships between graph vertices. The proposed approach extends and enhances prior work based on a set of random walks of graph vertices by using semantic walks instead of random walks which provides more useful embeddings for text graphs. A set of experiments are conducted to evaluate the performance of SemanticGraph2Vec. SemanticGraph2Vec is employed on a part-of-speech tagging task. Experimental results demonstrate that SemanticGraph2Vec outperforms two state-of-the-art baselines methods in terms of precision and F1 score.}
}

@book{Euzenat2013OntologyMatching,
  title = {Ontology {{Matching}}},
  author = {Euzenat, Jérôme and Shvaiko, Pavel},
  date = {2013},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-38721-0},
  url = {http://link.springer.com/10.1007/978-3-642-38721-0},
  urldate = {2019-01-05},
  isbn = {978-3-642-38720-3 978-3-642-38721-0},
  langid = {english}
}

@online{Explosion2021FactsFigures,
  title = {Facts \& {{Figures}}},
  author = {{Explosion}},
  date = {2021},
  url = {https://spacy.io/usage/facts-figures#benchmarks-speed},
  urldate = {2021-02-09},
  organization = {spaCy Documentation}
}

@inproceedings{Fabbri2021ConvoSummConversationSummarization,
  title = {{{ConvoSumm}}: {{Conversation Summarization Benchmark}} and {{Improved Abstractive Summarization}} with {{Argument Mining}}},
  shorttitle = {{{ConvoSumm}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Fabbri, Alexander and Rahman, Faiaz and Rizvi, Imad and Wang, Borui and Li, Haoran and Mehdad, Yashar and Radev, Dragomir},
  date = {2021-08},
  pages = {6866--6880},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.535},
  url = {https://aclanthology.org/2021.acl-long.535},
  urldate = {2023-10-11},
  abstract = {While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues–viewpoints–assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations.},
  eventtitle = {{{ACL-IJCNLP}} 2021}
}

@software{Falcon2019PyTorchLightning,
  title = {{{PyTorch Lightning}}},
  author = {Falcon, William and {The PyTorch Lightning team}},
  date = {2019-03},
  doi = {10.5281/zenodo.3828935},
  url = {https://github.com/Lightning-AI/lightning},
  urldate = {2025-07-15},
  abstract = {Pretrain, finetune ANY AI model of ANY size on multiple GPUs, TPUs with zero code changes.},
  version = {1.4}
}

@inproceedings{Falk2024OverviewPerpectiveArg2024First,
  title = {Overview of {{PerpectiveArg2024 The First Shared Task}} on {{Perspective Argument Retrieval}}},
  booktitle = {Proceedings of the 11th {{Workshop}} on {{Argument Mining}} ({{ArgMining}} 2024)},
  author = {Falk, Neele and Waldis, Andreas and Gurevych, Iryna},
  editor = {Ajjour, Yamen and Bar-Haim, Roy and El Baff, Roxanne and Liu, Zhexiong and Skitalinskaya, Gabriella},
  date = {2024-08},
  pages = {130--149},
  publisher = {Association for Computational Linguistics},
  location = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.argmining-1.14},
  url = {https://aclanthology.org/2024.argmining-1.14/},
  urldate = {2025-09-03},
  abstract = {Argument retrieval is the task of finding relevant arguments for a given query. While existing approaches rely solely on the semantic alignment of queries and arguments, this first shared task on perspective argument retrieval incorporates perspectives during retrieval, ac- counting for latent influences in argumenta- tion. We present a novel multilingual dataset covering demographic and socio-cultural (so- cio) variables, such as age, gender, and politi- cal attitude, representing minority and major- ity groups in society. We distinguish between three scenarios to explore how retrieval systems consider explicitly (in both query and corpus) and implicitly (only in query) formulated per- spectives. This paper provides an overview of this shared task and summarizes the results of the six submitted systems. We find substantial challenges in incorporating perspectivism, especially when aiming for personalization based solely on the text of arguments without explicitly providing socio profiles. Moreover, re- trieval systems tend to be biased towards the majority group but partially mitigate bias for the female gender. While we bootstrap per- spective argument retrieval, further research is essential to optimize retrieval systems to facilitate personalization and reduce polarization.},
  eventtitle = {{{ArgMining}} 2024}
}

@inproceedings{Fan2024SurveyRAGMeeting,
  title = {A {{Survey}} on {{RAG Meeting LLMs}}: {{Towards Retrieval-Augmented Large Language Models}}},
  shorttitle = {A {{Survey}} on {{RAG Meeting LLMs}}},
  booktitle = {Proceedings of the 30th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  date = {2024-08-24},
  series = {{{KDD}} '24},
  pages = {6491--6501},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3637528.3671470},
  url = {https://dl.acm.org/doi/10.1145/3637528.3671470},
  urldate = {2025-06-06},
  abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/},
  isbn = {979-8-4007-0490-1}
}

@inproceedings{Fang2025KiRAGKnowledgeDrivenIterative,
  title = {{{KiRAG}}: {{Knowledge-Driven Iterative Retriever}} for {{Enhancing Retrieval-Augmented Generation}}},
  shorttitle = {{{KiRAG}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Fang, Jinyuan and Meng, Zaiqiao and MacDonald, Craig},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {18969--18985},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.929/},
  urldate = {2025-07-29},
  abstract = {Iterative retrieval-augmented generation (iRAG) models offer an effective approach for multihop question answering (QA). However, their retrieval processes face two key challenges: (1) they can be disrupted by irrelevant documents or factually inaccurate chain-of-thoughts; (2) their retrievers are not designed to dynamically adapt to the evolving information needs in multi-step reasoning, making it difficult to identify and retrieve the missing information required at each iterative step. Therefore, we propose KiRAG, which uses a knowledge-driven iterative retriever model to enhance the retrieval process of iRAG. Specifically, KiRAG decomposes documents into knowledge triples and performs iterative retrieval with these triples to enable a factually reliable retrieval process. Moreover, KiRAG integrates reasoning into the retrieval process to dynamically identify and retrieve knowledge that bridges information gaps, effectively adapting to the evolving information needs. Empirical results show that KiRAG significantly outperforms existing iRAG models, with an average improvement of 9.40\% in R@3 and 5.14\% in F1 on multi-hop QA datasets.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@article{Farquhar2024DetectingHallucinationsLarge,
  title = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  date = {2024-06},
  journaltitle = {Nature},
  volume = {630},
  number = {8017},
  pages = {625--630},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07421-0},
  url = {https://www.nature.com/articles/s41586-024-07421-0},
  urldate = {2024-06-24},
  abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has~been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
  langid = {english}
}

@inproceedings{Fatemi2023TalkGraphEncoding,
  title = {Talk like a {{Graph}}: {{Encoding Graphs}} for {{Large Language Models}}},
  shorttitle = {Talk like a {{Graph}}},
  author = {Fatemi, Bahare and Halcrow, Jonathan and Perozzi, Bryan},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=IuXR1CCrSi},
  urldate = {2025-03-24},
  abstract = {Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8\% to 61.8\%, depending on the task.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{Feely2025UsingPseudoCases,
  title = {Using {{Pseudo Cases}} and~{{Stratified Case-Based Reasoning}} to~{{Generate}} and~{{Evaluate Training Adjustments}} for~{{Marathon Runners}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Feely, Ciara and Caulfield, Brian and Lawlor, Aonghus and Smyth, Barry},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  pages = {88--101},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77918-3_7},
  abstract = {Recommender systems have become a regular feature in our daily lives. They influence the books we read, the movies we watch and the content we consume on social media. There is opportunity to apply recommender systems to more complex domains, such as exercise, and in this paper we consider how such systems can play a role in supporting runners as they train for a marathon. However, making recommendations for more complex domains introduces additional challenges such as how to provide varied recommendations and how to evaluate these suggestions. In this work we address both of these issues using a stratified case-based recommendation approach and the use of so-called pseudo-cases for evaluation. The stratified approach allows for different recommendations to be generated for each runner based on whether they would like to continue along their current training trajectory, or target a more ambitious or a more conservative goal. We further describe how to evaluate these recommendations in terms of their feasibility, plausibility, effectiveness and safety using a large-scale, real-world dataset of more than 130,000 runners and their marathon training experiences.},
  isbn = {978-3-031-77918-3},
  langid = {english}
}

@inproceedings{Feger2020StructureContentAssessing,
  title = {Structure or {{Content}}? {{Towards Assessing Argument Relevance}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Computational Models}} of {{Argument}}},
  author = {Feger, Marc and Steinmann, Jan and Meter, Christian},
  editor = {Prakken, Henry and Bistarelli, Stefano and Santini, Francesco and Taticchi, Carlo},
  date = {2020},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {326},
  pages = {203--214},
  publisher = {IOS Press},
  location = {Perugia, Italy},
  doi = {10.3233/FAIA200505},
  abstract = {In this paper, we provide a detailed analysis of PageRank to determine the relevance of arguments along with content- and knowledge-based methods from the field of natural language processing. We do not only show how the cross-linking of arguments is only slightly involved in the recognition of relevance, we rather show how basic common knowledge and reader-involving methods outperform the purely structure-related PageRank. The methods we propose are based on the latest research and correlate strongly with human awareness regarding the relevance of arguments. Altogether, we show that PageRank does not fully capture the relevance of arguments and must be extended by a contextual level in order to take concepts of natural language into account at the web level, as they are unavoidably involved in argumentation.},
  eventtitle = {Computational {{Models}} of {{Argument}}}
}

@book{Fellbaum1998WordNetElectronicLexical,
  title = {{{WordNet}}: {{An Electronic Lexical Database}}},
  shorttitle = {{{WordNet}}},
  editor = {Fellbaum, Christiane},
  date = {1998},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/7287.001.0001},
  url = {https://direct.mit.edu/books/book/1928/wordnetan-electronic-lexical-database},
  urldate = {2021-02-12},
  isbn = {978-0-262-27255-1},
  langid = {english}
}

@inproceedings{Feng2024CBRRenCaseBasedReasoning,
  title = {{{CBR-Ren}}: {{A Case-Based Reasoning Driven Retriever-Generator Model}} for~{{Hybrid Long-Form Numerical Reasoning}}},
  shorttitle = {{{CBR-Ren}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Feng, Boda and Gao, Hui and Zhang, Peng and Zhang, Jing},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {111--126},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_8},
  abstract = {Numerical reasoning over hybrid data aims to extract critical facts from long-form documents and tables, and generate arithmetic expressions based on these facts to answer the question. Most existing methods are based on the retriever-generator model. However, the inferential power of the retriever-generator model is poor, resulting in insufficient attention to critical facts. To solve these problems, combining Large Language Model (LLM) and Case-Based Reasoning (CBR), we propose a Case-Based Driven Retriever-generator model (CBR-Ren) to enhance the ability of the retriever-generator model for retrieving and distinguishing critical facts. In the retrieval stage, the model introduces a golden explanation by prompt technology of LLM, which helps the retriever construct explicit templates for inferring critical facts and reduces the impact of non-critical facts on the generator. In the generator stage, the CBR-driven retrieval algorithm enhances the representation learning ability of the encoder and obtains the relevant knowledge in decoder history. In addition, the model proposes fact weighting, which enhances the ability to locate critical facts and helps to generate correct numerical expressions. Experimental results on the FinQA and ConvFinQA demonstrate the effectiveness of CBR-Ren, which outperforms all the baselines.},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Ferrara2017UnsupervisedDetectionArgumentative,
  title = {Unsupervised {{Detection}} of {{Argumentative Units}} Though {{Topic Modeling Techniques}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Argument Mining}}},
  author = {Ferrara, Alfio and Montanelli, Stefano and Petasis, Georgios},
  date = {2017-09},
  pages = {97--107},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/W17-5113},
  url = {https://www.aclweb.org/anthology/W17-5113},
  urldate = {2020-10-21},
  abstract = {In this paper we present a new unsupervised approach, “Attraction to Topics” – A2T , for the detection of argumentative units, a sub-task of argument mining. Motivated by the importance of topic identification in manual annotation, we examine whether topic modeling can be used for performing unsupervised detection of argumentative sentences, and to what extend topic modeling can be used to classify sentences as claims and premises. Preliminary evaluation results suggest that topic information can be successfully used for the detection of argumentative sentences, at least for corpora used for evaluation. Our approach has been evaluated on two English corpora, the first of which contains 90 persuasive essays, while the second is a collection of 340 documents from user generated content.}
}

@article{Ferraz2013OntologyAssociationRules,
  title = {Ontology in Association Rules},
  author = {Ferraz, Inhaúma Neves and Garcia, Ana Cristina Bicharra},
  date = {2013-09-11},
  journaltitle = {SpringerPlus},
  shortjournal = {Springerplus},
  volume = {2},
  eprint = {24083103},
  eprinttype = {pubmed},
  issn = {2193-1801},
  doi = {10.1186/2193-1801-2-452},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3786067/},
  urldate = {2019-10-19},
  abstract = {Data mining has emerged to address the problem of transforming data into useful knowledge. Although most data mining techniques, such as the use of association rules, may substantially reduce the search effort over large data sets, often, the consequential outcomes surpass the amount of information humanly manageable. On the other hand, important association rules may be overlooked owing to the setting of the support threshold, which is a very subjective metric, but rooted in most data mining techniques. This paper presents a study on the effects, in terms of precision and recall, of using a data preparation technique, called SemPrune, which is built on domain ontology. SemPrune is intended for pre- and post-processing phases of data mining. Identifying generalization/specialization relations, as well as composition/decomposition relations, is the key to successfully applying SemPrune.},
  pmcid = {PMC3786067}
}

@article{Ferreira2016JointlyLearningEmbed,
  title = {Jointly {{Learning}} to {{Embed}} and {{Predict}} with {{Multiple Languages}}.},
  author = {Ferreira, Daniel C and Martins, André F T and Almeida, Mariana S C},
  date = {2016-01-01},
  journaltitle = {ACL}
}

@thesis{Figueroa2022FineCoarseGranular,
  title = {Fine and {{Coarse Granular Argument Identification}} and {{Classification}} in {{Persuasive Essays}}},
  author = {Figueroa, Arturo and Pednekar, Ashwin and Mehyar, Bilal},
  date = {2022-04-26},
  institution = {Trier University},
  location = {Trier, Germany},
  abstract = {Argumentation is necessary for humans to communicate thoughts in order to establish a standpoint. Argument Mining is a recent research field in NLP that addresses the task of the automatic identification and extraction of Argumentative Discourse Units (ADUs) from unstructured natural language text. This research focuses on investigating different methods of segmenting text at varying levels of granularity for ADU Identification and Classification in persuasive essays. Two different measures of assessing the quality of unit segmentation are introduced. We investigate three different classification approaches with five types of classifiers. The findings show that the sub-clause level segmentation achieves a better segmentation quality than the sentence level segmentation which is often used in previous research, and performs relatively well in classification.},
  langid = {english}
}

@inproceedings{Filter2026BalancedReciprocityData,
  title = {Balanced {{Reciprocity}} for~{{Data Sharing}}—{{Axiomatization}} and~{{Mechanism Design}}},
  booktitle = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}},
  author = {Filter, Björn and Schubsda, Jan and Möller, Ralf and Özçep, Özgür Lütfü},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  pages = {18--31},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6_2},
  abstract = {Collaborative environments, particularly in data-sharing and cooperative settings, require robust mechanisms to fairly allocate rewards among participants. In many scenarios, participants contribute valuable data that improve the collective outcome, yet ensuring that each party is fairly compensated for their contribution remains a complex challenge. Classical cooperative game theory develops mechanisms under the assumption of non-replicable resources, which does not fit the scenario of sharing models or data. Hence, it does not address the potential imbalance in reciprocal benefits among pairs of participants, which can lead to strategic exploitation and, consequently, unfair reward allocations. We explicate fairness w.r.t. balanced reciprocity with an axiom and prove the existence of a reward allocation mechanism that fulfills this axiom, as well as other well-known axioms intended to capture incentivization and fairness. By emphasizing mutual fairness, our approach mitigates the risk of exploitation, fosters stable cooperation, and aligns with the principle of fair exchange. We propose a solution that guarantees proportional benefit allocation while maximizing group welfare under these constraints.},
  isbn = {978-3-032-02813-6},
  langid = {english}
}

@inproceedings{Findlater2004ComparisonStaticAdaptive,
  title = {A Comparison of Static, Adaptive, and Adaptable Menus},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Findlater, Leah and McGrenere, Joanna},
  date = {2004-04-25},
  series = {{{CHI}} '04},
  pages = {89--96},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/985692.985704},
  url = {https://doi.org/10.1145/985692.985704},
  urldate = {2022-05-02},
  abstract = {Software applications continue to grow in terms of the number of features they offer, making personalization increasingly important. Research has shown that most users prefer the control afforded by an adaptable approach to personalization rather than a system-controlled adaptive approach. No study, however, has compared the efficiency of the two approaches. In a controlled lab study with 27 subjects we compared the measured and perceived efficiency of three menu conditions: static, adaptable and adaptive. Each was implemented as a split menu, in which the top four items remained static, were adaptable by the subject, or adapted according to the subject's frequently and recently used items. The static menu was found to be significantly faster than the adaptive menu, and the adaptable menu was found to be significantly faster than the adaptive menu under certain conditions. The majority of users preferred the adaptable menu overall. Implications for interface design are discussed.},
  isbn = {978-1-58113-702-6}
}

@book{Firth1957SynopsisLinguisticTheory,
  title = {A {{Synopsis}} of {{Linguistic Theory}}, 1930-1955},
  author = {Firth, John Rupert},
  date = {1957},
  series = {Studies in {{Linguistic Analysis}}},
  eprint = {T8LDtgAACAAJ},
  eprinttype = {googlebooks},
  langid = {english},
  pagetotal = {book}
}

@article{Fleiss1973EquivalenceWeightedKappa,
  title = {The {{Equivalence}} of {{Weighted Kappa}} and the {{Intraclass Correlation Coefficient}} as {{Measures}} of {{Reliability}}},
  author = {Fleiss, Joseph L. and Cohen, Jacob},
  date = {1973-10-01},
  journaltitle = {Educational and Psychological Measurement},
  shortjournal = {Educational and Psychological Measurement},
  volume = {33},
  number = {3},
  pages = {613--619},
  publisher = {SAGE Publications Inc},
  issn = {0013-1644},
  doi = {10.1177/001316447303300309},
  url = {https://doi.org/10.1177/001316447303300309},
  urldate = {2021-03-07},
  langid = {english}
}

@article{Forbus1995MACFACModel,
  title = {{{MAC}}/{{FAC}} - {{A Model}} of {{Similarity-Based Retrieval}}.},
  author = {Forbus, Kenneth D and Gentner, Dedre and Law, Keith},
  date = {1995-01-01},
  journaltitle = {Cognitive Science},
  volume = {19},
  number = {2},
  pages = {141--205},
  doi = {10.1207/s15516709cog1902_1},
  url = {http://doi.wiley.com/10.1207/s15516709cog1902_1},
  urldate = {2018-09-01}
}

@article{Forbus2017AnalogyRelationalRepresentations,
  title = {Analogy and {{Relational Representations}} in the {{Companion Cognitive Architecture}}},
  author = {Forbus, Kenneth D. and Hinrich, Thomas},
  date = {2017-12-28},
  journaltitle = {AI Magazine},
  volume = {38},
  number = {4},
  pages = {34--42},
  issn = {2371-9621},
  doi = {10.1609/aimag.v38i4.2743},
  url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/2743},
  urldate = {2020-05-31},
  abstract = {The Companion cognitive architecture is aimed at reaching human-level AI by creating software social organisms, systems that interact with people using natural modalities, working and learning over extended periods of time as collaborators rather than tools. Our two central hypotheses about how to achieve this are (1) analogical reasoning and learning are central to cognition, and (2) qualitative representations provide a level of description that facilitates reasoning, learning, and communication. This paper discusses the evidence we have gathered supporting these hypotheses from our experiments with the Companion architecture. Although we are far from our ultimate goals, these experiments provide strong breadth for the utility of analogy and QR across a range of tasks. We also discuss three lessons learned and highlight three important open problems for cognitive systems research more broadly.},
  issue = {4},
  langid = {english}
}

@article{Fox1989StopListGeneral,
  title = {A {{Stop List}} for {{General Text}}},
  author = {Fox, Christopher},
  date = {1989-09},
  journaltitle = {SIGIR Forum},
  volume = {24},
  number = {1--2},
  pages = {19--21},
  issn = {0163-5840},
  doi = {10.1145/378881.378888},
  abstract = {A stop list, or negative dictionary is a device used in automatic indexing to filter out words that would make poor index terms. Traditionally stop lists are supposed to have included only the most frequently occurring words. In practice, however, stop lists have tended to include infrequently occurring words, and have not included many frequently occurring words. Infrequently occurring words seem to have been included because stop list compilers have not, for whatever reason, consulted empirical studies of word frequencies. Frequently occurring words seem to have been left out for the same reason, and also because many of them might still be important as index terms.This paper reports an exercise in generating a stop list for general text based on the Brown corpus of 1,014,000 words drawn from a broad range of literature in English. We start with a list of tokens occurring more than 300 times in the Brown corpus. From this list of 278 words, 32 are culled on the grounds that they are too important as potential index terms. Twenty-six words are then added to the list in the belief that they may occur very frequently in certain kinds of literature. Finally, 149 words are added to the list because the finite state machine based filter in which this list is intended to be used is able to filter them at almost no cost. The final product is a list of 421 stop words that should be maximally efficient and effective in filtering the most frequently occurring and semantically neutral words in general literature in English.}
}

@online{Francesco2024GraphNeuralReRanking,
  title = {Graph {{Neural Re-Ranking}} via {{Corpus Graph}}},
  author = {Francesco, Andrea Giuseppe Di and Giannetti, Christian and Tonellotto, Nicola and Silvestri, Fabrizio},
  date = {2024-06-17},
  eprint = {2406.11720},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.11720},
  url = {http://arxiv.org/abs/2406.11720},
  urldate = {2025-03-24},
  abstract = {Re-ranking systems aim to reorder an initial list of documents to satisfy better the information needs associated with a user-provided query. Modern re-rankers predominantly rely on neural network models, which have proven highly effective in representing samples from various modalities. However, these models typically evaluate query-document pairs in isolation, neglecting the underlying document distribution that could enhance the quality of the re-ranked list. To address this limitation, we propose Graph Neural Re-Ranking (GNRR), a pipeline based on Graph Neural Networks (GNNs), that enables each query to consider documents distribution during inference. Our approach models document relationships through corpus subgraphs and encodes their representations using GNNs. Through extensive experiments, we demonstrate that GNNs effectively capture cross-document interactions, improving performance on popular ranking metrics. In TREC-DL19, we observe a relative improvement of 5.8\% in Average Precision compared to our baseline. These findings suggest that integrating the GNN segment offers significant advantages, especially in scenarios where understanding the broader context of documents is crucial.},
  pubstate = {prepublished}
}

@inproceedings{Francois2025KnowledgeRepresentationApproach,
  title = {A {{Knowledge Representation Approach}} for~{{Reasoning}} with~{{Adaptation Rules}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {François, Nicolas and Lieber, Jean},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  pages = {204--219},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_14},
  abstract = {The adaptation knowledge container of a CBR system is often represented by adaptation rules that can be learned from the case base using various approaches proposed in the CBR literature. However, the formal representation of such rules has~been much less investigated. This paper introduces a formalism for representing such rules, given a formalism that can be used to express cases in~an attribute-value formalism. One benefit of this study is a case retrieval algorithm for~which the retrieved case is the best one with respect to adaptation (i.e. the one that requires the least adaptation effort as defined by the cost of adaptation rule sequences). Moreover, when attributes have Boolean range, its complexity is independent of the size of the case base.},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Franz2016CytoscapeJsGraph,
  title = {Cytoscape.Js: A Graph Theory Library for Visualisation and Analysis},
  shorttitle = {Cytoscape.Js},
  author = {Franz, Max and Lopes, Christian T. and Huck, Gerardo and Dong, Yue and Sumer, Onur and Bader, Gary D.},
  date = {2016-01-15},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {32},
  number = {2},
  pages = {309--311},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btv557},
  url = {https://doi.org/10.1093/bioinformatics/btv557},
  urldate = {2022-04-21},
  abstract = {Summary: Cytoscape.js is an open-source JavaScript-based graph library. Its most common use case is as a visualization software component, so it can be used to render interactive graphs in a web browser. It also can be used in a headless manner, useful for graph operations on a server, such as Node.js. Availability and implementation: Cytoscape.js is implemented in JavaScript. Documentation, downloads and source code are available at http://js.cytoscape.org . Contact:gary.bader@utoronto.ca}
}

@book{Freeman2011ArgumentStructureRepresentation,
  title = {Argument {{Structure}}: {{Representation}} and {{Theory}}},
  shorttitle = {Argument {{Structure}}},
  author = {Freeman, James B.},
  date = {2011},
  series = {Argumentation {{Library}}},
  volume = {18},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-007-0357-5},
  url = {https://link.springer.com/10.1007/978-94-007-0357-5},
  urldate = {2025-09-04},
  isbn = {978-94-007-0356-8 978-94-007-0357-5},
  langid = {english}
}

@book{Freeman2011DialecticsMacrostructureArguments,
  title = {Dialectics and the {{Macrostructure}} of {{Arguments}}: {{A Theory}} of {{Argument Structure}}},
  shorttitle = {Dialectics and the {{Macrostructure}} of {{Arguments}}},
  author = {Freeman, James B.},
  date = {2011-09-20},
  publisher = {De Gruyter Mouton},
  doi = {10.1515/9783110875843},
  url = {https://www.degruyterbrill.com/document/doi/10.1515/9783110875843/html},
  urldate = {2025-09-04},
  abstract = {Dialectics and the Macrostructure of Arguments by James B. Freeman was published on September 20, 2011 by De Gruyter Mouton.},
  isbn = {978-3-11-087584-3},
  langid = {english}
}

@inproceedings{Frobe2023ContinuousIntegrationReproducible,
  title = {Continuous {{Integration}} for~{{Reproducible Shared Tasks}} with~{{TIRA}}.Io},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Fröbe, Maik and Wiegmann, Matti and Kolyada, Nikolay and Grahm, Bastian and Elstner, Theresa and Loebe, Frank and Hagen, Matthias and Stein, Benno and Potthast, Martin},
  editor = {Kamps, Jaap and Goeuriot, Lorraine and Crestani, Fabio and Maistro, Maria and Joho, Hideo and Davis, Brian and Gurrin, Cathal and Kruschwitz, Udo and Caputo, Annalina},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {236--241},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-28241-6_20},
  abstract = {A major obstacle to the long-term impact of most shared tasks is their lack of reproducibility. Often only the test collections and the papers of the organizers and participants are published. Third parties who want to independently evaluate the state of the art for a task on other data must re-implement the participants’ software. The tools developed to collect software from participants in shared tasks only partially verify its reliability at the time of submission, much less long-term, and do not enable third parties to reuse it later. We have overhauled the TIRA~Integrated Research Architecture to address all of these issues. The new version simplifies task setup for organizers and software submission for participants, scales from a local computer to the cloud, supports on-demand resource allocation up to parallel CPU and GPU processing, and enables export for local reproduction with just a few lines of code. This is achieved by implementing the TIRA protocol with an industry-standard continuous integration and deployment~(CI/CD) pipeline using Git, Docker, and Kubernetes.},
  isbn = {978-3-031-28241-6},
  langid = {english}
}

@inproceedings{Frobe2023InformationRetrievalExperiment,
  title = {The {{Information Retrieval Experiment Platform}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Fröbe, Maik and Reimer, Jan Heinrich and MacAvaney, Sean and Deckers, Niklas and Reich, Simon and Bevendorff, Janek and Stein, Benno and Hagen, Matthias and Potthast, Martin},
  date = {2023-07-18},
  series = {{{SIGIR}} '23},
  pages = {2826--2836},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3539618.3591888},
  url = {https://dl.acm.org/doi/10.1145/3539618.3591888},
  urldate = {2025-09-03},
  abstract = {We integrate irdatasets, ir\_measures, and PyTerrier with TIRA in the Information Retrieval Experiment Platform (TIREx) to promote more standardized, reproducible, scalable, and even blinded retrieval experiments. Standardization is achieved when a retrieval approach implements PyTerrier's interfaces and the input and output of an experiment are compatible with ir\_datasets and ir\_measures. However, none of this is a must for reproducibility and scalability, as TIRA can run any dockerized software locally or remotely in a cloud-native execution environment. Version control and caching ensure efficient (re)execution. TIRA allows for blind evaluation when an experiment runs on a remote server or cloud not under the control of the experimenter. The test data and ground truth are then hidden from public access, and the retrieval software has to process them in a sandbox that prevents data leaks.We currently host an instance of TIREx with 15 corpora (1.9\textasciitilde billion documents) on which 32 shared retrieval tasks are based. Using Docker images of 50\textasciitilde standard retrieval approaches, we automatically evaluated all approaches on all tasks (50 ⋅ 32 = 1,600 runs) in less than a week on a midsize cluster (1,620 cores and 24 GPUs). This instance of TIREx is open for submissions and will be integrated with the IR Anthology, as well as released open source.},
  isbn = {978-1-4503-9408-6}
}

@inproceedings{Fromm2019TACAMTopicContext,
  title = {{{TACAM}}: {{Topic And Context Aware Argument Mining}}},
  shorttitle = {{{TACAM}}},
  booktitle = {{{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}}},
  author = {Fromm, Michael and Faerman, Evgeniy and Seidl, Thomas},
  date = {2019-10-14},
  series = {{{WI}} '19},
  pages = {99--106},
  publisher = {Association for Computing Machinery},
  location = {Thessaloniki, Greece},
  doi = {10.1145/3350546.3352506},
  url = {https://doi.org/10.1145/3350546.3352506},
  urldate = {2020-09-02},
  abstract = {In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.},
  isbn = {978-1-4503-6934-3}
}

@online{Fu2023DecoderOnlyEncoderDecoderInterpreting,
  title = {Decoder-{{Only}} or {{Encoder-Decoder}}? {{Interpreting Language Model}} as a {{Regularized Encoder-Decoder}}},
  shorttitle = {Decoder-{{Only}} or {{Encoder-Decoder}}?},
  author = {Fu, Zihao and Lam, Wai and Yu, Qian and So, Anthony Man-Cho and Hu, Shengding and Liu, Zhiyuan and Collier, Nigel},
  date = {2023-04-08},
  eprint = {2304.04052},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.04052},
  url = {http://arxiv.org/abs/2304.04052},
  urldate = {2024-04-29},
  abstract = {The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.},
  pubstate = {prepublished}
}

@online{Fu2024FeatUpModelAgnosticFramework,
  title = {{{FeatUp}}: {{A Model-Agnostic Framework}} for {{Features}} at {{Any Resolution}}},
  shorttitle = {{{FeatUp}}},
  author = {Fu, Stephanie and Hamilton, Mark and Brandt, Laura and Feldman, Axel and Zhang, Zhoutong and Freeman, William T.},
  date = {2024-03-15},
  eprint = {2403.10516},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.10516},
  urldate = {2024-03-28},
  abstract = {Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training. We show that FeatUp significantly outperforms other feature upsampling and image super-resolution approaches in class activation map generation, transfer learning for segmentation and depth prediction, and end-to-end training for semantic segmentation.},
  pubstate = {prepublished}
}

@inproceedings{Gabel2025EfficientCaseRetrieval,
  title = {Efficient {{Case Retrieval Using Dropout Similarity Highway Multigraphs}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Gabel, Thomas},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  pages = {220--235},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_15},
  abstract = {The time required to retrieve a query’s nearest neighbor may quickly become a CBR system’s bottleneck when its case base contains a large volume of cases. Approximate retrieval techniques that build and employ complex index~data structures can mitigate this issue by providing an acceptable tradeoff between time complexity and retrieval accuracy. In this paper, we propose a specialized multigraph-based index structure composed of multiple nested sub-case bases with individually labeled edge sets. We develop tailored algorithms for both constructing the index~and utilizing it in a more efficient retrieval process, and we also evaluate our approach empirically using established benchmark datasets.},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Galil1986EfficientAlgorithmsFinding,
  title = {Efficient Algorithms for Finding Maximum Matching in Graphs},
  author = {Galil, Zvi},
  date = {1986-03-01},
  journaltitle = {ACM Comput. Surv.},
  volume = {18},
  number = {1},
  pages = {23--38},
  issn = {0360-0300},
  doi = {10.1145/6462.6502},
  url = {https://dl.acm.org/doi/10.1145/6462.6502},
  urldate = {2025-06-02},
  abstract = {This paper surveys the techniques used for designing the most efficient algorithms for finding a maximum cardinality or weighted matching in (general or bipartite) graphs. It also lists some open problems concerning possible improvements in existing algorithms and the existence of fast parallel algorithms for these problems.}
}

@online{Gan2025RetrievalAugmentedGeneration,
  title = {Retrieval {{Augmented Generation Evaluation}} in the {{Era}} of {{Large Language Models}}: {{A Comprehensive Survey}}},
  shorttitle = {Retrieval {{Augmented Generation Evaluation}} in the {{Era}} of {{Large Language Models}}},
  author = {Gan, Aoran and Yu, Hao and Zhang, Kai and Liu, Qi and Yan, Wenyu and Huang, Zhenya and Tong, Shiwei and Hu, Guoping},
  date = {2025-04-21},
  eprint = {2504.14891},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.14891},
  url = {http://arxiv.org/abs/2504.14891},
  urldate = {2025-06-06},
  abstract = {Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development.},
  pubstate = {prepublished}
}

@book{Ganter1999FormalConceptAnalysis,
  title = {Formal {{Concept Analysis}}: {{Mathematical Foundations}}},
  shorttitle = {Formal {{Concept Analysis}}},
  author = {Ganter, Bernhard and Wille, Rudolf},
  date = {1999},
  publisher = {Springer-Verlag},
  location = {Berlin Heidelberg},
  doi = {10.1007/978-3-642-59830-2},
  url = {https://www.springer.com/gp/book/9783540627715},
  urldate = {2020-05-29},
  abstract = {This is the first textbook on formal concept analysis. It gives a systematic presentation of the mathematical foundations and their relations to applications in computer science, especially in data analysis and knowledge processing. Above all, it presents graphical methods for representing conceptual systems that have proved themselves in communicating knowledge. Theory and graphical representation are thus closely coupled together. The mathematical foundations are treated thoroughly and illuminated by means of numerous examples. Since computers are being used ever more widely for knowledge processing, formal methods for conceptual analysis are gaining in importance. This book makes the basic theory for such methods accessible in a compact form.},
  isbn = {978-3-540-62771-5},
  langid = {english}
}

@inproceedings{Gao2021SimCSESimpleContrastive,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  date = {2021-11},
  pages = {6894--6910},
  publisher = {Association for Computational Linguistics},
  location = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.552},
  url = {https://aclanthology.org/2021.emnlp-main.552/},
  urldate = {2025-07-15},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  eventtitle = {{{EMNLP}} 2021}
}

@online{Gao2024RetrievalAugmentedGenerationLarge,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  date = {2024-03-27},
  eprint = {2312.10997},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10997},
  url = {http://arxiv.org/abs/2312.10997},
  urldate = {2024-04-01},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  pubstate = {prepublished}
}

@article{Garcia2004DefeasibleLogicProgramming,
  title = {Defeasible Logic Programming: An Argumentative Approach},
  shorttitle = {Defeasible Logic Programming},
  author = {García, Alejandro J. and Simari, Guillermo R.},
  date = {2004-01},
  journaltitle = {Theory and Practice of Logic Programming},
  volume = {4},
  number = {1--2},
  pages = {95--138},
  issn = {1475-3081, 1471-0684},
  doi = {10.1017/S1471068403001674},
  url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/defeasible-logic-programming-an-argumentative-approach/2FDF8D95C4D7DCC6B240C934066109DC},
  urldate = {2025-09-09},
  abstract = {The work reported here introduces Defeasible Logic Programming(DeLP), a formalism that combines results of Logic Programming andDefeasible Argumentation. DeLP provides the possibility ofrepresenting information in the form of weak rulesin a declarative manner, and a defeasible argumentation inferencemechanism for warranting the entailed conclusions. In DeLP anargumentation formalism will be used for deciding betweencontradictory goals. Queries will be supported by arguments thatcould be defeated by other arguments. A query \$q\$ will succeed when there is anargument \$\{\textbackslash mathcal A\}\$for \$q\$ that iswarranted, i.e. the argument \$\{\textbackslash mathcalA\}\$ that supports \$q\$ is found undefeated by a warrant procedurethat implements a dialectical analysis. The defeasible argumentationbasis of DeLP allows to build applications that deal with incompleteand contradictory information in dynamic domains. Thus, theresulting approach is suitable for representing agent's knowledgeand for providing an argumentation based reasoning mechanism toagents.},
  langid = {english}
}

@inproceedings{Garcia2009IndexBalancedAccuracy,
  title = {Index of {{Balanced Accuracy}}: {{A Performance Measure}} for {{Skewed Class Distributions}}},
  shorttitle = {Index of {{Balanced Accuracy}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {García, V. and Mollineda, R. A. and Sánchez, J. S.},
  date = {2009-06-10},
  pages = {441--448},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/978-3-642-02172-5_57},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-02172-5_57},
  urldate = {2021-03-13},
  abstract = {This paper introduces a new metric, named Index of Balanced Accuracy, for evaluating learning processes in two-class imbalanced domains. The method combines an unbiased index of its overall accuracy...},
  eventtitle = {Iberian {{Conference}} on {{Pattern Recognition}} and {{Image Analysis}}},
  langid = {english}
}

@inproceedings{Garikaparthi2025IRISInteractiveResearch,
  title = {{{IRIS}}: {{Interactive Research Ideation System}} for {{Accelerating Scientific Discovery}}},
  shorttitle = {{{IRIS}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Garikaparthi, Aniketh and Patwardhan, Manasi and Vig, Lovekesh and Cohan, Arman},
  editor = {Mishra, Pushkar and Muresan, Smaranda and Yu, Tao},
  date = {2025-07},
  pages = {592--603},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-demo.57/},
  urldate = {2025-07-29},
  abstract = {The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS for interactive hypothesis generation, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System.},
  isbn = {979-8-89176-253-4}
}

@inproceedings{Gemechu2019DecompositionalArgumentMining,
  title = {Decompositional {{Argument Mining}}: {{A General Purpose Approach}} for {{Argument Graph Construction}}},
  shorttitle = {Decompositional {{Argument Mining}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gemechu, Debela and Reed, Chris},
  date = {2019-07},
  pages = {516--526},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/P19-1049},
  url = {https://www.aclweb.org/anthology/P19-1049},
  urldate = {2020-09-02},
  abstract = {This work presents an approach decomposing propositions into four functional components and identify the patterns linking those components to determine argument structure. The entities addressed by a proposition are target concepts and the features selected to make a point about the target concepts are aspects. A line of reasoning is followed by providing evidence for the points made about the target concepts via aspects. Opinions on target concepts and opinions on aspects are used to support or attack the ideas expressed by target concepts and aspects. The relations between aspects, target concepts, opinions on target concepts and aspects are used to infer the argument relations. Propositions are connected iteratively to form a graph structure. The approach is generic in that it is not tuned for a specific corpus and evaluated on three different corpora from the literature: AAEC, AMT, US2016G1tv and achieved an F score of 0.79, 0.77 and 0.64, respectively.},
  eventtitle = {{{ACL}} 2019}
}

@inproceedings{Gemechu2024ARIESGeneralBenchmark,
  title = {{{ARIES}}: {{A General Benchmark}} for {{Argument Relation Identification}}},
  shorttitle = {{{ARIES}}},
  booktitle = {Proceedings of the 11th {{Workshop}} on {{Argument Mining}}},
  author = {Gemechu, Debela and Ruiz-Dolz, Ramon and Reed, Chris},
  editor = {Ajjour, Yamen and Bar-Haim, Roy and El Baff, Roxanne and Liu, Zhexiong and Skitalinskaya, Gabriella},
  date = {2024-08},
  pages = {1--14},
  publisher = {Association for Computational Linguistics},
  location = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.argmining-1.1},
  url = {https://aclanthology.org/2024.argmining-1.1/},
  urldate = {2025-07-28},
  abstract = {Measuring advances in argument mining is one of the main challenges in the area. Different theories of argument, heterogeneous annotations, and a varied set of argumentation domains make it difficult to contextualise and understand the results reported in different work from a general perspective. In this paper, we present ARIES, a general benchmark for Argument Relation Identification aimed at providing with a standard evaluation for argument mining research. ARIES covers the three different language modelling approaches: sequence and token modelling, and sequence-to-sequence-to-sequence alignment, together with the three main Transformer-based model architectures: encoder-only, decoder-only, and encoder-decoder. Furthermore, the benchmark consists of eight different argument mining datasets, covering the most common argumentation domains, and standardised with the same annotation structures. This paper provides a first comprehensive and comparative set of results in argument mining across a broad range of configurations to compare with, both advancing the state-of-the-art, and establishing a standard way to measure future advances in the area. Across varied task setups and architectures, our experiments reveal consistent challenges in cross-dataset evaluation, with notably poor results. Given the models' struggle to acquire transferable skills, the task remains challenging, opening avenues for future research.},
  eventtitle = {{{ArgMining}} 2024}
}

@inproceedings{Gemechu2024ExternalKnowledgeDrivenArgument,
  title = {External {{Knowledge-Driven Argument Mining}}: {{Leveraging Attention-Enhanced Multi-Network Models}}},
  shorttitle = {External {{Knowledge-Driven Argument Mining}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gemechu, Debela and Reed, Chris},
  editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  date = {2024-11},
  pages = {3688--3709},
  publisher = {Association for Computational Linguistics},
  location = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.216},
  url = {https://aclanthology.org/2024.emnlp-main.216/},
  urldate = {2025-07-28},
  abstract = {Argument mining (AM) involves the identification of argument relations (AR) between Argumentative Discourse Units (ADUs). The essence of ARs among ADUs is context-dependent and lies in maintaining a coherent flow of ideas, often centered around the relations between discussed entities, topics, themes or concepts. However, these relations are not always explicitly stated; rather, inferred from implicit chains of reasoning connecting the concepts addressed in the ADUs. While humans can infer such background knowledge, machines face challenges when the contextual cues are not explicitly provided. This paper leverages external resources, including WordNet, ConceptNet, and Wikipedia to identify semantic paths (knowledge paths) connecting the concepts discussed in the ADUs to obtain the implicit chains of reasoning. To effectively leverage these paths for AR prediction, we propose attention-based Multi-Network architectures. Various architecture are evaluated on the external resources, and the Wikipedia based configuration attains F-scores of 0.85, 0.84, 0.70, and 0.87, respectively, on four diverse datasets, showing strong performance over the baselines.},
  eventtitle = {{{EMNLP}} 2024}
}

@inproceedings{Gemechu2025CUMAMCoherenceDrivenUnified,
  title = {{{CU-MAM}}: {{Coherence-Driven Unified Macro-Structures}} for {{Argument Mining}}},
  shorttitle = {{{CU-MAM}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gemechu, Debela and Reed, Chris},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {19731--19749},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.969/},
  urldate = {2025-07-28},
  abstract = {Argument Mining (AM) involves the automatic identification of argument structure in natural language. Traditional AM methods rely on micro-structural features derived from the internal properties of individual Argumentative Discourse Units (ADUs). However, argument structure is shaped by a macro-structure capturing the functional interdependence among ADUs. This macro-structure consists of segments, where each segment contains ADUs that fulfill specific roles to maintain coherence within the segment (**local coherence**) and across segments (**global coherence**). This paper presents an approach that models macro-structure, capturing both local and global coherence to identify argument structures. Experiments on heterogeneous datasets demonstrate superior performance in both in-dataset and cross-dataset evaluations. The cross-dataset evaluation shows that macro-structure enhances transferability to unseen datasets.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Gemechu2025NaturalLanguageReasoning,
  title = {Natural {{Language Reasoning}} in {{Large Language Models}}: {{Analysis}} and {{Evaluation}}},
  shorttitle = {Natural {{Language Reasoning}} in {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2025},
  author = {Gemechu, Debela and Ruiz-Dolz, Ramon and Beyer, Henrike and Reed, Chris},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {3717--3741},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.findings-acl.192/},
  urldate = {2025-07-28},
  abstract = {While Large Language Models (LLMs) have demonstrated promising results on a range of reasoning benchmarks—particularly in formal logic, mathematical tasks, and Chain-of-Thought prompting—less is known about their capabilities in unconstrained natural language reasoning. Argumentative reasoning, a form of reasoning naturally expressed in language and central to everyday discourse, presents unique challenges for LLMs due to its reliance on context, implicit assumptions, and value judgments. This paper addresses a gap in the study of reasoning in LLMs by presenting the first large-scale evaluation of their unconstrained natural language reasoning capabilities based on natural language argumentation. The paper offers three contributions: (i) the formalisation of a new strategy designed to evaluate argumentative reasoning in LLMs: argument-component selection; (ii) the creation of the Argument Reasoning Tasks (ART) dataset, a new benchmark for argument-component selection based on argument structures for natural language reasoning; and (iii) an extensive experimental analysis involving four different models, demonstrating the limitations of LLMs on natural language reasoning tasks.},
  eventtitle = {Findings 2025},
  isbn = {979-8-89176-256-5}
}

@inproceedings{Gemechu2025OpenArgumentMining,
  title = {The {{Open Argument Mining Framework}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Gemechu, Debela and Ruiz-Dolz, Ramon and Górska, Kamila and Moslemnejad, Somaye and Maguire, Eimear and Zografistou, Dimitra and Jo, Yohan and Lawrence, John and Reed, Chris},
  editor = {Mishra, Pushkar and Muresan, Smaranda and Yu, Tao},
  date = {2025-07},
  pages = {318--328},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-demo.31/},
  urldate = {2025-07-28},
  abstract = {Despite extensive research in Argument Mining (AM), the field faces significant challenges in limited reproducibility, difficulty in comparing systems due to varying task combinations, and a lack of interoperability caused by the heterogeneous nature of argumentation theory. These challenges are further exacerbated by the absence of dedicated tools, with most advancements remaining isolated research outputs rather than reusable systems. The \textbackslash textttoAMF (Open Argument Mining Framework) addresses these issues by providing an open-source, modular, and scalable platform that unifies diverse AM methods. Initially released with seventeen integrated modules, the \textbackslash textttoAMF serves as a starting point for researchers and developers to build, experiment with, and deploy AM pipelines while ensuring interoperability and allowing multiple theories of argumentation to co-exist within the same framework. Its flexible design supports integration via Python APIs, drag-and-drop tools, and web interfaces, streamlining AM development for research and industry setup, facilitating method comparison, and reproducibility.},
  isbn = {979-8-89176-253-4}
}

@inproceedings{Gemechu2025PracticalSolutionsPractical,
  title = {Practical {{Solutions}} to {{Practical Problems}} in {{Developing Argument Mining Systems}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Gemechu, Debela and Ruiz-Dolz, Ramon and Lawrence, John and Reed, Chris},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {100--106},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.9/},
  urldate = {2025-07-28},
  abstract = {The Open Argument Mining Framework (oAMF) addresses key challenges in argument mining research which still persist despite the field's impressive growth. Researchers often face difficulties with cross-system comparisons, incompatible representation languages, and limited access to reusable tools. The oAMF introduces a standardised yet flexible architecture that enables seamless component benchmarking, rapid pipeline prototyping using elements from diverse research traditions, and unified evaluation methodologies that preserve theoretical compatibility. By reducing technical overhead, the framework allows researchers to focus on advancing core argument mining capabilities rather than reimplementing infrastructure, fostering greater collaboration at a time when computational reasoning is increasingly vital in the era of large language models.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@online{GeminiTeam2024GeminiFamilyHighly,
  title = {Gemini: {{A Family}} of {{Highly Capable Multimodal Models}}},
  shorttitle = {Gemini},
  author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and family=Glehn, given=Tamara, prefix=von, useprefix=true and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and family=Liedekerke, given=Raoul, prefix=de, useprefix=true and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and family=Driessche, given=George, prefix=van den, useprefix=false and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and family=Amersfoort, given=Joost, prefix=van, useprefix=true and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and family=Dincklage, given=Daniel, prefix=von, useprefix=true and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, Héctor Fernández and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and family=Weide, given=Tom, prefix=van der, useprefix=true and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, Rémi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and family=Girgin, given=Ser, prefix=tan, useprefix=false and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and family=Salm, given=Claudia, prefix=van der, useprefix=true and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and family=Cesare, given=Dario, prefix=de, useprefix=true and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M., Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias and family=Kerkhof, given=Jan, prefix=van de, useprefix=true and Pikus, Marcin and Zaher, Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Vu, Tu and Andreev, Alek and He, Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
  date = {2024-06-17},
  eprint = {2312.11805},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.11805},
  url = {http://arxiv.org/abs/2312.11805},
  urldate = {2024-06-24},
  abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
  pubstate = {prepublished}
}

@incollection{Gentner2012AnalogicalReasoning,
  title = {Analogical {{Reasoning}}},
  booktitle = {Encyclopedia of {{Human Behavior}} ({{Second Edition}})},
  author = {Gentner, D. and Smith, L.},
  editor = {Ramachandran, V. S.},
  date = {2012-01-01},
  pages = {130--136},
  publisher = {Academic Press},
  location = {San Diego},
  doi = {10.1016/B978-0-12-375000-6.00022-7},
  url = {https://www.sciencedirect.com/science/article/pii/B9780123750006000227},
  urldate = {2021-02-13},
  abstract = {Analogical reasoning is a kind of reasoning that is based on finding a common relational system between two situations, exemplars, or domains. When such a common system can be found, then what is known about one situation can be used to infer new information about the other. The basic intuition behind analogical reasoning is that when there are substantial similarities between situations, there are likely to be further similarities. This article describes the processes involved in analogical reasoning, reviews seminal research and recent developments in the field, and proposes new avenues of investigation.},
  isbn = {978-0-08-096180-4},
  langid = {english}
}

@article{Gilardi2023ChatGPTOutperformsCrowd,
  title = {{{ChatGPT}} Outperforms Crowd Workers for Text-Annotation Tasks},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
  date = {2023-07-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {30},
  pages = {e2305016120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2305016120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2305016120},
  urldate = {2024-02-10},
  abstract = {Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.}
}

@unpublished{Gleize2019AreYouConvinced,
  title = {Are {{You Convinced}}? {{Choosing}} the {{More Convincing Evidence}} with a {{Siamese Network}}},
  shorttitle = {Are {{You Convinced}}?},
  author = {Gleize, Martin and Shnarch, Eyal and Choshen, Leshem and Dankin, Lena and Moshkowich, Guy and Aharonov, Ranit and Slonim, Noam},
  date = {2019-07-21},
  eprint = {1907.08971},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.08971},
  urldate = {2019-09-25},
  abstract = {With the advancement in argument detection, we suggest to pay more attention to the challenging task of identifying the more convincing arguments. Machines capable of responding and interacting with humans in helpful ways have become ubiquitous. We now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. But what makes an argument more persuasive? What will convince you? In this paper, we present a new data set, IBM-EviConv, of pairs of evidence labeled for convincingness, designed to be more challenging than existing alternatives. We also propose a Siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own. Finally, we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting.}
}

@incollection{Goddu2009MakingLinkedConvergentDistinction,
  title = {Against {{Making}} the {{Linked-Convergent Distinction}}},
  booktitle = {Pondering on {{Problems}} of {{Argumentation}}: {{Twenty Essays}} on {{Theoretical Issues}}},
  author = {Goddu, G. C.},
  editor = {family=Eemeren, given=Frans H., prefix=van, useprefix=true and Garssen, Bart},
  date = {2009},
  pages = {181--189},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-1-4020-9165-0_13},
  url = {https://doi.org/10.1007/978-1-4020-9165-0_13},
  urldate = {2024-06-21},
  abstract = {The intuition guiding the avowed distinction between linked and convergent argument structures is easy enough to grasp – in various arguments some of the premises appear to link together to form a single reason for the conclusion, while other premises appear to constitute separate reasons which independently converge on the conclusion. Though the intuition is easy enough to grasp, as James Freeman has recently pointed out: “the problem of clearly distinguishing linked from convergent argument structure has proven vexing” (Freeman, 2001, p. 397). Indeed, the question remains whether the intuition truly captures a real distinction.},
  isbn = {978-1-4020-9165-0},
  langid = {english}
}

@inproceedings{Goebel2018ExplainableAINew,
  title = {Explainable {{AI}}: {{The New}} 42?},
  shorttitle = {Explainable {{AI}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {295--303},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-99740-7_21},
  abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce’s abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to “explain” their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
  isbn = {978-3-319-99740-7},
  langid = {english}
}

@article{Goel2017EditorialReasoningBrain,
  title = {Editorial: {{The Reasoning Brain}}: {{The Interplay}} between {{Cognitive Neuroscience}} and {{Theories}} of {{Reasoning}}},
  shorttitle = {Editorial},
  author = {Goel, Vinod and Navarrete, Gorka and Noveck, Ira A. and Prado, Jérôme},
  date = {2017},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {10},
  publisher = {Frontiers},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2016.00673},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2016.00673/full},
  urldate = {2021-01-16},
  abstract = {Editorial: The Reasoning Brain: The Interplay between Cognitive Neuroscience and Theories of Reasoning},
  langid = {english}
}

@inproceedings{Goel2017WhatHotCaseBased,
  title = {What's {{Hot}} in {{Case-Based Reasoning}}},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Goel, Ashok and Diaz-Agudo, Belen},
  date = {2017-02},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/15041},
  langid = {english}
}

@inproceedings{Goffredo2025DISPUTool30Fallacy,
  title = {{{DISPUTool}} 3.0: {{Fallacy Detection}} and {{Repairing}} in {{Argumentative Political Debates}}},
  shorttitle = {{{DISPUTool}} 3.0},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Goffredo, Pierpaolo and Dore, Deborah and Cabrio, Elena and Villata, Serena},
  editor = {Mishra, Pushkar and Muresan, Smaranda and Yu, Tao},
  date = {2025-07},
  pages = {472--480},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-demo.45/},
  urldate = {2025-08-01},
  abstract = {This paper introduces and evaluates a novel web-based application designed to identify and repair fallacious arguments in political debates. DISPUTool 3.0 offers a comprehensive tool for argumentation analysis of political debate, integrating state-of-the-art natural language processing techniques to mine and classify argument components and relations. DISPUTool 3.0 builds on the ElecDeb60to20 dataset, covering US presidential debates from 1960 to 2020. In this paper, we introduce a novel task which is integrated as a new module in DISPUTool, i.e., the automatic detection and classification of fallacious arguments, and the automatic repairing of such misleading arguments. The goal is to show to the user a tool which not only identifies fallacies in political debates, but it also shows how the argument looks like once the veil of fallacy falls down. An extensive evaluation of the module is addressed employing both automated metrics and human assessments. With the inclusion of this module, DISPUTool 3.0 advances even more user critical thinking in front of the augmenting spread of such nefarious kind of content in political debates and beyond. The tool is publicly available here: https://3ia-demos.inria.fr/disputool/},
  isbn = {979-8-89176-253-4}
}

@inproceedings{Gonzalez-Monge2025ExplainingTranslationalEmbedding,
  title = {Explaining {{Translational Embedding Models}} in~{{Recommender Systems Using Knowledge Graphs}} and~{{Language Models}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {González-Monge, Mario and Díaz-Agudo, Belén and Recio-Garcia, Juan A.},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  pages = {81--95},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_6},
  abstract = {We propose a modular and scalable architecture for building explainable recommendation systems that leverages knowledge graphs, translational embeddings, and LLMs. First, enriched knowledge graphs are constructed, followed by embedding-based relationship capture. Rather than a post-hoc addition, explanations are integrated into the core recommendation process. Visual explanations highlight relevant paths in the knowledge graph that connect users to recommended items, while natural language explanations, generated through an LLM, provide intuitive justifications that users can easily understand. A MovieLens 1M case study, enhanced with DBpedia, demonstrates strong experimental results and qualitative evidence of coherent, justifiable recommendations, particularly in sparse data scenarios. This highlights the potential of our approach for improving explainable recommender systems.},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Gordon2007CarneadesModelArgument,
  title = {The {{Carneades}} Model of Argument and Burden of Proof},
  author = {Gordon, Thomas F. and Prakken, Henry and Walton, Douglas},
  date = {2007-07-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  series = {Argumentation in {{Artificial Intelligence}}},
  volume = {171},
  number = {10},
  pages = {875--896},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2007.04.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370207000677},
  urldate = {2022-04-21},
  abstract = {We present a formal, mathematical model of argument structure and evaluation, taking seriously the procedural and dialogical aspects of argumentation. The model applies proof standards to determine the acceptability of statements on an issue-by-issue basis. The model uses different types of premises (ordinary premises, assumptions and exceptions) and information about the dialectical status of statements (stated, questioned, accepted or rejected) to allow the burden of proof to be allocated to the proponent or the respondent, as appropriate, for each premise separately. Our approach allows the burden of proof for a premise to be assigned to a different party than the one who has the burden of proving the conclusion of the argument, and also to change the burden of proof or applicable proof standard as the dialogue progresses from stage to stage. Useful for modeling legal dialogues, the burden of production and burden of persuasion can be handled separately, with a different responsible party and applicable proof standard for each. Carneades enables critical questions of argumentation schemes to be modeled as additional premises, using premise types to capture the varying effect on the burden of proof of different kinds of questions.},
  langid = {english}
}

@inproceedings{Goudas2014ArgumentExtractionNews,
  title = {Argument {{Extraction}} from {{News}}, {{Blogs}}, and {{Social Media}}},
  booktitle = {Artificial {{Intelligence}}: {{Methods}} and {{Applications}}},
  author = {Goudas, Theodosis and Louizos, Christos and Petasis, Georgios and Karkaletsis, Vangelis},
  editor = {Likas, Aristidis and Blekas, Konstantinos and Kalles, Dimitris},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {287--299},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-07064-3_23},
  abstract = {Argument extraction is the task of identifying arguments, along with their components in text. Arguments can be usually decomposed into a claim and one or more premises justifying it. Among the novel aspects of this work is the thematic domain itself which relates to Social Media, in contrast to traditional research in the area, which concentrates mainly on law documents and scientific publications. The huge increase of social media communities, along with their user tendency to debate, makes the identification of arguments in these texts a necessity. Argument extraction from Social Media is more challenging because texts may not always contain arguments, as is the case of legal documents or scientific publications usually studied. In addition, being less formal in nature, texts in Social Media may not even have proper syntax or spelling. This paper presents a two-step approach for argument extraction from social media texts. During the first step, the proposed approach tries to classify the sentences into “sentences that contain arguments” and “sentences that don’t contain arguments”. In the second step, it tries to identify the exact fragments that contain the premises from the sentences that contain arguments, by utilizing conditional random fields. The results exceed significantly the base line approach, and according to literature, are quite promising.},
  isbn = {978-3-319-07064-3},
  langid = {english}
}

@inproceedings{Grill2020BootstrapYourOwn,
  title = {Bootstrap Your Own Latent a New Approach to Self-Supervised Learning},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  date = {2020-12-06},
  series = {{{NIPS}} '20},
  pages = {21271--21284},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6\% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  isbn = {978-1-7138-2954-6}
}

@article{Grosse2015IntegratingArgumentationSentiment,
  title = {Integrating Argumentation and Sentiment Analysis for Mining Opinions from {{Twitter}}},
  author = {Grosse, Kathrin and González, María P. and Chesñevar, Carlos I. and Maguitman, Ana G.},
  date = {2015-07-17},
  journaltitle = {AI Communications},
  shortjournal = {AIC},
  volume = {28},
  number = {3},
  pages = {387--401},
  issn = {18758452, 09217126},
  doi = {10.3233/AIC-140627},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIC-140627},
  urldate = {2023-10-20}
}

@inproceedings{Grover2016Node2vecScalableFeature,
  title = {Node2vec: {{Scalable Feature Learning}} for {{Networks}}},
  shorttitle = {Node2vec},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Grover, Aditya and Leskovec, Jure},
  date = {2016-08-13},
  series = {{{KDD}} '16},
  pages = {855--864},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2939672.2939754},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939754},
  urldate = {2025-07-15},
  abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
  isbn = {978-1-4503-4232-2}
}

@inproceedings{Gruber2025DebArgVisInteractiveVisualisation,
  title = {{{DebArgVis}}: {{An Interactive Visualisation Tool}} for {{Exploring Argumentative Dynamics}} in {{Debate}}},
  shorttitle = {{{DebArgVis}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Gruber, Martin and Kikteva, Zlata and Rutter, Ignaz and Hautli-Janisz, Annette},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {140--146},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.13/},
  urldate = {2025-07-28},
  abstract = {Television debates play a key role in shaping public opinion, however, the rapid exchange of viewpoints in these settings often makes it difficult to perceive the underlying nature of the discussion. While there exist several debate visualisation techniques, to the best of our knowledge, none of them emphasise the argumentative dynamics in particular. With DebArgVis, we present a new interactive debate visualisation tool that leverages data annotated with argumentation structures to demonstrate how speaker interactions unfold over time, enabling users to deepen their comprehension of the debate.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Grumbach2024CaseBasedSupportResponding,
  title = {Towards a~{{Case-Based Support}} for~{{Responding Emergency Calls}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Grumbach, Lisa and Winzig, Alexander and Bergmann, Ralph},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {273--288},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_18},
  abstract = {In emergency situations, the quick and precise initiation of rescue measures is crucial. Dispatchers are responsible for answering emergency calls and deciding about measures and resources. However, currently there is no support on the basis of an intelligent system that is able to exploit experiences from previous situations. Therefore, we propose a concept for a case-based support for emergency call handling in this work. First, we investigate where case-based reasoning can be applied in the decision process and sketch our vision of a hybrid intelligent approach that combines expert and experiential knowledge. For the case-based approach, we focus on deriving adequate measures and resource types. Furthermore, we propose a mechanism that supports the dispatcher in the choice of the questions such that precise decisions can be derived. The approach is prototypically implemented and will be evaluated with experts in future work.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@book{Guarino2009HandbookOntologies,
  title = {Handbook on {{Ontologies}}},
  author = {Guarino, Nicola and Oberle, Daniel and Staab, Steffen},
  editor = {Staab, Steffen and Studer, Rudi},
  editortype = {redactor},
  date = {2009-01-01},
  series = {What {{Is}} an {{Ontology}}},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-92673-3},
  isbn = {978-3-540-70999-2},
  pagetotal = {1}
}

@inproceedings{Guo2009DomainAdaptationLatent,
  title = {Domain {{Adaptation}} with {{Latent Semantic Association}} for {{Named Entity Recognition}}},
  booktitle = {Proceedings of {{Human Language Technologies}}: {{The}} 2009 {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Guo, Honglei and Zhu, Huijia and Guo, Zhili and Zhang, Xiaoxun and Wu, Xian and Su, Zhong},
  date = {2009-06},
  pages = {281--289},
  publisher = {Association for Computational Linguistics},
  location = {Boulder, Colorado},
  url = {https://www.aclweb.org/anthology/N09-1032},
  urldate = {2020-04-27},
  eventtitle = {{{NAACL-HLT}} 2009}
}

@online{Guo2022GradientDescentBasedKNNAlgorithm,
  title = {A {{Gradient-Descent-Based}} k-{{NN Algorithm}}},
  author = {Guo, Wenqi},
  date = {2022-04-24},
  doi = {10.31219/osf.io/kqdxu},
  abstract = {In this paper, we reviewed a few works that solved the speed issue of the k-NN classification algorithm. This makes it possible to solve real problems using the classificational k-NN algorithm. We then use gradient descent to achieve the equivalent goal of feature scaling, increasing the accuracy of the k-NN classification algorithm, including on nominal data. I was just a part-time college student when I wrote this paper, things might be wrong. Don't assume its credibility is high.},
  pubstate = {prepublished}
}

@article{Guo2022SurveyAutomatedFactChecking,
  title = {A {{Survey}} on {{Automated Fact-Checking}}},
  author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  date = {2022-02-09},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {178--206},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00454},
  url = {https://doi.org/10.1162/tacl_a_00454},
  urldate = {2025-07-15},
  abstract = {Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.}
}

@inproceedings{Guo2024DSAgentAutomatedData,
  title = {{{DS-Agent}}: {{Automated Data Science}} by {{Empowering Large Language Models}} with {{Case-Based Reasoning}}},
  shorttitle = {{{DS-Agent}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Guo, Siyuan and Deng, Cheng and Wen, Ying and Chen, Hechang and Chang, Yi and Wang, Jun},
  date = {2024-07-08},
  pages = {16813--16848},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v235/guo24b.html},
  urldate = {2024-09-14},
  abstract = {In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\% success rate in the development stage, while attaining 36\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{Gupta2020INFOTABSInferenceTables,
  title = {{{INFOTABS}}: {{Inference}} on {{Tables}} as {{Semi-structured Data}}},
  shorttitle = {{{INFOTABS}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gupta, Vivek and Mehta, Maitrey and Nokhiz, Pegah and Srikumar, Vivek},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  date = {2020-07},
  pages = {2309--2324},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.210},
  url = {https://aclanthology.org/2020.acl-main.210/},
  urldate = {2025-07-15},
  abstract = {In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.},
  eventtitle = {{{ACL}} 2020}
}

@article{Gurevych2017ArgumentationSocialMedia,
  title = {Argumentation in {{Social Media}}},
  author = {Gurevych, Iryna and Lippi, Marco and Torroni, Paolo},
  date = {2017-06-27},
  journaltitle = {ACM Transactions on Internet Technology},
  shortjournal = {ACM Trans. Internet Technol.},
  volume = {17},
  number = {3},
  pages = {23:1--23:2},
  issn = {1533-5399},
  doi = {10.1145/3056539},
  url = {https://dl.acm.org/doi/10.1145/3056539},
  urldate = {2023-10-20}
}

@inproceedings{Gutfreund2016AutomaticArgumentsConstruction,
  title = {Automatic {{Arguments Construction}}: {{From Search Engine}} to {{Research Engine}}},
  booktitle = {2016 {{AAAI Fall Symposium Series}}},
  author = {Gutfreund, Dan and Katz, Yoav and Slonim, Noam},
  date = {2016},
  url = {https://aaai.org/papers/14100-14100-automatic-arguments-construction-from-search-engine-to-research-engine/}
}

@inproceedings{Guthrie2006CloserLookSkipgram,
  title = {A {{Closer Look}} at {{Skip-gram Modelling}}},
  booktitle = {Proceedings of the {{Fifth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'06)},
  author = {Guthrie, David and Allison, Ben and Liu, Wei and Guthrie, Louise and Wilks, Yorick},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
  date = {2006-05},
  publisher = {European Language Resources Association (ELRA)},
  location = {Genoa, Italy},
  url = {https://aclanthology.org/L06-1210/},
  urldate = {2025-09-10},
  abstract = {Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.},
  eventtitle = {{{LREC}} 2006}
}

@inproceedings{Guu2015TraversingKnowledgeGraphs,
  title = {Traversing {{Knowledge Graphs}} in {{Vector Space}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Guu, Kelvin and Miller, John and Liang, Percy},
  date = {2015-09},
  pages = {318--327},
  publisher = {Association for Computational Linguistics},
  location = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1038},
  url = {https://www.aclweb.org/anthology/D15-1038},
  urldate = {2020-06-16},
  eventtitle = {{{EMNLP}} 2015}
}

@inproceedings{Habernal2016WhatMakesConvincing,
  title = {What Makes a Convincing Argument? {{Empirical}} Analysis and Detecting Attributes of Convincingness in {{Web}} Argumentation},
  shorttitle = {What Makes a Convincing Argument?},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Habernal, Ivan and Gurevych, Iryna},
  date = {2016-11},
  pages = {1214--1223},
  publisher = {Association for Computational Linguistics},
  location = {Austin, Texas},
  doi = {10.18653/v1/D16-1129},
  url = {https://www.aclweb.org/anthology/D16-1129},
  urldate = {2019-09-04},
  eventtitle = {{{EMNLP}} 2016}
}

@inproceedings{Habernal2016WhichArgumentMore,
  title = {Which Argument Is More Convincing? {{Analyzing}} and Predicting Convincingness of {{Web}} Arguments Using Bidirectional {{LSTM}}},
  shorttitle = {Which Argument Is More Convincing?},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Habernal, Ivan and Gurevych, Iryna},
  date = {2016-08},
  pages = {1589--1599},
  publisher = {Association for Computational Linguistics},
  location = {Berlin, Germany},
  doi = {10.18653/v1/P16-1150},
  url = {https://www.aclweb.org/anthology/P16-1150},
  urldate = {2019-09-04},
  eventtitle = {{{ACL}} 2016}
}

@article{Hallgren2012ComputingInterRaterReliability,
  title = {Computing {{Inter-Rater Reliability}} for {{Observational Data}}: {{An Overview}} and {{Tutorial}}},
  shorttitle = {Computing {{Inter-Rater Reliability}} for {{Observational Data}}},
  author = {Hallgren, Kevin A.},
  date = {2012},
  journaltitle = {Tutorials in quantitative methods for psychology},
  shortjournal = {Tutor Quant Methods Psychol},
  volume = {8},
  number = {1},
  eprint = {22833776},
  eprinttype = {pubmed},
  pages = {23--34},
  issn = {1913-4126},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/},
  urldate = {2021-02-23},
  abstract = {Many research designs require the assessment of inter-rater reliability (IRR) to demonstrate consistency among observational ratings provided by multiple coders. However, many studies use incorrect statistical procedures, fail to fully report the information necessary to interpret their results, or do not address how IRR affects the power of their subsequent analyses for hypothesis testing. This paper provides an overview of methodological issues related to the assessment of IRR with a focus on study design, selection of appropriate statistics, and the computation, interpretation, and reporting of some commonly-used IRR statistics. Computational examples include SPSS and R syntax for computing Cohen’s kappa and intra-class correlations to assess IRR.},
  pmcid = {PMC3402032}
}

@article{Hamming2013ErrorDetectingError,
  title = {Error {{Detecting}} and {{Error Correcting Codes}}},
  author = {Hamming, R W},
  date = {2013-07-29},
  journaltitle = {Bell System Technical Journal},
  shortjournal = {Bell System Technical Journal},
  volume = {29},
  number = {2},
  pages = {147--160},
  doi = {10.1002/j.1538-7305.1950.tb00463.x},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6772729},
  urldate = {2018-09-01}
}

@inproceedings{Hamp1997GermaNetLexicalSemanticNet,
  title = {{{GermaNet}} - a {{Lexical-Semantic Net}} for {{German}}},
  booktitle = {Automatic {{Information Extraction}} and {{Building}} of {{Lexical Semantic Resources}} for {{NLP Applications}}},
  author = {Hamp, Birgit and Feldweg, Helmut},
  date = {1997},
  url = {https://www.aclweb.org/anthology/W97-0802},
  urldate = {2021-02-11}
}

@inproceedings{Hanney1996LearningAdaptationRules,
  title = {Learning Adaptation Rules from a Case-Base},
  booktitle = {Advances in {{Case-Based Reasoning}}},
  author = {Hanney, Kathleen and Keane, Mark T.},
  editor = {Smith, Ian and Faltings, Boi},
  date = {1996},
  pages = {179--192},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/BFb0020610},
  abstract = {A major challenge for case-based reasoning (CBR) is to overcome the knowledge-engineering problems incurred by developing adaptation knowledge. This paper describes an approach to automating the acquisition of adaptation knowledge overcoming many of the associated knowledge-engineering costs. This approach makes use of inductive techniques, which learn adaptation knowledge from case comparison. We also show how this adaptation knowledge can be usefully applied. The method has been tested in a property-evaluation CBR system and the technique is illustrated by examples taken from this domain. In addition, we examine how any available domain knowledge might be exploited in such an adaptation-rule learning-system.},
  isbn = {978-3-540-49568-0},
  langid = {english}
}

@inproceedings{Hanney1997AdaptationKnowledgeBottleneck,
  title = {The Adaptation Knowledge Bottleneck: {{How}} to Ease It by Learning from Cases},
  shorttitle = {The Adaptation Knowledge Bottleneck},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Hanney, Kathleen and Keane, Mark T.},
  editor = {Leake, David B. and Plaza, Enric},
  date = {1997},
  pages = {359--370},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-63233-6_506},
  abstract = {Assuming that adaptation knowledge will continue to be an important part of CBR systems, a major challenge for the area is to overcome the knowledge-engineering problems that arise in its acquisition. This paper describes an approach to automating the acquisition of adaptation knowledge overcoming many of the associated knowledge-engineering costs. This approach makes use of inductive techniques, which learn adaptation knowledge from case comparison. We also show how this adaptation knowledge can be usefully applied and report on how available domain knowledge might be exploited in such an adaptation-rule learning-system.},
  isbn = {978-3-540-69238-6},
  langid = {english}
}

@inproceedings{Hardalov2022SurveyStanceDetection,
  title = {A {{Survey}} on {{Stance Detection}} for {{Mis-}} and {{Disinformation Identification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Hardalov, Momchil and Arora, Arnav and Nakov, Preslav and Augenstein, Isabelle},
  editor = {Carpuat, Marine and family=Marneffe, given=Marie-Catherine, prefix=de, useprefix=true and Meza Ruiz, Ivan Vladimir},
  date = {2022-07},
  pages = {1259--1277},
  publisher = {Association for Computational Linguistics},
  location = {Seattle, United States},
  doi = {10.18653/v1/2022.findings-naacl.94},
  url = {https://aclanthology.org/2022.findings-naacl.94/},
  urldate = {2025-07-15},
  abstract = {Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.},
  eventtitle = {Findings 2022}
}

@book{Hardy1988Inequalities,
  title = {Inequalities},
  author = {Hardy, G H and Littlewood, J E and Pólya, G},
  date = {1988-01-01},
  series = {Cambridge {{Mathematical Library}}},
  publisher = {Cambridge University Press},
  isbn = {978-0-521-35880-4},
  pagetotal = {324}
}

@article{Harrell2005UsingArgumentDiagramming,
  title = {Using {{Argument Diagramming Software}} in the {{Classroom}}},
  author = {Harrell, Maralee},
  date = {2005-05-01},
  journaltitle = {Teaching Philosophy},
  volume = {28},
  number = {2},
  pages = {163--177},
  doi = {10.5840/teachphil200528222},
  url = {https://www.pdcnet.org/pdc/bvdb.nsf/purchase?openform&fp=teachphil&id=teachphil_2005_0028_0002_0163_0178},
  urldate = {2022-04-21},
  abstract = {Many undergraduates, philosophy majors included, read philosophical texts similar to the way they read stories. One method for teaching students how to discern the argumentative structure of a philosophy text is through argument diagrams (text boxes used to represent claims with arrows and lines used to represent connections between these claims). This paper provides criteria for an ideal argument diagramming software and then reviews the strengths and weaknesses of such software currently available, e.g. Araucaria, Argutect, Athena Standard, Inspiration, and Reason!Able.},
  langid = {english}
}

@inproceedings{Hasan2014WhyAreYou,
  title = {Why Are {{You Taking}} This {{Stance}}? {{Identifying}} and {{Classifying Reasons}} in {{Ideological Debates}}},
  shorttitle = {Why Are {{You Taking}} This {{Stance}}?},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Hasan, Kazi Saidul and Ng, Vincent},
  editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  date = {2014-10},
  pages = {751--762},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1083},
  url = {https://aclanthology.org/D14-1083/},
  urldate = {2025-09-03},
  abstract = {Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts.},
  eventtitle = {{{EMNLP}} 2014}
}

@thesis{Hassan2023AnalyzingArgumentationTwitter,
  title = {Analyzing {{Argumentation}} in {{Twitter Conversations}} with {{Graphs}}},
  author = {Hassan, Ahmed and Kakande, Arthur and Masroor, Maliha},
  date = {2023-03-31},
  institution = {Trier University},
  location = {Trier, Germany},
  abstract = {Social media platforms hold an immense amount of data that can be extensively exploited for drawing insightful information. The paper dives into the Twitter platform as a data source for assessing the political side of social media spectrum, specifically, regarding the 2020 US Presidential Elections. In this paper, we explore the idea of treating Twitter conversations as argument graphs to aid the process of manual annotation. The generated graphs are also exploited in creating argument detection models such as the parent-child text relations and argument relation detection using state-of-the-art machine learning approaches. This paper explains argumentative structure, identifies the various tasks involved in argument mining, and explains the challenges of conducting argument mining on Twitter.},
  langid = {english}
}

@inproceedings{Hautli-Janisz2022QT30CorpusArgument,
  title = {{{QT30}}: {{A Corpus}} of {{Argument}} and {{Conflict}} in {{Broadcast Debate}}},
  shorttitle = {{{QT30}}},
  booktitle = {Proceedings of the {{Thirteenth Language Resources}} and {{Evaluation Conference}}},
  author = {Hautli-Janisz, Annette and Kikteva, Zlata and Siskou, Wassiliki and Gorska, Kamila and Becker, Ray and Reed, Chris},
  editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
  date = {2022-06},
  pages = {3291--3300},
  publisher = {European Language Resources Association},
  location = {Marseille, France},
  url = {https://aclanthology.org/2022.lrec-1.352/},
  urldate = {2025-07-15},
  abstract = {Broadcast political debate is a core pillar of democracy: it is the public's easiest access to opinions that shape policies and enables the general public to make informed choices. With QT30, we present the largest corpus of analysed dialogical argumentation ever created (19,842 utterances, 280,000 words) and also the largest corpus of analysed broadcast political debate to date, using 30 episodes of BBC's `Question Time' from 2020 and 2021. Question Time is the prime institution in UK broadcast political debate and features questions from the public on current political issues, which are responded to by a weekly panel of five figures of UK politics and society. QT30 is highly argumentative and combines language of well-versed political rhetoric with direct, often combative, justification-seeking of the general public. QT30 is annotated with Inference Anchoring Theory, a framework well-known in argument mining, which encodes the way arguments and conflicts are created and reacted to in dialogical settings. The resource is freely available at http://corpora.aifdb.org/qt30.},
  eventtitle = {{{LREC}} 2022}
}

@online{Havrilla2024TeachingLargeLanguage,
  title = {Teaching {{Large Language Models}} to {{Reason}} with {{Reinforcement Learning}}},
  author = {Havrilla, Alex and Du, Yuqing and Raparthy, Sharath Chandra and Nalmpantis, Christoforos and Dwivedi-Yu, Jane and Zhuravinskyi, Maksym and Hambro, Eric and Sukhbaatar, Sainbayar and Raileanu, Roberta},
  date = {2024-03-07},
  eprint = {2403.04642},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.04642},
  url = {http://arxiv.org/abs/2403.04642},
  urldate = {2024-03-28},
  abstract = {Reinforcement Learning from Human Feedback (\textbackslash textbf\{RLHF\}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbackslash textbf\{PPO\}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\textbackslash textbf\{SFT\}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of \$10\textasciicircum 6\$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.},
  pubstate = {prepublished}
}

@inproceedings{He2025TableLoRALowrankAdaptation,
  title = {{{TableLoRA}}: {{Low-rank Adaptation}} on {{Table Structure Understanding}} for {{Large Language Models}}},
  shorttitle = {{{TableLoRA}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {He, Xinyi and Liu, Yihao and Zhou, Mengyu and He, Yeye and Dong, Haoyu and Han, Shi and Yuan, Zejian and Zhang, Dongmei},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {22376--22391},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.1090/},
  urldate = {2025-08-01},
  abstract = {Tabular data are crucial in many fields and their understanding by large language models (LLMs) under high parameter efficiency paradigm is important. However, directly applying parameter-efficient fine-tuning (PEFT) techniques to tabular tasks presents significant challenges, particularly in terms of better table serialization and the representation of two-dimensional structured information within a one-dimensional sequence. To address this, we propose TableLoRA, a module designed to improve LLMs' understanding of table structure during PEFT. It incorporates special tokens for serializing tables with special token encoder and uses 2D LoRA to encode low-rank information on cell positions. Experiments on four tabular-related datasets demonstrate that TableLoRA consistently outperforms vanilla LoRA and surpasses various table encoding methods tested in control experiments. These findings reveal that TableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process tabular data effectively, especially in low-parameter settings, demonstrating its potential as a robust solution for handling table-related tasks.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Heindorf2020CauseNetCausalityGraph,
  title = {{{CauseNet}}: {{Towards}} a {{Causality Graph Extracted}} from the {{Web}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}} ({{CIKM}} 2020)},
  author = {Heindorf, Stefan and Scholten, Yan and Wachsmuth, Henning and Ngomo, Axel-Cyrille Ngonga and Potthast, Martin},
  date = {2020-10-19},
  pages = {8},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3340531.3412763},
  abstract = {Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, 1 a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83\% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.},
  isbn = {978-1-4503-6859-9},
  langid = {english}
}

@inproceedings{Heinisch2023ArchitecturalSweetSpots,
  title = {Architectural {{Sweet Spots}} for {{Modeling Human Label Variation}} by the {{Example}} of {{Argument Quality}}: {{It}}'s {{Best}} to {{Relate Perspectives}}!},
  shorttitle = {Architectural {{Sweet Spots}} for {{Modeling Human Label Variation}} by the {{Example}} of {{Argument Quality}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Heinisch, Philipp and Orlikowski, Matthias and Romberg, Julia and Cimiano, Philipp},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {11138--11154},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.687},
  url = {https://aclanthology.org/2023.emnlp-main.687},
  urldate = {2024-04-09},
  abstract = {Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a continuum of approaches ranging from models that fully aggregate perspectives into a majority label to “share nothing”-architectures in which each annotator is considered in isolation from all other annotators. In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that predict labels for single annotators but include layers that model the relations between different annotators are beneficial. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F1-scores up to 43\% over a majority-label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives.},
  eventtitle = {{{EMNLP}} 2023}
}

@inproceedings{Heinisch2024TellMeWho,
  title = {Tell Me Who You Are and {{I}} Tell You How You Argue: {{Predicting Stances}} and {{Arguments}} for {{Stakeholder Groups}}},
  shorttitle = {Tell Me Who You Are and {{I}} Tell You How You Argue},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2024},
  author = {Heinisch, Philipp and Dumani, Lorik and Cimiano, Philipp and Schenkel, Ralf},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  date = {2024-06},
  pages = {1968--1982},
  publisher = {Association for Computational Linguistics},
  location = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.findings-naacl.128},
  url = {https://aclanthology.org/2024.findings-naacl.128},
  urldate = {2024-12-11},
  abstract = {Argument mining has focused so far mainly on the identification, extraction, and formalization of arguments. An important yet unaddressedtask consists in the prediction of the argumentative behavior of stakeholders in a debate. Predicting the argumentative behavior in advance can support foreseeing issues in public policy making or help recognize potential disagreements early on and help to resolve them. In this paper, we consider the novel task of predicting the argumentative behavior of individual stakeholders. We present ARGENST, a framework that relies on a recommender-based architecture to predict the stance and the argumentative main point on a specific controversial topic for a given stakeholder, which is described in terms of a profile including properties related to demographic attributes, religious and political orientation, socio-economic background, etc. We evaluate our approach on the well-known debate.org dataset in terms of accuracy for predicting stance as well as in terms of similarity of the generated arguments to the ground truth arguments using BERTScore. As part of a case study, we show how juries of members representing different stakeholder groups and perspectives can be assembled to simulate the public opinion on a given topic.},
  eventtitle = {Findings 2024}
}

@inproceedings{Heinrich2025MultiClassMeansEndAssessing,
  title = {Multi-{{Class}} versus {{Means-End}}: {{Assessing Classification Approaches}} for {{Argument Patterns}}},
  shorttitle = {Multi-{{Class}} versus {{Means-End}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Heinrich, Maximilian and Khatib, Khalid Al and Stein, Benno},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {195--204},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.19/},
  urldate = {2025-07-28},
  abstract = {In the study of argumentation, the schemes introduced by Walton et al. (2008) represent a significant advancement in understanding and analyzing the structure and function of arguments. Walton's framework is particularly valuable for computational reasoning, as it facilitates the identification of argument patterns and the reconstruction of enthymemes. Despite its practical utility, automatically identifying these schemes remains a challenging problem. To aid human annotators, Visser et al. (2021) developed a decision tree for scheme classification. Building on this foundation, we propose a means-end approach to argument scheme classification that systematically leverages expert knowledge—encoded in a decision tree—to guide language models through a complex classification task. We assess the effectiveness of the means-end approach by conducting a comprehensive comparison with a standard multi-class approach across two datasets, applying both prompting and supervised learning methods to each approach. Our results indicate that the means-end approach, when combined with supervised learning, achieves scores only slightly lower than those of the multi-class classification approach. At the same time, the means-end approach enhances explainability by identifying the specific steps in the decision tree that pose the greatest challenges for each scheme—offering valuable insights for refining the overall means-end classification process.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@thesis{Hemel2006NixOSNixBased,
  title = {{{NixOS}}: The {{Nix}} Based Operating System},
  shorttitle = {{{NixOS}}},
  author = {Hemel, Armijn},
  date = {2006-08-24},
  doi = {10.5281/zenodo.12906987},
  url = {https://zenodo.org/records/12906987},
  urldate = {2024-07-28},
  abstract = {The subject of this thesis is how the Nix package management system can be applied to manage a whole Linux distribution. Many conventional packagemanagement systems have drawbacks that Nix fixes. But, Nix has never been used to deploy and manage a whole system. In this thesis a proof of concept Linux distribution called NixOS is described. NixOS uses the Nix package management system to manage all software that is installed on the system, including the Linux kernel, all software and system services. Using Nix to manage all software on a system, as is done on NixOS, has several advantages. Developers don’t need to be worried that unwanted dependencies are picked up during the build of a software package, since these are completely eliminated. System administrators get the possibility to deploy services using Nix and how they can immediately use all benefits from Nix, including atomic upgrades and rollbacks, without going through a painful cycle of rolling back a service, with all its, possibly also updated, dependencies. This thesis describes the implementation NixOS, including pitfalls that were encountered and choices that were made. Also shown are some concrete results of running NixOS and how NixOS can be bettered.}
}

@article{Hesse1959DefiningAnalogy,
  title = {On {{Deﬁning Analogy}}},
  author = {Hesse, M},
  date = {1959},
  journaltitle = {Proc. Aristotelian Soc.},
  volume = {60},
  pages = {79--100}
}

@article{Hevner2004DesignScienceInformation,
  title = {Design {{Science}} in {{Information Systems Research}}},
  author = {Hevner, Alan R. and March, Salvatore T. and Park, Jinsoo and Ram, Sudha},
  date = {2004-03-01},
  journaltitle = {MIS Quarterly},
  volume = {28},
  number = {1},
  pages = {75--105},
  issn = {0276-7783},
  doi = {10.2307/25148625},
  url = {https://misq.umn.edu/design-science-in-information-systems-research.html},
  abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
  langid = {english}
}

@article{Hevner2007ThreeCycleView,
  title = {A {{Three Cycle View}} of {{Design Science Research}}},
  author = {Hevner, Alan},
  date = {2007-01-01},
  journaltitle = {Scandinavian Journal of Information Systems},
  volume = {19},
  number = {2},
  issn = {1901-0990},
  url = {https://aisel.aisnet.org/sjis/vol19/iss2/4}
}

@book{Hevner2010DesignResearchInformation,
  title = {Design {{Research}} in {{Information Systems}}: {{Theory}} and {{Practice}}},
  shorttitle = {Design {{Research}} in {{Information Systems}}},
  author = {Hevner, Alan and Chatterjee, Samir},
  date = {2010},
  series = {Integrated {{Series}} in {{Information Systems}}},
  volume = {22},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4419-5653-8},
  url = {https://link.springer.com/10.1007/978-1-4419-5653-8},
  urldate = {2025-06-03},
  isbn = {978-1-4419-5652-1 978-1-4419-5653-8},
  langid = {english}
}

@inproceedings{Hewett2019UtilityDiscourseParsing,
  title = {The {{Utility}} of {{Discourse Parsing Features}} for {{Predicting Argumentation Structure}}},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Argument Mining}}},
  author = {Hewett, Freya and Prakash Rane, Roshan and Harlacher, Nina and Stede, Manfred},
  date = {2019-08},
  pages = {98--103},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/W19-4512},
  url = {https://aclanthology.org/W19-4512},
  urldate = {2023-10-26},
  abstract = {Research on argumentation mining from text has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the `argumentative microtexts' (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the microtexts, we find that PDTB features can indeed improve its performance.},
  eventtitle = {{{ArgMining}} 2019}
}

@article{Hidayaturrahman2021EnhancingArgumentationComponent,
  title = {Enhancing Argumentation Component Classification Using Contextual Language Model},
  author = {{Hidayaturrahman} and Dave, Emmanuel and Suhartono, Derwin and Arymurthy, Aniati Murni},
  date = {2021-07-22},
  journaltitle = {Journal of Big Data},
  shortjournal = {J Big Data},
  volume = {8},
  number = {1},
  pages = {103},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00490-2},
  url = {https://doi.org/10.1186/s40537-021-00490-2},
  urldate = {2023-04-07},
  abstract = {Arguments facilitate humans to deliver their ideas. The outcome of the discussion heavily relies on the validity of the argument. If an argument is well-composed, it is more effective to grasp the core idea behind the argument. To grade the argument, machines can be utilized by decomposing into semantic label components. In natural language processing, multiple language models are available to perform this task. It is divided into context-free and contextual models. The majority of previous studies used hand-crafted features to perform argument component classification, while state of the art language models utilize machine learning. The majority of these language models ignore the context in an argument. This research paper aims to analyze whether by including the context in the classification process may improve the accuracy of the language model which will enhance the argumentation mining process as well. The same document corpus is fed into several language models. Word2Vec and GLoVe represent the context free models, while BERT and ELMo as context sensitive language models. Accuracy and time from each model are then compared to determine the importance of context. The result shows that contextual language models are proven to be able to boost classification accuracy by approximately 20\%. However, time comes as a cost where contextual models require longer training and prediction time. The benefit from the increase in accuracy outweighs the burden of time. Thus, as a contextual task, argumentation mining is suggested to use contextual model where context must be included to achieve promising results.},
  langid = {english}
}

@book{Hinkelmann2025HybrideKIMit,
  title = {Hybride KI mit Machine Learning und Knowledge Graphs: Innovative Lösungen aus der Praxis},
  shorttitle = {Hybride KI mit Machine Learning und Knowledge Graphs},
  editor = {Hinkelmann, Knut and Hoppe, Thomas and Humm, Bernhard G.},
  date = {2025},
  publisher = {Springer Fachmedien},
  location = {Wiesbaden},
  doi = {10.1007/978-3-658-44781-6},
  url = {https://link.springer.com/10.1007/978-3-658-44781-6},
  urldate = {2025-03-05},
  isbn = {978-3-658-44780-9 978-3-658-44781-6},
  langid = {ngerman}
}

@article{Hirschberg2015AdvancesNaturalLanguage,
  title = {Advances in Natural Language Processing},
  author = {Hirschberg, Julia and Manning, Christopher D.},
  date = {2015-07-17},
  journaltitle = {Science},
  volume = {349},
  number = {6245},
  pages = {261--266},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaa8685},
  url = {https://www.science.org/doi/10.1126/science.aaa8685},
  urldate = {2025-09-10},
  abstract = {Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.}
}

@inproceedings{Hlaoui2002NewAlgorithmInexact,
  title = {A New Algorithm for Inexact Graph Matching},
  booktitle = {2002 {{International Conference}} on {{Pattern Recognition}}},
  author = {Hlaoui, Adel and Wang, Shengrui},
  date = {2002-08},
  volume = {4},
  pages = {180-183 vol.4},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2002.1047427},
  url = {https://ieeexplore.ieee.org/document/1047427},
  urldate = {2025-05-14},
  abstract = {The graph is an essential data structure for representing relational information. When graphs are used to represent objects, comparing objects amounts to graph matching. Inexact graph matching is the process of finding the best possible matching between two graphs when exact matching is impossible. We propose a new algorithm for the inexact matching problem. The new algorithm decomposes the matching process into K phases, where the value of K ranges from 1 to the minimum of the numbers of nodes in the two graphs to be matched. The efficiency of the new algorithm results from the use of small values of K, significantly reducing the search space while still producing very good matchings (most of them optimal) between graphs. The algorithm is compared with the error-correcting subgraph isomorphism algorithm based on A*.},
  eventtitle = {2002 {{International Conference}} on {{Pattern Recognition}}}
}

@article{Hlavacova2017GoldenRuleMorphology,
  title = {Golden {{Rule}} of {{Morphology}} and {{Variants}} of {{Word}} Forms},
  author = {Hlaváčová, Jaroslava},
  date = {2017-12-01},
  journaltitle = {Journal of Linguistics/Jazykovedný casopis},
  volume = {68},
  number = {2},
  pages = {136--144},
  publisher = {Sciendo},
  doi = {10.1515/jazcas-2017-0024},
  url = {https://content.sciendo.com/view/journals/jazcas/68/2/article-p136.xml},
  urldate = {2021-03-08},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d1385e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}In many languages, some words can be written in several ways. We call them variants. Values of all their morphological categories are identical, which leads to an identical morphological tag. Together with the identical lemma, we have two or more wordforms with the same morphological description. This ambiguity may cause problems in various NLP applications. There are two types of variants – those affecting the whole paradigm (global variants) and those affecting only wordforms sharing some combinations of morphological values (inflectional variants). In the paper, we propose means how to tag all wordforms, including their variants, unambiguously. We call this requirement “Golden rule of morphology”. The paper deals mainly with Czech, but the ideas can be applied to other languages as well.{$<$}/p{$><$}/section{$>$}},
  langid = {english}
}

@thesis{Hoelzmann2024UntersuchungKoreferenzaufloesungArgumentgraphen,
  type = {bathesis},
  title = {Untersuchung der Koreferenzauflösung in Argumentgraphen mittels Prompting},
  author = {Hölzmann, Lukas},
  date = {2024-12-18},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {ngerman},
  pagetotal = {54}
}

@inproceedings{Hoffmann2020UsingSiameseGraph,
  title = {Using {{Siamese Graph Neural Networks}} for {{Similarity-Based Retrieval}} in {{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Hoffmann, Maximilian and Malburg, Lukas and Klein, Patrick and Bergmann, Ralph},
  editor = {Watson, Ian and Weber, Rosina},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {229--244},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58342-2_15},
  abstract = {Similarity-based retrieval of semantic graphs is widely used in real-world scenarios, e. g., in the domain of business workflows. To tackle the problem of complex and time-consuming graph similarity computations during retrieval, the MAC/FAC approach is used in Process-Oriented Case-Based Reasoning (POCBR), where similar graphs are extracted from a preselected set of candidate graphs. These graphs result from a similarity computation with a computationally inexpensive similarity measure. The contribution of this paper is a novel similarity measure where vector space embeddings generated by two siamese Graph Neural Networks (GNNs) are used to approximate the similarities of a precise but therefore computationally complex graph similarity measure. Our approach includes a specific encoding scheme for semantic graphs that enables their usage in neural networks. The evaluation examines the quality and performance of these models in preselecting retrieval candidates and in approximating the ground-truth similarities of the graph similarity measure for two workflow domains. The results show great potential of the approach for being used in a MAC/FAC scenario, either as a preselection model or as an approximation of the graph similarity measure.},
  isbn = {978-3-030-58342-2},
  langid = {english}
}

@inproceedings{Hoffmann2022GPUBasedGraphMatching,
  title = {{{GPU-Based Graph Matching}} for~{{Accelerating Similarity Assessment}} in~{{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Hoffmann, Maximilian and Malburg, Lukas and Bach, Nico and Bergmann, Ralph},
  editor = {Keane, Mark T. and Wiratunga, Nirmalie},
  date = {2022},
  pages = {240--255},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-14923-8_16},
  abstract = {In Process-Oriented Case-Based Reasoning (POCBR), determining the similarity between cases represented as semantic graphs often requires some kind of inexact graph matching, which generally is an NP-hard problem. Heuristic search algorithms such as A* search have been successfully applied for this task, but the computational performance is still a limiting factor for large case bases. As related work shows a great potential for accelerating A* search by using GPUs, we propose a novel approach called AMonG for efficiently computing graph similarities with an A* graph matching process involving GPU computing. The three-phased matching process distributes the search process over multiple search instances running in parallel on the GPU. We develop and examine different strategies within these phases that allow to customize the matching process adjusted to the problem situation to be solved. The experimental evaluation compares the proposed GPU-based approach with a pure CPU-based one. The results clearly demonstrate that the GPU-based approach significantly outperforms the CPU-based approach in a retrieval scenario, leading to an average speedup factor of 16.},
  isbn = {978-3-031-14923-8},
  langid = {english}
}

@article{Hoffmann2022UsingGraphEmbedding,
  title = {Using {{Graph Embedding Techniques}} in {{Process-Oriented Case-Based Reasoning}}},
  author = {Hoffmann, Maximilian and Bergmann, Ralph},
  date = {2022-02},
  journaltitle = {Algorithms},
  volume = {15},
  number = {2},
  pages = {27},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  doi = {10.3390/a15020027},
  url = {https://www.mdpi.com/1999-4893/15/2/27},
  urldate = {2025-05-15},
  abstract = {Similarity-based retrieval of semantic graphs is a core task of Process-Oriented Case-Based Reasoning (POCBR) with applications in real-world scenarios, e.g., in smart manufacturing. The involved similarity computation is usually complex and time-consuming, as it requires some kind of inexact graph matching. To tackle these problems, we present an approach to modeling similarity measures based on embedding semantic graphs via Graph Neural Networks (GNNs). Therefore, we first examine how arbitrary semantic graphs, including node and edge types and their knowledge-rich semantic annotations, can be encoded in a numeric format that is usable by GNNs. Given this, the architecture of two generic graph embedding models from the literature is adapted to enable their usage as a similarity measure for similarity-based retrieval. Thereby, one of the two models is more optimized towards fast similarity prediction, while the other model is optimized towards knowledge-intensive, more expressive predictions. The evaluation examines the quality and performance of these models in preselecting retrieval candidates and in approximating the ground-truth similarities of a graph-matching-based similarity measure for two semantic graph domains. The results show the great potential of the approach for use in a retrieval scenario, either as a preselection model or as an approximation of a graph similarity measure.},
  issue = {2},
  langid = {english}
}

@inproceedings{Hoffmann2023RankingBasedCaseRetrieval,
  title = {Ranking-{{Based Case Retrieval}} with {{Graph Neural Networks}} in {{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Proceedings of the {{Thirty-Sixth International Florida Artificial Intelligence Research Society Conference}}},
  author = {Hoffmann, Maximilian and Bergmann, Ralph},
  date = {2023-05-08},
  volume = {36},
  publisher = {AAAI Press},
  location = {Clearwater Beach, FL, USA},
  doi = {10.32473/flairs.36.133039},
  url = {https://journals.flvc.org/FLAIRS/article/view/133039},
  urldate = {2025-07-07},
  eventtitle = {{{FLAIRS}}},
  langid = {english}
}

@thesis{Hoffmann2025HybridAIProcess,
  type = {phdthesis},
  title = {Hybrid {{AI}} for {{Process Management}} - {{Improving Similarity Assessment}} in {{Process-Oriented Case-Based Reasoning}} via {{Deep Learning}}},
  shorttitle = {Hybrid {{AI}} for {{Process Management}}},
  author = {Hoffmann, Maximilian},
  date = {2025},
  institution = {Trier University},
  location = {Trier, Germany},
  url = {https://ubt.opus.hbz-nrw.de/frontdoor/index/index/docId/2518},
  urldate = {2025-07-04},
  abstract = {Case-Based Reasoning (CBR) is a symbolic Artificial Intelligence (AI) approach that has been successfully applied across various domains, including medical diagnosis, product configuration, and customer support, to solve problems based on experiential knowledge and analogy. A key aspect of CBR is its problem-solving procedure, where new solutions are created by referencing similar experiences, which makes CBR explainable and effective even with small amounts of data. However, one of the most significant challenges in CBR lies in defining and computing meaningful similarities between new and past problems, which heavily relies on domain-specific knowledge. This knowledge, typically only available through human experts, must be manually acquired, leading to what is commonly known as the knowledge-acquisition bottleneck. One way to mitigate the knowledge-acquisition bottleneck is through a hybrid approach that combines the symbolic reasoning strengths of CBR with the learning capabilities of Deep Learning (DL), a sub-symbolic AI method. DL, which utilizes deep neural networks, has gained immense popularity due to its ability to automatically learn from raw data to solve complex AI problems such as object detection, question answering, and machine translation. While DL minimizes manual knowledge acquisition by automatically training models from data, it comes with its own limitations, such as requiring large datasets, and being difficult to explain, often functioning as a "black box". By bringing together the symbolic nature of CBR and the data-driven learning abilities of DL, a neuro-symbolic, hybrid AI approach can potentially overcome the limitations of both methods, resulting in systems that are both explainable and capable of learning from data. The focus of this thesis is on integrating DL into the core task of similarity assessment within CBR, specifically in the domain of process management. Processes are fundamental to numerous industries and sectors, with process management techniques, particularly Business Process Management (BPM), being widely applied to optimize organizational workflows. Process-Oriented Case-Based Reasoning (POCBR) extends traditional CBR to handle procedural data, enabling applications such as adaptive manufacturing, where past processes are analyzed to find alternative solutions when problems arise. However, applying CBR to process management introduces additional complexity, as procedural cases are typically represented as semantically annotated graphs, increasing the knowledge-acquisition effort for both case modeling and similarity assessment. The key contributions of this thesis are as follows: It presents a method for preparing procedural cases, represented as semantic graphs, to be used as input for neural networks. Handling such complex, structured data represents a significant challenge, particularly given the scarcity of available process data in most organizations. To overcome the issue of data scarcity, the thesis proposes data augmentation techniques to artificially expand the process datasets, enabling more effective training of DL models. Moreover, it explores several deep learning architectures and training setups for learning similarity measures between procedural cases in POCBR applications. This includes the use of experience-based Hyperparameter Optimization (HPO) methods to fine-tune the deep learning models. Additionally, the thesis addresses the computational challenges posed by graph-based similarity assessments in CBR. The traditional method of determining similarity through subgraph isomorphism checks, which compare nodes and edges across graphs, is computationally expensive. To alleviate this issue, the hybrid approach seeks to use DL models to approximate these similarity calculations more efficiently, thus reducing the computational complexity involved in graph matching. The experimental evaluations of the corresponding contributions provide consistent results that indicate the benefits of using DL-based similarity measures and case retrieval methods in POCBR applications. The comparison with existing methods, e.g., based on subgraph isomorphism, shows several advantages but also some disadvantages of the compared methods. In summary, the methods and contributions outlined in this work enable more efficient and robust applications of hybrid CBR and DL in process management applications.},
  langid = {english},
  pagetotal = {182}
}

@unpublished{Hogan2021KnowledgeGraphs,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and family=Amato, given=Claudia, prefix=d', useprefix=true and family=Melo, given=Gerard, prefix=de, useprefix=true and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  date = {2021-01-23},
  eprint = {2003.02320},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.02320},
  urldate = {2021-01-31},
  abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.}
}

@article{Horn2000TeachingPhilosophyArgumentation,
  title = {Teaching Philosophy with Argumentation Maps},
  author = {Horn, Robert E.},
  date = {2000},
  journaltitle = {Newsletter of the American Philosophical Association},
  abstract = {The greatest challenge of doing philosophy today may be one of not being able to see the forest for the trees. Whether teachers and students will readily admit to it, the content of philosophical studies is too often presented in twigs, many of which are rarely if ever connected to larger branches of thought. The problem for students is not usually one of understanding a particular argument,  but rather understanding where and how all the arguments fit together. Consider, for example, the difficulty involved in trying to determine the current status of a longstanding philosophical debate. What arguments have already been made? Which have been rebutted?  Who has argued what against whom? What counter-rebuttals have been offered?  The most interesting and important arguments are usually carried on in the journals of several different fields. Research and the debates over the nature of consciousness, for instance, appear in the journals of neurobiology, cognitive science, anthropology, psychology, and philosophy. How can we expect to keep track of what is being currently thought about or written on the subject? At the same time, philosophers and students of philosophy also have wide-ranging}
}

@article{Hosseinzadeh2025AttentionMechanismsTransformers,
  title = {Attention {{Mechanisms}} in {{Transformers}}: {{A General Survey}}},
  shorttitle = {Attention {{Mechanisms}} in {{Transformers}}},
  author = {Hosseinzadeh, Rasoul and Sadeghzadeh, Mahdi},
  date = {2025-07},
  journaltitle = {Journal of AI and Data Mining},
  shortjournal = {JAIDM},
  volume = {13},
  number = {3},
  doi = {10.22044/jadm.2025.15584.2679},
  url = {https://doi.org/10.22044/jadm.2025.15584.2679},
  urldate = {2025-09-09},
  langid = {english}
}

@article{Hossin2015ReviewEvaluationMetrics,
  title = {A {{Review}} on {{Evaluation Metrics}} for {{Data Classification Evaluations}}},
  author = {Hossin, M. and Sulaiman, M.N.},
  date = {2015-03-31},
  journaltitle = {International Journal of Data Mining \& Knowledge Management Process},
  shortjournal = {IJDKP},
  volume = {5},
  number = {2},
  pages = {01--11},
  issn = {2231007X, 22309608},
  doi = {10.5121/ijdkp.2015.5201},
  url = {http://www.aircconline.com/ijdkp/V5N2/5215ijdkp01.pdf},
  urldate = {2021-03-13}
}

@inproceedings{Hotz2025AdvancedSearchTechniques,
  title = {Advanced {{Search Techniques}} for~{{Determining Optimal Sequences}} of~{{Adaptation Rules}} in~{{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Hotz, Maxim and Malburg, Lukas and Bergmann, Ralph},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  pages = {236--251},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_16},
  abstract = {The application of adaptation knowledge still poses a major challenge in modern Case-Based Reasoning (CBR) systems. This is especially the case in the subdomain of Process-Oriented Case-Based Reasoning (POCBR), where cases represent procedural experimental knowledge. Current adaptation methods in this field make use of proprietary local search techniques to apply adaptation knowledge, which is inefficient and does not allow exploitation of advanced search and optimization techniques. Therefore, this work presents an approach to transform rule-based adaptation into a planning problem that is solvable with both Constrained Optimization Planning (COP) and Genetic Algorithms (GAs). The results of an experimental evaluation indicate that this transformation is feasible, although it does not achieve significant improvements in terms of adaptation quality without fine-tuning the default search parameters. However, integration into state-of-the-art planning frameworks builds the basis for using a variety of additional features and updates, enhancing the modeling and configuration process.},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Houbraken2014IndexBasedSubgraphMatching,
  title = {The {{Index-Based Subgraph Matching Algorithm}} with {{General Symmetries}} ({{ISMAGS}}): {{Exploiting Symmetry}} for {{Faster Subgraph Enumeration}}},
  shorttitle = {The {{Index-Based Subgraph Matching Algorithm}} with {{General Symmetries}} ({{ISMAGS}})},
  author = {Houbraken, Maarten and Demeyer, Sofie and Michoel, Tom and Audenaert, Pieter and Colle, Didier and Pickavet, Mario},
  date = {2014-05-30},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {5},
  pages = {e97896},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0097896},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0097896},
  urldate = {2025-06-10},
  abstract = {Subgraph matching algorithms are used to find and enumerate specific interconnection structures in networks. By enumerating these specific structures/subgraphs, the fundamental properties of the network can be derived. More specifically in biological networks, subgraph matching algorithms are used to discover network motifs, specific patterns occurring more often than expected by chance. Finding these network motifs yields information on the underlying biological relations modelled by the network. In this work, we present the Index-based Subgraph Matching Algorithm with General Symmetries (ISMAGS), an improved version of the Index-based Subgraph Matching Algorithm (ISMA). ISMA quickly finds all instances of a predefined motif in a network by intelligently exploring the search space and taking into account easily identifiable symmetric structures. However, more complex symmetries (possibly involving switching multiple nodes) are not taken into account, resulting in superfluous output. ISMAGS overcomes this problem by using a customised symmetry analysis phase to detect all symmetric structures in the network motif subgraphs. These structures are then converted to symmetry-breaking constraints used to prune the search space and speed up calculations. The performance of the algorithm was tested on several types of networks (biological, social and computer networks) for various subgraphs with a varying degree of symmetry. For subgraphs with complex (multi-node) symmetric structures, high speed-up factors are obtained as the search space is pruned by the symmetry-breaking constraints. For subgraphs with no or simple symmetric structures, ISMAGS still reduces computation times by optimising set operations. Moreover, the calculated list of subgraph instances is minimal as it contains no instances that differ by only a subgraph symmetry. An implementation of the algorithm is freely available at https://github.com/mhoubraken/ISMAGS.},
  langid = {english}
}

@article{Hsu2003InterraterAgreementMeasures,
  title = {Interrater {{Agreement Measures}}: {{Comments}} on {{Kappan}}, {{Cohen}}'s {{Kappa}}, {{Scott}}'s {{Pi}}, and {{Aickin}}'s {{Alpha}}},
  shorttitle = {Interrater {{Agreement Measures}}},
  author = {Hsu, Louis M. and Field, Ronald},
  date = {2003-08-01},
  journaltitle = {Understanding Statistics},
  volume = {2},
  number = {3},
  pages = {205--219},
  publisher = {Routledge},
  issn = {1534-844X},
  doi = {10.1207/S15328031US0203_03},
  url = {https://doi.org/10.1207/S15328031US0203_03},
  urldate = {2021-03-07},
  abstract = {The Cohen (1960) kappa interrater agreement coefficient has been criticized for penalizing raters (e.g., diagnosticians) for their a priori agreement about the base rates of categories (e.g., base rates of disorders). A modification of kappa, called kappan (alias S coefficient, C coefficient, G index, and RE coefficient) has been proposed as an alternative to Cohen's kappa: Kappan was intended to reward rather than penalize classification agreements attributable to interrater agreement about base rates. In this article, we show that kappan has some serious limitations: It can be large when raters who randomly assign objects (e.g., patients) to categories (diagnoses) radically disagree about base rates, and it can be much larger when these raters have very different beliefs about base rates than when they are in complete agreement about base rates. Contrary to the views of recent critics of Cohen's kappa, we argue that Cohen's kappa (which does not have these serious limitations) is generally preferable to kappan. Cohen's kappa is also compared to two other kappa-type statistics (Scott's, 1955, Π; Aickin's, 1990, α). Unlike Scott's Π, Cohen's kappa can yield useful information about interrater agreement in the presence of marginal heterogeneity; Cohen's kappa is easier to calculate and more conservative than Aickin's α; and in addition, much more information is available about factors affecting Cohen's kappa than about factors affecting Aickin's α.}
}

@inproceedings{Hu2025RemovalHallucinationHallucination,
  title = {Removal of {{Hallucination}} on {{Hallucination}}: {{Debate-Augmented RAG}}},
  shorttitle = {Removal of {{Hallucination}} on {{Hallucination}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hu, Wentao and Zhang, Wengyu and Jiang, Yiyang and Zhang, Chen Jason and Wei, Xiaoyong and Qing, Li},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {15839--15853},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.770/},
  urldate = {2025-07-29},
  abstract = {Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate-Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Huang2008SimilarityMeasuresText,
  title = {Similarity Measures for Text Document Clustering},
  booktitle = {Proceedings of the Sixth New Zealand Computer Science Research Student Conference ({{NZCSRSC2008}})},
  author = {Huang, Anna},
  date = {2008-04},
  pages = {49--56},
  location = {Christchurch, New Zealand},
  url = {http://www.academia.edu/download/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf},
  urldate = {2018-09-05},
  abstract = {Clustering is a useful technique that organizes a large quantity of unordered text documents into a small number of meaningful and coherent clusters, thereby providing a basis for intuitive and informative navigation and browsing mechanisms. Partitional clustering algorithms have been recognized to be more suitable as opposed to the hierarchical clustering schemes for processing large datasets. A wide variety of distance functions and similarity measures have been used for clustering, such as squared Euclidean distance, cosine similarity, and relative entropy. In this paper, we compare and analyze the effectiveness of these measures in partitional clustering for text document datasets. Our experiments utilize the standard Kmeans algorithm and we report results on seven text document datasets and five distance/similarity measures that have been most commonly used in text clustering.}
}

@inproceedings{Huang2021DocumentlevelEntitybasedExtraction,
  title = {Document-Level {{Entity-based Extraction}} as {{Template Generation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Huang, Kung-Hsiang and Tang, Sam and Peng, Nanyun},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  date = {2021-11},
  pages = {5257--5269},
  publisher = {Association for Computational Linguistics},
  location = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.426},
  url = {https://aclanthology.org/2021.emnlp-main.426},
  urldate = {2024-08-15},
  abstract = {Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TopK Copy, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26\%), binary RE (+4.8\%), and 4-ary RE (+2.7\%) in F1 score.},
  eventtitle = {{{EMNLP}} 2021}
}

@inproceedings{Huang2023ReasoningLargeLanguage,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {1049--1065},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.67},
  url = {https://aclanthology.org/2023.findings-acl.67},
  urldate = {2024-09-16},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  eventtitle = {Findings 2023}
}

@inproceedings{Huang2024LargeLanguageModels,
  title = {Large {{Language Models}} for {{Graphs}}: {{Progresses}} and {{Directions}}},
  shorttitle = {Large {{Language Models}} for {{Graphs}}},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2024},
  author = {Huang, Chao and Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla, Nitesh},
  date = {2024-05-13},
  series = {{{WWW}} '24},
  pages = {1284--1287},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3589335.3641251},
  url = {https://dl.acm.org/doi/10.1145/3589335.3641251},
  urldate = {2024-09-16},
  abstract = {Graph neural networks (GNNs) have emerged as fundamental methods for handling structured graph data in various domains, including citation networks, molecule prediction, and recommender systems. They enable the learning of informative node or graph representations, which are crucial for tasks such as link prediction and node classification in the context of graphs. To achieve high-quality graph representation learning, certain essential factors come into play: clean labels, accurate graph structures, and sufficient initial node features. However, real-world graph data often suffer from noise and sparse labels, while different datasets have unique feature constructions. These factors significantly impact the generalization capabilities of graph neural networks, particularly when faced with unseen tasks. Recently, due to the efficent text processing and task generalization capability of large language models (LLMs), there has been a promising approach to address the challenges mentioned above by combining large language models with graph data.  This tutorial offers an overview of incorporating large language models into the graph domain, accompanied by practical examples. The methods are categorized into three dimensions: utilizing LLMs as augmenters, predictors, and agents for graph learning tasks. We will delve into the current progress and future directions within this field. By introducing this emerging topic, our aim is to enhance the audience's understanding of LLM-based graph learning techniques, foster idea exchange, and encourage discussions that drive continuous advancements in this domain.},
  isbn = {979-8-4007-0172-6}
}

@inproceedings{Hullermeier2004ComparisonRankingProcedures,
  title = {Comparison of {{Ranking Procedures}} in {{Pairwise Preference Learning}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Information Processing}} and {{Management}} of {{Uncertainty}} in {{Knoweldge-Based Systems}}},
  author = {Hüllermeier, Eyke and Fürnkranz, Johannes},
  date = {2004},
  location = {Perugia, Italy},
  eventtitle = {{{IPMU}}}
}

@inproceedings{Hullermeier2011PreferenceBasedCBRFirst,
  title = {Preference-{{Based CBR}}: {{First Steps}} toward a {{Methodological Framework}}},
  shorttitle = {Preference-{{Based CBR}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Hüllermeier, Eyke and Schlegel, Patrice},
  editor = {Ram, Ashwin and Wiratunga, Nirmalie},
  date = {2011},
  pages = {77--91},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23291-6_8},
  abstract = {Building on recent research on preference handling in artificial intelligence and related fields, our general goal is to develop a coherent and universally applicable methodological framework for CBR on the basis of formal concepts and methods for knowledge representation and reasoning with preferences. A preference-based approach to CBR appears to be appealing for several reasons, notably because case-based experiences naturally lend themselves to representations in terms of preference relations, even when not dealing with preference information in a literal sense. Moreover, the flexibility and expressiveness of a preference-based formalism well accommodate the uncertain and approximate nature of case-based problem solving. In this paper, we make a first step toward a preference-based formalization of CBR. Apart from providing a general outline of the framework as a whole, we specifically address the step of case-based inference. The latter consists of inferring preferences for candidate solutions in the context of a new problem, given such preferences in similar situations. Our case-based approach to predicting preference models is concretely realized for a scenario in which solutions are represented in the form of subsets of a reference set. First experimental results are presented to demonstrate the effectiveness of this approach.},
  isbn = {978-3-642-23291-6},
  langid = {english}
}

@inproceedings{Hullermeier2013PreferencebasedCBRGeneral,
  title = {Preference-Based {{CBR}}: General Ideas and Basic Principles},
  shorttitle = {Preference-Based {{CBR}}},
  booktitle = {Proceedings of the {{Twenty-Third}} International Joint Conference on {{Artificial Intelligence}}},
  author = {Hüllermeier, Eyke and Cheng, Weiwei},
  date = {2013-08-03},
  series = {{{IJCAI}} '13},
  pages = {3012--3016},
  publisher = {AAAI Press},
  location = {Beijing, China},
  url = {https://www.ijcai.org/Abstract/13/447},
  urldate = {2025-07-07},
  abstract = {Building on recent research on preference handling in artificial intelligence and related fields, our goal is to develop a coherent and generic methodological framework for case-based reasoning (CBR) on the basis of formal concepts and methods for knowledge representation and reasoning with preferences. A preference-based approach to CBR appears to be appealing for several reasons, notably because case-based experiences naturally lend themselves to representations in terms of preference or order relations. Moreover, the flexibility and expressiveness of a preference-based formalism well accommodate the uncertain and approximate nature of case-based problem solving. In this paper, we outline the basic ideas of preference-based CBR and sketch a formal framework for realizing these ideas.},
  eventtitle = {{{IJCAI}}},
  isbn = {978-1-57735-633-2}
}

@inproceedings{Hulpus2019ExplainingNaturalLanguage,
  title = {Towards {{Explaining Natural Language Arguments}} with {{Background Knowledge}}},
  booktitle = {Workshop on {{Semantic Explainability}} ({{SemEx}} 2019) in the 18th {{Intl}}. {{Semantic Web Conf}}. ({{ISWC}} 2019)},
  author = {Hulpus, Ioana and Kobbe, Jonathan and Becker, Maria and Opitz, Juri and Hirst, Graeme and Meilicke, Christian and Nastase, Vivi and Stuckenschmidt, Heiner and Frank, Anette},
  date = {2019},
  volume = {2465},
  pages = {16},
  location = {Christchurch, New Zealand},
  url = {Towards Explaining Natural Language Arguments with Background Knowledge.},
  abstract = {In this paper, we propose the task of argument explicitation, a task that makes the structure of a natural language argument explicit, as well as the background knowledge the argument is built on, in the form of implicit premises or contextual knowledge. The purpose of argument explicitation is to support the understanding of an argument by providing users with an end-to-end analysis that offers a critical assessment of arguments including identification of argument weaknesses. Besides, the results of the argument explicitation process can be used by machines to retrieve similar arguments as well as counter-arguments. We propose a framework for argument explicitation that joins a variety of AI and NLPbased argumentation mining sub-tasks that by now have mostly been treated separately in the literature. We identify the challenges this task entails, while at the same time highlighting the opportunities brought by the recent development of structured, external knowledge sources.},
  langid = {english}
}

@article{Hunter2007Matplotlib2DGraphics,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle = {Matplotlib},
  author = {Hunter, John D.},
  date = {2007-05},
  journaltitle = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2007.55},
  url = {https://ieeexplore.ieee.org/document/4160265},
  urldate = {2025-07-15},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems}
}

@article{Islam2020DeepLearningMisinformation,
  title = {Deep Learning for Misinformation Detection on Online Social Networks: A Survey and New Perspectives},
  shorttitle = {Deep Learning for Misinformation Detection on Online Social Networks},
  author = {Islam, Md Rafiqul and Liu, Shaowu and Wang, Xianzhi and Xu, Guandong},
  date = {2020-09-29},
  journaltitle = {Social Network Analysis and Mining},
  shortjournal = {Soc. Netw. Anal. Min.},
  volume = {10},
  number = {1},
  pages = {82},
  issn = {1869-5469},
  doi = {10.1007/s13278-020-00696-x},
  url = {https://doi.org/10.1007/s13278-020-00696-x},
  urldate = {2025-07-15},
  abstract = {Recently, the use of social networks such as Facebook, Twitter, and Sina Weibo has become an inseparable part of our daily lives. It is considered as a convenient platform for users to share personal messages, pictures, and videos. However, while people enjoy social networks, many deceptive activities such as fake news or rumors can mislead users into believing misinformation. Besides, spreading the massive amount of misinformation in social networks has become a global risk. Therefore, misinformation detection (MID) in social networks has gained a great deal of attention and is considered an emerging area of research interest. We find that several studies related to MID have been studied to new research problems and techniques. While important, however, the automated detection of misinformation is difficult to accomplish as it requires the advanced model to understand how related or unrelated the reported information is when compared to real information. The existing studies have mainly focused on three broad categories of misinformation: false information, fake news, and rumor detection. Therefore, related to the previous issues, we present a comprehensive survey of automated misinformation detection on (i) false information, (ii) rumors, (iii) spam, (iv) fake news, and (v) disinformation. We provide a state-of-the-art review on MID where deep learning (DL) is used to automatically process data and create patterns to make decisions not only to extract global features but also to achieve better results. We further show that DL is an effective and scalable technique for the state-of-the-art MID. Finally, we suggest several open issues that currently limit real-world implementation and point to future directions along this dimension.},
  langid = {english}
}

@inproceedings{Ivanova2024LetsDiscussQuality,
  title = {Let's Discuss! {{Quality Dimensions}} and {{Annotated Datasets}} for {{Computational Argument Quality Assessment}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ivanova, Rositsa V and Huber, Thomas and Niklaus, Christina},
  editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  date = {2024-11},
  pages = {20749--20779},
  publisher = {Association for Computational Linguistics},
  location = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1155},
  url = {https://aclanthology.org/2024.emnlp-main.1155/},
  urldate = {2025-09-09},
  abstract = {Research in the computational assessment of Argumentation Quality has gained popularity over the last ten years. Various quality dimensions have been explored through the creation of domain-specific datasets and assessment methods. We survey the related literature (211 publications and 32 datasets), while addressing potential overlaps and blurry boundaries to related domains. This paper provides a representative overview of the state of the art in Computational Argument Quality Assessment with a focus on quality dimensions and annotated datasets. The aim of the survey is to identify research gaps and to aid future discussions and work in the domain.},
  eventtitle = {{{EMNLP}} 2024}
}

@article{Jaccard1912DistributionFloraAlpine,
  title = {The {{Distribution}} of the {{Flora}} in the {{Alpine Zone}}},
  author = {Jaccard, Paul},
  date = {1912},
  journaltitle = {New Phytologist},
  volume = {11},
  number = {2},
  pages = {37--50},
  issn = {1469-8137},
  doi = {10.1111/j.1469-8137.1912.tb05611.x},
  url = {https://nph.onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8137.1912.tb05611.x},
  urldate = {2021-03-11},
  langid = {english}
}

@inproceedings{Jain2011LearningRerankQuerydependent,
  title = {Learning to Re-Rank: Query-Dependent Image Re-Ranking Using Click Data},
  shorttitle = {Learning to Re-Rank},
  booktitle = {Proceedings of the 20th International Conference on {{World}} Wide Web},
  author = {Jain, Vidit and Varma, Manik},
  date = {2011-03-28},
  series = {{{WWW}} '11},
  pages = {277--286},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1963405.1963447},
  url = {https://dl.acm.org/doi/10.1145/1963405.1963447},
  urldate = {2025-07-15},
  abstract = {Our objective is to improve the performance of keyword based image search engines by re-ranking their original results. To this end, we address three limitations of existing search engines in this paper. First, there is no straight-forward, fully automated way of going from textual queries to visual features. Image search engines therefore primarily rely on static and textual features for ranking. Visual features are mainly used for secondary tasks such as finding similar images. Second, image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts. Such labels are well known to be noisy due to various factors including ambiguous queries, unknown user intent and subjectivity in human judgments. This leads to learning a sub-optimal ranker. Finally, a static ranker is typically built to handle disparate user queries. The ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results. We demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data.We hypothesize that images clicked in response to a query are mostly relevant to the query. We therefore re-rank the original search results so as to promote images that are likely to be clicked to the top of the ranked list. Our re-ranking algorithm employs Gaussian Process regression to predict the normalized click count for each image, and combines it with the original ranking score. Our approach is shown to significantly boost the performance of the Bing image search engine on a wide range of tail queries.},
  isbn = {978-1-4503-0632-4}
}

@article{Jaiswal2024FaNREMsFairNormalized,
  title = {{{FaN-REMs}}: {{Fair}} and {{Normalized Retrieval Evaluation Metrics}} for {{Learning Retrieval Systems}}},
  shorttitle = {{{FaN-REMs}}},
  author = {Jaiswal, Amar and Kumar, Mohit and Pathak, Ajeet Ram and Yigzaw, Kassaye Yitbarek},
  date = {2024},
  journaltitle = {IEEE Access},
  volume = {12},
  pages = {195370--195395},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3514916},
  url = {https://ieeexplore.ieee.org/document/10794782},
  urldate = {2025-01-10},
  abstract = {Retrieval evaluation metrics are vital for resilient artificial intelligence (AI) retrieval systems and its subfields, such as case-based reasoning (CBR). Despite extensive research in CBR over the decades, the field still lacks a specialized retrieval evaluation metric (REM). This study aims to critically investigate and devise retrieval evaluation metrics that are generic, fair, normalized, and non-deceptive, which make it suitable for domains including learning retrieval systems such as CBR. It focuses on enhancing CBR retrievals by addressing key flaws in widely used metrics, including the normalized discounted cumulative gain at k ( nDCG\textbackslash text@k ) metric in IR. The proposed method entails devising a fair and normalized (FaN) relevancy metric for a case retrieved against a test query, blending system-generated and oracle-assessed relevancies. This underpins three rank-based metrics: relevancy at k ( R\textbackslash text@k ), average relevancy at k ( AR\textbackslash text@k ), and mean average relevancy at k ( MAR\textbackslash text@k ). These metrics are designed for both single and multiple query evaluations and offer a comprehensive retrieval analysis. Despite inherent challenges in evaluating evaluation metrics, FaN-REMs demonstrated robust performance across plausible domain values for the FaN relevancy function. These metrics effectively assess retrievals across different implementations, similarity measures, and applications, with inherent normalization allowing for comparisons across heterogeneous systems. These metrics were instrumental in developing a CBR-based clinical decision support system (CDSS) for the SupportPrim study in Norway, demonstrating the practical application and relevance of this research in real-world AI systems. FaN-REMs show promise as benchmark metrics to compare various retrieval and CBR systems. Suitable for set-based and rank-based evaluations, rank-based FaN-REMs demonstrate superior discriminatory capability. The experimental results affirm the viability of FaN-REMs in real-world CBR system development and maintenance.},
  eventtitle = {{{IEEE Access}}}
}

@article{Jamshidi2018MicroservicesJourneyFar,
  title = {Microservices: {{The Journey So Far}} and {{Challenges Ahead}}},
  shorttitle = {Microservices},
  author = {Jamshidi, Pooyan and Pahl, Claus and Mendonça, Nabor C. and Lewis, James and Tilkov, Stefan},
  date = {2018-05},
  journaltitle = {IEEE Software},
  volume = {35},
  number = {3},
  pages = {24--35},
  issn = {1937-4194},
  doi = {10.1109/MS.2018.2141039},
  url = {https://ieeexplore.ieee.org/document/8354433},
  urldate = {2023-11-20},
  abstract = {Microservices are an architectural approach emerging out of service-oriented architecture, emphasizing self-management and lightweightness as the means to improve software agility, scalability, and autonomy. This article examines microservice evolution from the technological and architectural perspectives and discusses key challenges facing future microservice developments.},
  eventtitle = {{{IEEE Software}}}
}

@inproceedings{Janier2014OVAArgumentAnalysis,
  title = {{{OVA}}+: An {{Argument Analysis Interface}}},
  shorttitle = {{{OVA}}+},
  booktitle = {Computational {{Models}} of {{Argument}} - {{Proceedings}} of {{COMMA}} 2014, {{Atholl Palace Hotel}}, {{Scottish Highlands}}, {{UK}}, {{September}} 9-12, 2014},
  author = {Janier, Mathilde and Lawrence, John and Reed, Chris},
  editor = {Parsons, Simon and Oren, Nir and Reed, Chris and Cerutti, Federico},
  date = {2014},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {266},
  pages = {463--464},
  publisher = {IOS Press},
  doi = {10.3233/978-1-61499-436-7-463},
  abstract = {This paper introduces OVA+, an on-line interface for the analysis of arguments. It is the result of an attempt to provide a tool relying on the Argument Interchange Format theory and Inference Anchoring Theory schemes.}
}

@inproceedings{Jarmulak2000GeneticAlgorithmsOptimise,
  title = {Genetic {{Algorithms}} to {{Optimise CBR Retrieval}}},
  booktitle = {Advances in {{Case-Based Reasoning}}},
  author = {Jarmulak, Jacek and Craw, Susan and Rowe, Ray},
  editor = {Blanzieri, Enrico and Portinale, Luigi},
  date = {2000},
  pages = {136--147},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44527-7_13},
  abstract = {Knowledge in a case-based reasoning (CBR) system is often more extensive than simply the cases, therefore knowledge engineering may still be very demanding. This paper offers a first step towards an automated knowledge acquisition and refinement tool for non-case CBR knowledge. A data-driven approach is presented where a Genetic Algorithm learns effective feature selection for inducing case-base index, and feature weights for similarity measure for case retrieval. The optimisation can be viewed as knowledge acquisition or maintenance depending on whether knowledge is being created or refined. Optimising CBRretrieval is achieved using cases from the case-base and only minimal expert input, and so can be easily applied to an evolving case-base or a changing environment. Experiments with a real tablet formulation problem show the gains of simultaneously optimising the index and similarity measure. Provided that the available data represents the problem domain well, the optimisation has good generalisation properties and the domain knowledge extracted is comparable to expert knowledge.},
  isbn = {978-3-540-44527-2},
  langid = {english}
}

@article{Jarvelin2002CumulatedGainbasedEvaluation,
  title = {Cumulated Gain-Based Evaluation of {{IR}} Techniques},
  author = {Järvelin, Kalervo and Kekäläinen, Jaana},
  date = {2002-10-01},
  journaltitle = {ACM Transactions on Information Systems},
  shortjournal = {ACM Trans. Inf. Syst.},
  volume = {20},
  number = {4},
  pages = {422--446},
  issn = {1046-8188},
  doi = {10.1145/582415.582418},
  url = {https://doi.org/10.1145/582415.582418},
  urldate = {2020-08-18},
  abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.}
}

@software{Jascob2020LemmInflect,
  title = {{{LemmInflect}}},
  author = {Jascob, Brad},
  date = {2020},
  origdate = {2019-05-11T20:33:58Z},
  url = {https://github.com/bjascob/LemmInflect},
  urldate = {2021-03-09},
  abstract = {A python module for English lemmatization and inflection.},
  version = {0.2.1}
}

@article{Jelodar2019LatentDirichletAllocation,
  title = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling: Models, Applications, a Survey},
  shorttitle = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling},
  author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
  date = {2019-06-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {78},
  number = {11},
  pages = {15169--15211},
  issn = {1573-7721},
  doi = {10.1007/s11042-018-6894-4},
  url = {https://doi.org/10.1007/s11042-018-6894-4},
  urldate = {2020-10-21},
  abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
  langid = {english}
}

@inproceedings{Jiang2023LowResourceTextClassification,
  title = {“{{Low-Resource}}” {{Text Classification}}: {{A Parameter-Free Classification Method}} with {{Compressors}}},
  shorttitle = {“{{Low-Resource}}” {{Text Classification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
  author = {Jiang, Zhiying and Yang, Matthew and Tsirlin, Mikhail and Tang, Raphael and Dai, Yiqin and Lin, Jimmy},
  date = {2023-07},
  pages = {6810--6828},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.findings-acl.426},
  url = {https://aclanthology.org/2023.findings-acl.426},
  urldate = {2023-09-04},
  abstract = {Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that's easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.},
  eventtitle = {Findings 2023}
}

@inproceedings{Jimenez-Diaz2024VisualizationSimilarityModels,
  title = {Visualization of~{{Similarity Models}} for~{{CBR Comprehension}} and~{{Maintenance}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Jimenez-Diaz, Guillermo and Díaz-Agudo, Belén},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {67--80},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_5},
  abstract = {Modeling similarity measures in Case-Based Reasoning systems is a critical and multifaceted task. Typically, similarity measures are manually defined, often with the input of domain experts or utilizing machine learning methods. These measures are then subjected to evaluation processes that include metrics that assess the properties of the retrieval and reuse processes on the case base. In this paper, we present SimViz, an exploratory visualization tool aimed at understanding and identifying errors in both data and similarity measures. Our tool represents an instance of Explainable Case-Based Reasoning, enabling interactive visualization through heatmaps and histograms and assisting in case comparison. These visualizations provide insight into the similarity between local and global attributes across different case representations.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Jimenez-Diaz2025FrameworkSupportingIterative,
  title = {A {{Framework}} for~{{Supporting}} the~{{Iterative Design}} of~{{CBR Applications}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Jimenez-Diaz, Guillermo and Lenz, Mirko and Malburg, Lukas and Díaz-Agudo, Belén and Bergmann, Ralph},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15662},
  pages = {252--266},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_17},
  abstract = {Iterative design is a well-known methodology that involves prototyping, testing, analyzing, and refining products or processes. Rapid prototyping and evaluation are crucial, enabling designers to quickly identify and resolve issues and iteratively improve the design. However, effective iterative design relies on tools that accelerate both prototype creation and visual evaluation. This paper aims to address this challenge by presenting a framework specifically designed to enhance the iterative design process for CBR applications. In this context, we emphasize the manual and knowledge-intensive task of defining similarity measures. Furthermore, we introduce a proof-of-concept implementation of this framework based on the CBRkit toolkit and the SimViz visualization tool. We studied the capabilities to support the iterative design of CBR applications through a case study in the prototypical cars domain.},
  eventtitle = {{{ICCBR}} 2025},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Jin2024LargeLanguageModels,
  title = {Large {{Language Models}} on {{Graphs}}: {{A Comprehensive Survey}}},
  shorttitle = {Large {{Language Models}} on {{Graphs}}},
  author = {Jin, Bowen and Liu, Gang and Han, Chi and Jiang, Meng and Ji, Heng and Han, Jiawei},
  date = {2024-12},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {36},
  number = {12},
  pages = {8622--8642},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2024.3469578},
  url = {https://ieeexplore.ieee.org/document/10697304},
  urldate = {2025-03-24},
  abstract = {Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}}
}

@article{Jivani2011ComparativeStudyStemming,
  title = {A Comparative Study of Stemming Algorithms},
  author = {Jivani, Anjali Ganesh},
  date = {2011-12},
  journaltitle = {researchgate.net},
  abstract = {Stemming is a pre-processing step in Text Mining applications as well as a very common requirement of Natural Language processing functions. In fact it is very important in most of the Information Retrieval systems. The main purpose of stemming is to reduce different grammatical forms/word forms of a word like its noun, adjective, verb, adverb etc. to its root form. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have~…}
}

@inproceedings{Jo2019CascadeModelProposition,
  title = {A {{Cascade Model}} for {{Proposition Extraction}} in {{Argumentation}}},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Argument Mining}}},
  author = {Jo, Yohan and Visser, Jacky and Reed, Chris and Hovy, Eduard},
  date = {2019-08},
  pages = {11--24},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/W19-4502},
  url = {https://aclanthology.org/W19-4502},
  urldate = {2023-10-26},
  abstract = {We present a model to tackle a fundamental but understudied problem in computational argumentation: proposition extraction. Propositions are the basic units of an argument and the primary building blocks of most argument mining systems. However, they are usually substituted by argumentative discourse units obtained via surface-level text segmentation, which may yield text segments that lack semantic information necessary for subsequent argument mining processes. In contrast, our cascade model aims to extract complete propositions by handling anaphora resolution, text segmentation, reported speech, questions, imperatives, missing subjects, and revision. We formulate each task as a computational problem and test various models using a corpus of the 2016 U.S. presidential debates. We show promising performance for some tasks and discuss main challenges in proposition extraction.},
  eventtitle = {{{ArgMining}} 2019}
}

@article{Jo2021ClassifyingArgumentativeRelations,
  title = {Classifying {{Argumentative Relations Using Logical Mechanisms}} and {{Argumentation Schemes}}},
  author = {Jo, Yohan and Bang, Seojin and Reed, Chris and Hovy, Eduard},
  date = {2021-08-02},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {721--739},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00394},
  url = {https://doi.org/10.1162/tacl_a_00394},
  urldate = {2022-01-13},
  abstract = {While argument mining has achieved significant success in classifying argumentative relations between statements (support, attack, and neutral), we have a limited computational understanding of logical mechanisms that constitute those relations. Most recent studies rely on black-box models, which are not as linguistically insightful as desired. On the other hand, earlier studies use rather simple lexical features, missing logical relations between statements. To overcome these limitations, our work classifies argumentative relations based on four logical and theory-informed mechanisms between two statements, namely, (i) factual consistency, (ii) sentiment coherence, (iii) causal relation, and (iv) normative relation. We demonstrate that our operationalization of these logical mechanisms classifies argumentative relations without directly training on data labeled with the relations, significantly better than several unsupervised baselines. We further demonstrate that these mechanisms also improve supervised classifiers through representation learning.}
}

@inproceedings{Johnson1991TreemapsSpacefillingApproach,
  title = {Tree-Maps: A Space-Filling Approach to the Visualization of Hierarchical Information Structures},
  shorttitle = {Tree-Maps},
  booktitle = {Proceeding {{Visualization}} '91},
  author = {Johnson, B. and Shneiderman, B.},
  date = {1991-10},
  pages = {284--291},
  doi = {10.1109/VISUAL.1991.175815},
  url = {https://ieeexplore.ieee.org/document/175815},
  urldate = {2025-02-13},
  abstract = {A method for visualizing hierarchically structured information is described. The tree-map visualization technique makes 100\% use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. Tree-maps can depict both the structure and content of the hierarchy. However, the approach is best suited to hierarchies in which the content of the leaf nodes and the structure of the hierarchy are of primary importance, and the content information associated with internal nodes is largely derived from their children.{$<>$}},
  eventtitle = {Proceeding {{Visualization}} '91}
}

@inproceedings{Joslyn2009MeasuringStructuralPreservation,
  title = {Measuring the Structural Preservation of Semantic Hierarchy Alignments},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Ontology Matching}}. {{CEUR Workshop Proceedings}}},
  author = {Joslyn, Cliff A. and Paulson, Patrick and White, Amanda and Al Saffar, Sinan},
  date = {2009},
  volume = {551},
  pages = {61--72}
}

@inproceedings{Jouili2009AttributedGraphMatching,
  title = {Attributed {{Graph Matching Using Local Descriptions}}},
  booktitle = {Advanced {{Concepts}} for {{Intelligent Vision Systems}}},
  author = {Jouili, Salim and Mili, Ines and Tabbone, Salvatore},
  editor = {Blanc-Talon, Jacques and Philips, Wilfried and Popescu, Dan and Scheunders, Paul},
  date = {2009},
  pages = {89--99},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04697-1_9},
  abstract = {In the pattern recognition context, objects can be represented as graphs with attributed nodes and edges involving their relations. Consequently, matching attributed graphs plays an important role in objects recognition. In this paper, a node signatures extraction is combined with an optimal assignment method for matching attributed graphs. In particular, we show how local descriptions are used to define a node-to-node cost in an assignment problem using the Hungarian method. Moreover, we propose a distance formula to compute the distance between attributed graphs. The experiments demonstrate that the newly presented algorithm is well-suited to pattern recognition applications. Compared with well-known methods, our algorithm gives good results for retrieving images.},
  isbn = {978-3-642-04697-1},
  langid = {english}
}

@book{Jurafsky2025SpeechLanguageProcessing,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  urldate = {2025-09-09}
}

@article{Jursic2010LemmaGenMultilingualLemmatisation,
  title = {{{LemmaGen}}: {{Multilingual Lemmatisation}} with {{Induced Ripple-Down Rules}}},
  author = {Juršič, Matjaž and Mozetič, Igor and Erjavec, Tomaž and Lavrač, Nada},
  date = {2010-05-01},
  journaltitle = {Journal of Universal Computer Science},
  volume = {16},
  number = {9},
  pages = {1190--1214},
  doi = {10.3217/jucs-016-09-1190},
  url = {http://www.jucs.org/jucs_16_9/lemma_gen_multilingual_lemmatisation},
  abstract = {Lemmatisation is the process of finding the normalised forms of words appearing in text. It is a useful preprocessing step for a number of language engineering and text mining tasks, and especially important for languages with rich inflectional morphology. This paper presents a new lemmatisation system, LemmaGen, which was trained to generate accurate and efficient lemmatisers for twelve different languages. Its evaluation on the corresponding lexicons shows that LemmaGen outperforms the lemmatisers generated by two alternative approaches, RDR and CST, both in terms of accuracy and efficiency. To our knowledge, LemmaGen is the most efficient publicly available lemmatiser trained on large lexicons of multiple languages, whose learning engine can be retrained to effectively generate lemmatisers of other languages.}
}

@article{Justice2006BinaryLinearProgramming,
  title = {A Binary Linear Programming Formulation of the Graph Edit Distance},
  author = {Justice, D. and Hero, A.},
  date = {2006-08},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {28},
  number = {8},
  pages = {1200--1214},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2006.152},
  url = {https://ieeexplore.ieee.org/document/1642656},
  urldate = {2025-06-24},
  abstract = {A binary linear programming formulation of the graph edit distance for unweighted, undirected graphs with vertex attributes is derived and applied to a graph recognition problem. A general formulation for editing graphs is used to derive a graph edit distance that is proven to be a metric, provided the cost function for individual edit operations is a metric. Then, a binary linear program is developed for computing this graph edit distance, and polynomial time methods for determining upper and lower bounds on the solution of the binary program are derived by applying solution methods for standard linear programming and the assignment problem. A recognition problem of comparing a sample input graph to a database of known prototype graphs in the context of a chemical information system is presented as an application of the new method. The costs associated with various edit operations are chosen by using a minimum normalized variance criterion applied to pairwise distances between nearest neighbors in the database of prototypes. The new metric is shown to perform quite well in comparison to existing metrics when applied to a database of chemical graphs.}
}

@article{Juttner2018VF2ppImprovedSubgraph,
  title = {{{VF2}}++—{{An}} Improved Subgraph Isomorphism Algorithm},
  author = {Jüttner, Alpár and Madarasi, Péter},
  date = {2018-06-19},
  journaltitle = {Discrete Applied Mathematics},
  shortjournal = {Discrete Applied Mathematics},
  series = {Computational {{Advances}} in {{Combinatorial Optimization}}},
  volume = {242},
  pages = {69--81},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2018.02.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0166218X18300829},
  urldate = {2025-05-25},
  abstract = {This paper presents a largely improved version of the VF2 algorithm for the Subgraph Isomorphism Problem. The improvements are twofold. Firstly, it is based on a new approach for determining the matching order of the nodes, and secondly, more efficient —~nevertheless easier to compute —~cutting rules are proposed. They together reduce the search space significantly. In addition to the usual Subgraph Isomorphism Problem, the paper also presents specialized algorithms for the Induced Subgraph Isomorphism and for the Graph Isomorphism Problems. Finally, an extensive experimental evaluation is provided using a wide range of inputs, including both real-life biological and chemical datasets and standard randomly generated graph series. The results show major and consistent running time improvements over the other known methods. The C++ implementations of the algorithms are available open-source as part of the LEMON graph and network optimization library.}
}

@article{Kakas2025PollutionAI,
  title = {The {{Pollution}} of {{AI}}},
  author = {Kakas, Antonis},
  date = {2025-04-29},
  journaltitle = {Commun. ACM},
  volume = {68},
  number = {05},
  pages = {27--30},
  issn = {0001-0782},
  doi = {10.1145/3707445},
  url = {https://dl.acm.org/doi/10.1145/3707445},
  urldate = {2025-05-08},
  abstract = {Proactively mitigating the potential polluting effects of artificial intelligence.}
}

@article{Kalfoglou2003OntologyMappingState,
  title = {Ontology Mapping: The State of the Art},
  shorttitle = {Ontology Mapping},
  author = {Kalfoglou, Yannis and Schorlemmer, Marco},
  date = {2003-01},
  journaltitle = {The Knowledge Engineering Review},
  volume = {18},
  number = {1},
  pages = {1--31},
  issn = {1469-8005, 0269-8889},
  doi = {10.1017/S0269888903000651},
  url = {https://www.cambridge.org/core/journals/knowledge-engineering-review/article/ontology-mapping-the-state-of-the-art/9A273424C3873243A0DC50FD43C6AD4E},
  urldate = {2019-01-05},
  abstract = {Ontology mapping is seen as a solution provider in today's landscape of ontology research. As the number of ontologies that are made publicly available and accessible on the Web increases steadily, so does the need for applications to use them. A single ontology is no longer enough to support the tasks envisaged by a distributed environment like the Semantic Web. Multiple ontologies need to be accessed from several applications. Mapping could provide a common layer from which several ontologies could be accessed and hence could exchange information in semantically sound manners. Developing such mappings has been the focus of a variety of works originating from diverse communities over a number of years. In this article we comprehensively review and present these works. We also provide insights on the pragmatics of ontology mapping and elaborate on a theoretical approach for defining ontology mapping.},
  langid = {english}
}

@inproceedings{Kapllani2020EmpiricalAnalysisMaintainability,
  title = {An {{Empirical Analysis}} of the {{Maintainability Evolution}} of {{Open Source Systems}}},
  booktitle = {Open {{Source Systems}}},
  author = {Kapllani, Gerta and Khomyakov, Ilya and Mirgalimova, Ruzilya and Sillitti, Alberto},
  editor = {Ivanov, Vladimir and Kruglov, Artem and Masyagin, Sergey and Sillitti, Alberto and Succi, Giancarlo},
  date = {2020},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  pages = {78--86},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-47240-5_8},
  abstract = {Maintainability is a key factor for the evolution of an open source system due to the highly distributed development teams that contribute to many projects. In the literature there are a number of different approaches that has been developed to evaluate the maintainability of a product but almost each method has been developed in an independent way without leveraging on the existing work and with almost no independent evaluation of the performance of the models. In most of the cases, the models are only validated through a limited set of projects only by the people that propose the specific approach. This paper is a first step towards a different direction focusing on the independent application of the existing models to popular open source projects.},
  isbn = {978-3-030-47240-5},
  langid = {english}
}

@article{Karami2020TwitterResearchSystematic,
  title = {Twitter and {{Research}}: {{A Systematic Literature Review Through Text Mining}}},
  shorttitle = {Twitter and {{Research}}},
  author = {Karami, Amir and Lundy, Morgan and Webb, Frank and Dwivedi, Yogesh K.},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {67698--67717},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2983656},
  url = {https://ieeexplore.ieee.org/document/9047963},
  urldate = {2023-10-20},
  abstract = {Researchers have collected Twitter data to study a wide range of topics. This growing body of literature, however, has not yet been reviewed systematically to synthesize Twitter-related papers. The existing literature review papers have been limited by constraints of traditional methods to manually select and analyze samples of topically related papers. The goals of this retrospective study are to identify dominant topics of Twitter-based research, summarize the temporal trend of topics, and interpret the evolution of topics withing the last ten years. This study systematically mines a large number of Twitter-based studies to characterize the relevant literature by an efficient and effective approach. This study collected relevant papers from three databases and applied text mining and trend analysis to detect semantic patterns and explore the yearly development of research themes across a decade. We found 38 topics in more than 18,000 manuscripts published between 2006 and 2019. By quantifying temporal trends, this study found that while 23.7\% of topics did not show a significant trend (P = 0.05), 21\% of topics had increasing trends and 55.3\% of topics had decreasing trends that these hot and cold topics represent three categories: application, methodology, and technology. The contributions of this paper can be utilized in the growing field of Twitter-based research and are beneficial to researchers, educators, and publishers.},
  eventtitle = {{{IEEE Access}}}
}

@inproceedings{Kargupta2025TrueFalseRetrievalAugmented,
  title = {Beyond {{True}} or {{False}}: {{Retrieval-Augmented Hierarchical Analysis}} of {{Nuanced Claims}}},
  shorttitle = {Beyond {{True}} or {{False}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kargupta, Priyanka and Tian, Runchu and Han, Jiawei},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {29664--29679},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.1434/},
  urldate = {2025-08-01},
  abstract = {Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely “true” or “false”—as is frequently the case with scientific and political claims. However, a claim (e.g., “vaccine A is better than vaccine B”) can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., “how many biomedical papers believe vaccine A is more transportable than B?”). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@online{Karpas2022MRKLSystemsModular,
  title = {{{MRKL Systems}}: {{A}} Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning},
  shorttitle = {{{MRKL Systems}}},
  author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and Shalev-Shwartz, Shai and Shashua, Amnon and Tenenholtz, Moshe},
  date = {2022-05-01},
  eprint = {2205.00445},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.00445},
  url = {http://arxiv.org/abs/2205.00445},
  urldate = {2025-09-12},
  abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
  pubstate = {prepublished}
}

@article{Katz1953NewStatusIndex,
  title = {A New Status Index Derived from Sociometric Analysis},
  author = {Katz, Leo},
  date = {1953-03-01},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {18},
  number = {1},
  pages = {39--43},
  issn = {1860-0980},
  doi = {10.1007/BF02289026},
  url = {https://doi.org/10.1007/BF02289026},
  urldate = {2025-02-20},
  abstract = {For the purpose of evaluating status in a manner free from the deficiencies of popularity contest procedures, this paper presents a new method of computation which takes into accountwho chooses as well ashow many choose. It is necessary to introduce, in this connection, the concept of attenuation in influence transmitted through intermediaries.},
  langid = {english}
}

@inproceedings{Kawarada2024ArgumentMiningTexttoText,
  title = {Argument {{Mining}} as a {{Text-to-Text Generation Task}}},
  booktitle = {Proceedings of the 18th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kawarada, Masayuki and Hirao, Tsutomu and Uchida, Wataru and Nagata, Masaaki},
  editor = {Graham, Yvette and Purver, Matthew},
  date = {2024-03},
  pages = {2002--2014},
  publisher = {Association for Computational Linguistics},
  location = {St. Julian's, Malta},
  url = {https://aclanthology.org/2024.eacl-long.121},
  urldate = {2024-06-24},
  abstract = {Argument Mining (AM) aims to uncover the argumentative structures within a text. Previous methods require several subtasks, such as span identification, component classification, and relation classification. Consequently, these methods need rule-based postprocessing to derive argumentative structures from the output of each subtask. This approach adds to the complexity of the model and expands the search space of the hyperparameters. To address this difficulty, we propose a simple yet strong method based on a text-to-text generation approach using a pretrained encoder-decoder language model. Our method simultaneously generates argumentatively annotated text for spans, components, and relations, eliminating the need for task-specific postprocessing and hyperparameter tuning. Furthermore, because it is a straightforward text-to-text generation method, we can easily adapt our approach to various types of argumentative structures.Experimental results demonstrate the effectiveness of our method, as it achieves state-of-the-art performance on three different types of benchmark datasets: the Argument-annotated Essays Corpus (AAEC), AbstRCT, and the Cornell eRulemaking Corpus (CDCP).},
  eventtitle = {{{EACL}} 2024}
}

@inproceedings{Keane1994AnalogicalAsidesCasebased,
  title = {Analogical Asides on Case-Based Reasoning},
  booktitle = {Topics in {{Case-Based Reasoning}}},
  author = {Keane, Mark T.},
  editor = {Wess, Stefan and Althoff, Klaus-Dieter and Richter, Michael M.},
  date = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {21--32},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-58330-0_74},
  abstract = {This paper explores some of the similarities and differences between cognitive models of analogy and case-based reasoning systems. I first point out a paradox in the treatment of adaptation in analogy and in case-based reasoning; a paradox which can be only resolved by expanding the role of adaptation in cognitive models of analogy. Some psychological research on the process of adaptation in human subjects is reported and then the implications of this research are propagated into analogy and then on into CBR. The argument is that some of the existing stages in CBR should be integrated into a more stream-lined architecture that would be more efficient than current schemes.},
  isbn = {978-3-540-48655-8},
  langid = {english}
}

@inproceedings{Keane2019HowCaseBasedReasoning,
  title = {How {{Case-Based Reasoning Explains Neural Networks}}: {{A Theoretical Analysis}} of {{XAI Using Post-Hoc Explanation-by-Example}} from a {{Survey}} of {{ANN-CBR Twin-Systems}}},
  shorttitle = {How {{Case-Based Reasoning Explains Neural Networks}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Keane, Mark T. and Kenny, Eoin M.},
  editor = {Bach, Kerstin and Marling, Cindy},
  date = {2019},
  pages = {155--171},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-29249-2_11},
  abstract = {This paper proposes a theoretical analysis of one approach to the eXplainable AI (XAI) problem, using post-hoc explanation-by-example, that relies on the twinning of artificial neural networks (ANNs) with case-based reasoning (CBR) systems; so-called ANN-CBR twins. It surveys these systems to advance a new theoretical interpretation of previous work and define a road map for CBR’s further role in XAI. A systematic survey of 1,102 papers was conducted to identify a fragmented literature on this topic and trace its influence to more recent work involving deep neural networks (DNNs). The twin-systems approach is advanced as one possible coherent, generic solution to the XAI problem. The paper concludes by road-mapping future directions for this XAI solution, considering (i) further tests of feature-weighting techniques, (ii) how explanatory cases might be deployed (e.g., in counterfactuals, a fortori cases), and (iii) the unwelcome, much-ignored issue of user evaluation.},
  isbn = {978-3-030-29249-2},
  langid = {english}
}

@article{Kelly2009MethodsEvaluatingInteractive,
  title = {Methods for {{Evaluating Interactive Information Retrieval Systems}} with {{Users}}},
  author = {Kelly, Diane},
  date = {2009-01-01},
  journaltitle = {Foundations and Trends in Information Retrieval},
  shortjournal = {Found. Trends Inf. Retr.},
  volume = {3},
  pages = {1--224},
  issn = {1554-0669},
  doi = {10.1561/1500000012},
  url = {https://doi.org/10.1561/1500000012},
  urldate = {2020-08-18},
  abstract = {This paper provides overview and instruction regarding the evaluation of interactive information retrieval systems with users. The primary goal of this article is to catalog and compile material related to this topic into a single source. This article (1) provides historical background on the development of user-centered approaches to the evaluation of interactive information retrieval systems; (2) describes the major components of interactive information retrieval system evaluation; (3) describes different experimental designs and sampling strategies; (4) presents core instruments and data collection techniques and measures; (5) explains basic data analysis techniques; and (4) reviews and discusses previous studies. This article also discusses validity and reliability issues with respect to both measures and methods, presents background information on research ethics and discusses some ethical issues which are specific to studies of interactive information retrieval (IIR). Finally, this article concludes with a discussion of outstanding challenges and future research directions.},
  issue = {1---2}
}

@article{Kemeny1959MathematicsNumbers,
  title = {Mathematics without {{Numbers}}},
  author = {Kemeny, John G.},
  date = {1959},
  journaltitle = {Daedalus},
  volume = {88},
  number = {4},
  eprint = {20026529},
  eprinttype = {jstor},
  pages = {577--591},
  publisher = {The MIT Press},
  issn = {0011-5266},
  url = {https://www.jstor.org/stable/20026529},
  urldate = {2025-03-15}
}

@article{Kendall-Morwick2014FacilitatingRepresentationRetrieval,
  title = {Facilitating Representation and Retrieval of Structured Cases: {{Principles}} and Toolkit},
  shorttitle = {Facilitating Representation and Retrieval of Structured Cases},
  author = {Kendall-Morwick, Joseph and Leake, David},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {106--114},
  issn = {0306-4379},
  doi = {10.1016/j.is.2012.11.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437912001494},
  urldate = {2025-05-13},
  abstract = {Case-Based reasoning (CBR) applications are increasingly used for problems involving structured data, especially in process-oriented domains. Available CBR frameworks rely primarily on flat case representations, leaving developers of process-oriented CBR systems without general-purpose tools aimed at their needs. This paper discusses needs to support structure-based retrieval for CBR and how they have been addressed in the development of the Structure Access Interface (SAI), a toolkit for representation and retrieval of structured cases which is now available for general use. Integrating SAI into a CBR project eliminates the need for developing a storage scheme for graph data structures and facilitates the development of retrieval algorithms by (1) providing useful code for common retrieval tasks and (2) delineating the tasks which may require domain-specific implementations. SAI maintains flexibility through its customizability yet carries solutions for many common tasks for CBR developers, as well as illuminating some general principles for addressing common representation and retrieval needs for process-oriented CBR tasks and the relationship between CBR needs for structured retrieval and needs and methods in graph oriented database research. The paper closes with an evaluation of SAI's scalability and demonstration of the value of enabling developers to select retrieval methods suited to their tasks. It closes with a discussion of SAI's relationship to the growing body of work on graph databases.}
}

@article{Kent1955MachineLiteratureSearching,
  title = {Machine Literature Searching {{VIII}}. {{Operational}} Criteria for Designing Information Retrieval Systems},
  author = {Kent, Allen and Berry, Madeline M and Luehrs, Fred U and Perry, J W},
  date = {1955-01-01},
  journaltitle = {American Documentation},
  shortjournal = {American Documentation},
  volume = {6},
  number = {2},
  pages = {93--101},
  doi = {10.1002/asi.5090060209},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.5090060209},
  urldate = {2018-09-01}
}

@inproceedings{Kenter2015ShortTextSimilarity,
  title = {Short {{Text Similarity}} with {{Word Embeddings}}},
  booktitle = {Proceedings of the 24th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Kenter, Tom and family=Rijke, given=Maarten, prefix=de, useprefix=true},
  date = {2015-10},
  series = {{{CIKM}} '15},
  pages = {1411--1420},
  publisher = {ACM},
  location = {Melbourne, Australia},
  doi = {10.1145/2806416.2806475},
  abstract = {Determining semantic similarity between texts is important in many tasks in information retrieval such as search, query suggestion, automatic summarization and image finding. Many approaches have been suggested, based on lexical matching, handcrafted patterns, syntactic parse trees, external sources of structured semantic knowledge and distributional semantics. However, lexical features, like string matching, do not capture semantic similarity beyond a trivial level. Furthermore, handcrafted patterns and external sources of structured semantic knowledge cannot be assumed to be available in all circumstances and for all domains. Lastly, approaches depending on parse trees are restricted to syntactically well-formed texts, typically of one sentence in length. We investigate whether determining short text similarity is possible using only semantic features---where by semantic we mean, pertaining to a representation of meaning---rather than relying on similarity in lexical or syntactic representations. We use word embeddings, vector representations of terms, computed from unlabelled data, that represent terms in a semantic space in which proximity of vectors can be interpreted as semantic similarity. We propose to go from word-level to text-level semantics by combining insights from methods based on external sources of semantic knowledge with word embeddings. A novel feature of our approach is that an arbitrary number of word embedding sets can be incorporated. We derive multiple types of meta-features from the comparison of the word vectors for short text pairs, and from the vector means of their respective word embeddings. The features representing labelled short text pairs are used to train a supervised learning algorithm. We use the trained model at testing time to predict the semantic similarity of new, unlabelled pairs of short texts  We show on a publicly available evaluation set commonly used for the task of semantic similarity that our method outperforms baseline methods that work under the same conditions.},
  eventtitle = {{{CIKM}} '15},
  isbn = {978-1-4503-3794-6}
}

@article{Khan2022TransformersVisionSurvey,
  title = {Transformers in {{Vision}}: {{A Survey}}},
  shorttitle = {Transformers in {{Vision}}},
  author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  date = {2022-09-13},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  pages = {200:1--200:41},
  issn = {0360-0300},
  doi = {10.1145/3505244},
  url = {https://dl.acm.org/doi/10.1145/3505244},
  urldate = {2025-07-15},
  abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  issue = {10s}
}

@inproceedings{Khartabil2016LargescaleArgumentVisualization,
  title = {Large-Scale {{Argument Visualization}} ({{LSAV}})},
  booktitle = {{{EuroVis}}},
  author = {Khartabil, D. and Wells, S. and Kennedy, J.},
  date = {2016},
  doi = {10.2312/eurp.20161143},
  abstract = {A tool for interacting with argument corpora is proposed that will help users to explore and understand the reasoning structure of large-scale arguments, to more rapidly comprehend complex new problem domains. Arguments are structures of premises and conclusions that underpin rational reasoning processes. Within complex knowledge domains, especially if they are contentious, argument structures can become large and complex. Visualization tools have been developed that support argument analysts and help them to work with arguments. Until recently, arguments were manually analyzed from natural language text, or constructed from scratch, but new communication modes mean that increasing amounts of debate and the arguments therein can be captured digitally. Furthermore, new tools and techniques for argument mining are beginning to automate the process of extracting argument structure from natural language; leading to much larger argument datasets that present problems for the current generation of argument visualization tools. Additionally, individual argument analysts have different foci which can lead to increased complexity within datasets, and additional facets that argument visualizations should account for but do not. We propose a tool for interacting with argument corpora that enable users to explore and understand the reasoning structure of large-scale arguments. The tool will support a range of interactivity techniques and will help users to explore and analyse large-scale arguments, to more rapidly comprehend complex new problem domains.}
}

@online{Khattab2020ColBERTEfficientEffective,
  title = {{{ColBERT}}: {{Efficient}} and {{Effective Passage Search}} via {{Contextualized Late Interaction}} over {{BERT}}},
  shorttitle = {{{ColBERT}}},
  author = {Khattab, Omar and Zaharia, Matei},
  date = {2020-06-04},
  eprint = {2004.12832},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2004.12832},
  url = {http://arxiv.org/abs/2004.12832},
  urldate = {2024-10-23},
  abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.},
  pubstate = {prepublished}
}

@inproceedings{Khattab2023DSPyCompilingDeclarative,
  title = {{{DSPy}}: {{Compiling Declarative Language Model Calls}} into {{State-of-the-Art Pipelines}}},
  shorttitle = {{{DSPy}}},
  author = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and A, Sri Vardhamanan and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  date = {2023-10-13},
  url = {https://openreview.net/forum?id=sY5N0zY5Od},
  urldate = {2025-02-26},
  abstract = {The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, or imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric, by creating and collecting demonstrations. We conduct two case studies, showing that succinct DSPy programs can express and optimize pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, DSPy can automatically produce pipelines that outperform out-of-the-box few-shot prompting as well as expert-created demonstrations for GPT-3.5 and Llama2-13b-chat. On top of that, DSPy programs compiled for relatively small LMs like 770M parameter T5 and Llama2-13b-chat are competitive with many approaches that rely on large and proprietary LMs like GPT-3.5 and on expert-written prompt chains. DSPy is available at https://github.com/stanfordnlp/dspy},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  langid = {english}
}

@online{Khosravi2024RelationalGraphConvolutional,
  title = {Relational {{Graph Convolutional Networks}} for {{Sentiment Analysis}}},
  author = {Khosravi, Asal and Rahmati, Zahed and Vefghi, Ali},
  date = {2024-04-16},
  eprint = {2404.13079},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.13079},
  url = {http://arxiv.org/abs/2404.13079},
  urldate = {2024-04-28},
  abstract = {With the growth of textual data across online platforms, sentiment analysis has become crucial for extracting insights from user-generated content. While traditional approaches and deep learning models have shown promise, they cannot often capture complex relationships between entities. In this paper, we propose leveraging Relational Graph Convolutional Networks (RGCNs) for sentiment analysis, which offer interpretability and flexibility by capturing dependencies between data points represented as nodes in a graph. We demonstrate the effectiveness of our approach by using pre-trained language models such as BERT and RoBERTa with RGCN architecture on product reviews from Amazon and Digikala datasets and evaluating the results. Our experiments highlight the effectiveness of RGCNs in capturing relational information for sentiment analysis tasks.},
  pubstate = {prepublished}
}

@unpublished{Kim2015CharacterAwareNeuralLanguage,
  title = {Character-{{Aware Neural Language Models}}},
  author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
  date = {2015-08-26},
  eprint = {1508.06615},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60\% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.}
}

@inproceedings{Kipf2017SemiSupervisedClassificationGraph,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-06},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  urldate = {2025-07-15},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{Kirschner2015LinkingThoughtsAnalysis,
  title = {Linking the {{Thoughts}}: {{Analysis}} of {{Argumentation Structures}} in {{Scientific Publications}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Argumentation Mining}} Held in Conjunction with the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}} – {{Human Language Technologies}}},
  author = {Kirschner, Christian and Eckle-Kohler, Judith and Gurevych, Iryna},
  date = {2015-06},
  pages = {1--11},
  doi = {10.3115/v1/W15-0501},
  url = {http://www.aclweb.org/anthology/W15-0501},
  urldate = {2018-09-02},
  eventtitle = {{{NAACL HLT}} 2015}
}

@inproceedings{Klein2019LearningWorkflowEmbeddings,
  title = {Learning {{Workflow Embeddings}} to {{Improve}} the {{Performance}} of {{Similarity-Based Retrieval}} for {{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Klein, Patrick and Malburg, Lukas and Bergmann, Ralph},
  editor = {Bach, Kerstin and Marling, Cindy},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {188--203},
  publisher = {Springer International Publishing},
  abstract = {In process-oriented case-based reasoning, similarity-based retrieval of workflow cases from large case bases is still a difficult issue due to the computationally expensive similarity assessment. The two-phase MAC/FAC (“Many are called, but few are chosen”) retrieval has been proven useful to reduce the retrieval time but comes at the cost of an additional modeling effort for implementing the MAC phase. In this paper, we present a new approach to implement the MAC phase for POCBR retrieval, which makes use of the StarSpace embedding algorithm to automatically learn a vector representation for workflows, which can be used to significantly speed-up the MAC retrieval phase. In an experimental evaluation in the domain of cooking workflows, we show that the presented approach outperforms two existing MAC/FAC approaches on the same data.},
  isbn = {978-3-030-29249-2},
  langid = {english}
}

@article{Kleinberg1999AuthoritativeSourcesHyperlinked,
  title = {Authoritative Sources in a Hyperlinked Environment},
  author = {Kleinberg, Jon M.},
  date = {1999-09-01},
  journaltitle = {J. ACM},
  volume = {46},
  number = {5},
  pages = {604--632},
  issn = {0004-5411},
  doi = {10.1145/324133.324140},
  url = {https://dl.acm.org/doi/10.1145/324133.324140},
  urldate = {2025-02-20},
  abstract = {The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.}
}

@article{Klie2023AnnotationErrorDetection,
  title = {Annotation {{Error Detection}}: {{Analyzing}} the {{Past}} and {{Present}} for a {{More Coherent Future}}},
  shorttitle = {Annotation {{Error Detection}}},
  author = {Klie, Jan-Christoph and Webber, Bonnie and Gurevych, Iryna},
  date = {2023-03},
  journaltitle = {Computational Linguistics},
  volume = {49},
  number = {1},
  pages = {157--198},
  doi = {10.1162/coli_a_00464},
  url = {https://aclanthology.org/2023.cl-1.4/},
  urldate = {2025-08-15},
  abstract = {Annotated data is an essential ingredient in natural language processing for training and evaluating machine learning models. It is therefore very desirable for the annotations to be of high quality. Recent work, however, has shown that several popular datasets contain a surprising number of annotation errors or inconsistencies. To alleviate this issue, many methods for annotation error detection have been devised over the years. While researchers show that their approaches work well on their newly introduced datasets, they rarely compare their methods to previous work or on the same datasets. This raises strong concerns on methods' general performance and makes it difficult to assess their strengths and weaknesses. We therefore reimplement 18 methods for detecting potential annotation errors and evaluate them on 9 English datasets for text classification as well as token and span labeling. In addition, we define a uniform evaluation setup including a new formalization of the annotation error detection task, evaluation protocol, and general best practices. To facilitate future research and reproducibility, we release our datasets and implementations in an easy-to-use and open source software package.1}
}

@misc{Kluge2013AnnotationGuidelines,
  title = {Annotation {{Guidelines}}},
  author = {Kluge, Roland},
  date = {2013-12-18}
}

@inproceedings{Knaebel2024ImpactArgumentArrangement,
  title = {The {{Impact}} of~{{Argument Arrangement}} on~{{Essay Scoring}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Knaebel, René and Schaefer, Robin and Stede, Manfred},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {147--162},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_9},
  abstract = {We study the question to what extent the task of predicting the quality of student essays can be supported with computing “flows” of semantic types of argumentative units. Specifically, we use tagsets for claim and premise types that were recently applied to the Argument Annotated Essays corpus (AAE; Stab/Gurevych 2017) by Schaefer et al (2023). We train argument component and semantic type classification models on AAE and then use them to label the essays in two corpora that have numeric essay ratings, viz. FEEDBACK/PERSUADE and ICLE. We train linear classification models on flow features and find that flows of our semantic types are a better predictor for essay quality (in a simplified, good/bad dichotomy) than flows of coarse argument components (major claim, claim, premise). Finally, we calculate feature impact and perform a qualitative inspection, which shows some tendencies for pattern occurrence in the two essay classes.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@article{Knowles2004NotionLemmaHeadwords,
  title = {The Notion of a “Lemma”: {{Headwords}}, Roots and Lexical Sets},
  shorttitle = {The Notion of a “Lemma”},
  author = {Knowles, Gerry and Don, Zuraidah Mohd},
  date = {2004-01-01},
  journaltitle = {International Journal of Corpus Linguistics},
  volume = {9},
  number = {1},
  pages = {69--81},
  publisher = {John Benjamins},
  issn = {1384-6655, 1569-9811},
  doi = {10.1075/ijcl.9.1.04kno},
  url = {https://www.jbe-platform.com/content/journals/10.1075/ijcl.9.1.04kno},
  urldate = {2021-03-08},
  abstract = {The notion of alemmais so familiar in corpus linguistics that it scarcely needs a formal definition. When a wordlist or a text is lemmatised, the process is apparently transparent, so that any observer can understand how the lemma relates to the original set or string of words. We shall argue in this paper that, on the contrary, the concept of lemma is not well defined, and is in need of a clear formal definition. The lemma is a fundamental concept in the processing of texts in at least some languages, a point we shall illustrate with respect to Arabic and Malay. It so happens that English lemmas are not typical of the general category, so that linguists who base their understanding of the lemma on English obtain a distorted view. It is essential to reverse the direction of argument, and to start with a general understanding of the lemma, and to consider English lemmas in the wider context.},
  langid = {english}
}

@inproceedings{Kobbe2019ExploitingBackgroundKnowledge,
  title = {Exploiting {{Background Knowledge}} for {{Argumentative Relation Classification}}},
  booktitle = {Proceesings of the 2nd {{Conference}} on {{Language}}, {{Data}} and {{Knowledge}} ({{LDK}} 2019)},
  author = {Kobbe, Jonathan and Opitz, Juri and Becker, Maria and Hulpus, Ioana and Stuckenschmidt, Heiner and Frank, Anette},
  date = {2019},
  pages = {14},
  location = {Dagstuhl, Germany},
  doi = {10.4230/OASICS.LDK.2019.8},
  url = {http://drops.dagstuhl.de/opus/volltexte/2019/10372/},
  urldate = {2020-05-11},
  abstract = {Argumentative relation classification is the task of determining the type of relation (e.g., support or attack) that holds between two argument units. Current state-of-the-art models primarily exploit surface-linguistic features including discourse markers, modals or adverbials to classify argumentative relations. However, a system that performs argument analysis using mainly rhetorical features can be easily fooled by the stylistic presentation of the argument as opposed to its content, in cases where a weak argument is concealed by strong rhetorical means. This paper explores the difficulties and the potential effectiveness of knowledge-enhanced argument analysis, with the aim of advancing the state-of-the-art in argument analysis towards a deeper, knowledge-based understanding and representation of arguments. We propose an argumentative relation classification system that employs linguistic as well as knowledge-based features, and investigate the effects of injecting background knowledge into a neural baseline model for argumentative relation classification. Starting from a Siamese neural network that classifies pairs of argument units into support vs. attack relations, we extend this system with a set of features that encode a variety of features extracted from two complementary background knowledge resources: ConceptNet and DBpedia. We evaluate our systems on three different datasets and show that the inclusion of background knowledge can improve the classification performance by considerable margins. Thus, our work offers a first step towards effective, knowledge-rich argument analysis.},
  langid = {english}
}

@inproceedings{Kobbe2020UnsupervisedStanceDetection,
  title = {Unsupervised Stance Detection for Arguments from Consequences},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Kobbe, Jonathan and Hulpu\textbackslash textcommabelows, Ioana and Stuckenschmidt, Heiner},
  date = {2020-11},
  pages = {50--60},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-main.4},
  url = {https://aclanthology.org/2020.emnlp-main.4},
  urldate = {2022-01-14},
  abstract = {Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic. To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences. We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence. Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT. Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.},
  eventtitle = {{{EMNLP}} 2020}
}

@inproceedings{Koehn2005EuroparlParallelCorpus,
  title = {Europarl: {{A}} Parallel Corpus for Statistical Machine Translation},
  author = {Koehn, Philipp},
  date = {2005-01-01},
  pages = {79--86},
  url = {http://courses.washington.edu/ling473/Project5.pdf},
  urldate = {2018-09-01},
  abstract = {For this project you will build a naïve Bayesian classifier which is able to classify text fragments according to their language. Determining the language of a document may seem trivial, since the most common few words in each language—or the character set it uses— could be thought of as an identifying signature. But we would like to use a more principled approach that quantitatively scores the probability of some fragment of text being written in one language, relative to others.},
  eventtitle = {Proceedings of the Tenth {{Machine Translation Summit}}}
}

@online{Koleva2024WikiTabNERAdvancingTable,
  title = {Wiki-{{TabNER}}: {{Advancing Table Interpretation Through Named Entity Recognition}}},
  shorttitle = {Wiki-{{TabNER}}},
  author = {Koleva, Aneta and Ringsquandl, Martin and Hatem, Ahmed and Runkler, Thomas and Tresp, Volker},
  date = {2024-03-07},
  eprint = {2403.04577},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.04577},
  url = {http://arxiv.org/abs/2403.04577},
  urldate = {2024-06-15},
  abstract = {Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random and similarity-based selection to choose the examples presented to the models. Our ablation study helps us gain insights into the impact of the few-shot examples. Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.},
  pubstate = {prepublished},
  version = {1}
}

@inproceedings{Koncel-Kedziorski2019TextGenerationKnowledge,
  title = {Text {{Generation}} from {{Knowledge Graphs}} with {{Graph Transformers}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Koncel-Kedziorski, Rik and Bekal, Dhanush and Luan, Yi and Lapata, Mirella and Hajishirzi, Hannaneh},
  date = {2019-06},
  pages = {2284--2293},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1238},
  url = {https://www.aclweb.org/anthology/N19-1238},
  urldate = {2020-10-16},
  abstract = {Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.},
  eventtitle = {{{NAACL-HLT}} 2019}
}

@article{Koopmans1957AssignmentProblemsLocation,
  title = {Assignment {{Problems}} and the {{Location}} of {{Economic Activities}}},
  author = {Koopmans, Tjalling C. and Beckmann, Martin},
  date = {1957},
  journaltitle = {Econometrica},
  volume = {25},
  number = {1},
  eprint = {1907742},
  eprinttype = {jstor},
  pages = {53--76},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/1907742},
  url = {https://www.jstor.org/stable/1907742},
  urldate = {2025-06-24},
  abstract = {Two problems in the allocation of indivisible resources are discussed. Both can be interpreted as problems of assigning plants to locations. The first problem, in which cost of transportation between plants is ignored, is found to be a linear programming problem, with which is associated a system of rents that sustains an optimal assignment. The recognition of cost of interplant transportation in the second problem introduces complications which call for more laborious and largely unexplored computations and which also appear to defeat the price system as a means of sustaining an optimal assignment.}
}

@article{Korman2018DefiningTextualEntailment,
  title = {Defining Textual Entailment},
  author = {Korman, Daniel Z. and Mack, Eric and Jett, Jacob and Renear, Allen H.},
  date = {2018},
  journaltitle = {Journal of the Association for Information Science and Technology},
  volume = {69},
  number = {6},
  pages = {763--772},
  issn = {2330-1643},
  doi = {10.1002/asi.24007},
  url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24007},
  urldate = {2020-08-13},
  abstract = {Textual entailment is a relationship that obtains between fragments of text when one fragment in some sense implies the other fragment. The automation of textual entailment recognition supports a wide variety of text-based tasks, including information retrieval, information extraction, question answering, text summarization, and machine translation. Much ingenuity has been devoted to developing algorithms for identifying textual entailments, but relatively little to saying what textual entailment actually is. This article is a review of the logical and philosophical issues involved in providing an adequate definition of textual entailment. We show that many natural definitions of textual entailment are refuted by counterexamples, including the most widely cited definition of Dagan et al. We then articulate and defend the following revised definition: T textually entails H = df typically, a human reading T would be justified in inferring the proposition expressed by H from the proposition expressed by T. We also show that textual entailment is context-sensitive, nontransitive, and nonmonotonic.},
  langid = {english}
}

@inproceedings{Kotonya2020ExplainableAutomatedFactChecking,
  title = {Explainable {{Automated Fact-Checking}}: {{A Survey}}},
  shorttitle = {Explainable {{Automated Fact-Checking}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Kotonya, Neema and Toni, Francesca},
  editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
  date = {2020-12},
  pages = {5430--5443},
  publisher = {International Committee on Computational Linguistics},
  location = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.474},
  url = {https://aclanthology.org/2020.coling-main.474/},
  urldate = {2025-07-15},
  abstract = {A number of exciting advances have been made in automated fact-checking thanks to increasingly larger datasets and more powerful systems, leading to improvements in the complexity of claims which can be accurately fact-checked. However, despite these advances, there are still desirable functionalities missing from the fact-checking pipeline. In this survey, we focus on the explanation functionality – that is fact-checking systems providing reasons for their predictions. We summarize existing methods for explaining the predictions of fact-checking systems and we explore trends in this topic. Further, we consider what makes for good explanations in this specific domain through a comparative analysis of existing fact-checking explanations against some desirable properties. Finally, we propose further research directions for generating fact-checking explanations, and describe how these may lead to improvements in the research area.},
  eventtitle = {{{COLING}} 2020}
}

@inproceedings{Kourani2024ProcessModelingLarge,
  title = {Process {{Modeling}} with~{{Large Language Models}}},
  booktitle = {Enterprise, {{Business-Process}} and {{Information Systems Modeling}}},
  author = {Kourani, Humam and Berti, Alessandro and Schuster, Daniel and family=Aalst, given=Wil M. P., prefix=van der, useprefix=true},
  editor = {family=Aa, given=Han, prefix=van der, useprefix=true and Bork, Dominik and Schmidt, Rainer and Sturm, Arnon},
  date = {2024},
  pages = {229--244},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-61007-3_18},
  abstract = {In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance the accessibility of process modeling, offering a more intuitive entry point for non-experts while augmenting the efficiency of experts. We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework’s ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field.},
  isbn = {978-3-031-61007-3},
  langid = {english}
}

@inproceedings{Koutra2011AlgorithmsGraphSimilarity,
  title = {Algorithms for Graph Similarity and Subgraph Matching},
  booktitle = {Proc. {{Ecol}}. {{Inference Conf}}.},
  author = {Koutra, Danai and Parikh, Ankur and Ramdas, Aaditya and Xiang, Jing},
  date = {2011},
  url = {https://people.eecs.berkeley.edu/~aramdas/reports/DBreport.pdf},
  urldate = {2018-09-04}
}

@article{Kpodjedo2014UsingLocalSimilarity,
  title = {Using Local Similarity Measures to Efficiently Address Approximate Graph Matching},
  author = {Kpodjedo, Segla and Galinier, Philippe and Antoniol, Giulio},
  date = {2014-02-19},
  journaltitle = {Discrete Applied Mathematics},
  shortjournal = {Discrete Applied Mathematics},
  series = {Combinatorial {{Optimization}}},
  volume = {164},
  pages = {161--177},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2012.01.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0166218X12000273},
  urldate = {2024-11-11},
  abstract = {In this paper, we investigate heuristics for Approximate Graph Matching (AGM), in particular when it can be formulated as a Maximum Common Edge Subgraph (MCES) problem. First, we observe empirically that initializing a local search with a tiny subset of a known optimal solution always results in much better solutions than starting with an empty solution. The main challenge could then be to retrieve such small subsets for any problem instance. For this purpose, we propose several local similarity measures and evaluate their ability to predict node matches which could be used to start a local search. The resulting algorithm (SIM-T) is a classic tabu algorithm that is initialized by a greedy procedure relying mainly, in its earliest steps, on similarity measures. We conducted experiments on a large collection of random graphs of various orders (from 50 to 3000 nodes) and densities. Results obtained are mostly excellent, especially on similar pairs of labeled graphs. Comparisons made with two recent state-of-the-art algorithms–“BP” and “PATH”–indicate a superiority of our approach, in terms of both scores and computation times.}
}

@article{Kreutz2022SchenQLIndepthAnalysis,
  title = {{{SchenQL}}: In-Depth Analysis of a Query Language for Bibliographic Metadata},
  shorttitle = {{{SchenQL}}},
  author = {Kreutz, Christin Katharina and Wolz, Michael and Knack, Jascha and Weyers, Benjamin and Schenkel, Ralf},
  date = {2022-06-01},
  journaltitle = {International Journal on Digital Libraries},
  shortjournal = {Int J Digit Libr},
  volume = {23},
  number = {2},
  pages = {113--132},
  issn = {1432-1300},
  doi = {10.1007/s00799-021-00317-8},
  url = {https://doi.org/10.1007/s00799-021-00317-8},
  urldate = {2024-02-01},
  abstract = {Information access to bibliographic metadata needs to be uncomplicated, as users may not benefit from complex and potentially richer data that may be difficult to obtain. Sophisticated research questions including complex aggregations could be answered with complex SQL queries. However, this comes with the cost of high complexity, which requires for a high level of expertise even for trained programmers. A domain-specific query language could provide a straightforward solution to this problem. Although less generic, it can support users not familiar with query construction in the formulation of complex information needs. In this paper, we present and evaluate SchenQL, a simple and applicable query language that is accompanied by a prototypical GUI. SchenQL focuses on querying bibliographic metadata using the vocabulary of domain experts. The easy-to-learn domain-specific query language is suitable for domain experts as well as casual users while still providing the possibility to answer complex information demands. Query construction and information exploration are supported by a prototypical GUI. We present an evaluation of the complete system: different variants for executing SchenQL queries are benchmarked; interviews with domain-experts and a bipartite quantitative user study demonstrate SchenQL’s suitability and high level of users’ acceptance.},
  langid = {english}
}

@inproceedings{Kreutz2022SchenQLQueryLanguage,
  title = {{{SchenQL}}: A Query Language for Bibliographic Data with Aggregations and Domain-Specific Functions},
  shorttitle = {{{SchenQL}}},
  booktitle = {Proceedings of the 22nd {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}}},
  author = {Kreutz, Christin Katharina and Blum, Martin and Schenkel, Ralf},
  date = {2022-06-20},
  series = {{{JCDL}} '22},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3529372.3533282},
  url = {https://dl.acm.org/doi/10.1145/3529372.3533282},
  urldate = {2024-02-01},
  abstract = {Current search interfaces of digital libraries are not suitable to satisfy complex or convoluted information needs directly, when it comes to cases such as "Find authors who only recently started working on a topic". They might offer possibilities to obtain this information only by requiring vast user interaction. We present SchenQL, a web interface of a domain-specific query language on bibliographic metadata, which offers information search and exploration by query formulation and navigation in the system. Our system focuses on supporting aggregation of data and providing specialised domain dependent functions while being suitable for domain experts as well as casual users of digital libraries.},
  isbn = {978-1-4503-9345-4}
}

@article{Krippendorff2004ReliabilityContentAnalysis,
  title = {Reliability in {{Content Analysis}}: {{Some Common Misconceptions}} and {{Recommendations}}},
  shorttitle = {Reliability in {{Content Analysis}}},
  author = {Krippendorff, Klaus},
  date = {2004-07-01},
  journaltitle = {Human Communication Research},
  shortjournal = {Human Communication Research},
  volume = {30},
  number = {3},
  pages = {411--433},
  issn = {0360-3989},
  doi = {10.1111/j.1468-2958.2004.tb00738.x},
  url = {https://doi.org/10.1111/j.1468-2958.2004.tb00738.x},
  urldate = {2021-03-12},
  abstract = {In a recent article in this journal, Lombard, Snyder-Duch, and Bracken (2002) surveyed 200 content analyses for their reporting of reliability tests, compared the virtues and drawbacks of five popular reliability measures, and proposed guidelines and standards for their use. Their discussion revealed that numerous misconceptions circulate in the content analysis literature regarding how these measures behave and can aid or deceive content analysts in their effort to ensure the reliability of their data. This article proposes three conditions for statistical measures to serve as indices of the reliability of data and examines the mathematical structure and the behavior of the five coefficients discussed by the authors, as well as two others. It compares common beliefs about these coefficients with what they actually do and concludes with alternative recommendations for testing reliability in content analysis and similar data-making efforts.}
}

@book{Krippendorff2018ContentAnalysisIntroduction,
  title = {Content Analysis: {{An}} Introduction to Its Methodology},
  shorttitle = {Content Analysis},
  author = {Krippendorff, Klaus},
  date = {2018},
  publisher = {Sage publications},
  abstract = {What matters in people's social lives? What motivates and inspires our society? How do we  enact what we know? Since the first edition published in 1980, Content Analysis has helped  shape and define the field. In the highly anticipated Fourth Edition, award-winning scholar and author Klaus Krippendorff introduces you to the most current method of analyzing the textual fabric of contemporary society. Students and scholars will learn to treat data not as physical events but as communications that are created and disseminated to be seen.}
}

@article{Kucuk2020StanceDetectionSurvey,
  title = {Stance {{Detection}}: {{A Survey}}},
  shorttitle = {Stance {{Detection}}},
  author = {Küçük, Dilek and Can, Fazli},
  date = {2020-02-06},
  journaltitle = {ACM Comput. Surv.},
  volume = {53},
  number = {1},
  pages = {12:1--12:37},
  issn = {0360-0300},
  doi = {10.1145/3369026},
  url = {https://dl.acm.org/doi/10.1145/3369026},
  urldate = {2025-07-15},
  abstract = {Automatic elicitation of semantic information from natural language texts is an important research problem with many practical application areas. Especially after the recent proliferation of online content through channels such as social media sites, news portals, and forums; solutions to problems such as sentiment analysis, sarcasm/controversy/veracity/rumour/fake news detection, and argument mining gained increasing impact and significance, revealed with large volumes of related scientific publications. In this article, we tackle an important problem from the same family and present a survey of stance detection in social media posts and (online) regular texts. Although stance detection is defined in different ways in different application settings, the most common definition is “automatic classification of the stance of the producer of a piece of text, towards a target, into one of these three classes: \{Favor, Against, Neither\}.” Our survey includes definitions of related problems and concepts, classifications of the proposed approaches so far, descriptions of the relevant datasets and tools, and related outstanding issues. Stance detection is a recent natural language processing topic with diverse application areas, and our survey article on this newly emerging topic will act as a significant resource for interested researchers and practitioners.}
}

@article{Kudenko2014SpecialIssueTransfer,
  title = {Special {{Issue}} on {{Transfer Learning}}},
  author = {Kudenko, Daniel},
  date = {2014-02-01},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {28},
  number = {1},
  pages = {5--6},
  issn = {1610-1987},
  doi = {10.1007/s13218-013-0289-5},
  url = {https://doi.org/10.1007/s13218-013-0289-5},
  urldate = {2020-09-29},
  langid = {english}
}

@article{Kuhn1955HungarianMethodAssignment,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  date = {1955},
  journaltitle = {Naval Research Logistics Quarterly},
  volume = {2},
  number = {1--2},
  pages = {83--97},
  issn = {1931-9193},
  doi = {10.1002/nav.3800020109},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
  urldate = {2025-06-24},
  abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
  langid = {english}
}

@article{Kumar2024LargeLanguageModels,
  title = {Large Language Models ({{LLMs}}): Survey, Technical Frameworks, and Future Challenges},
  shorttitle = {Large Language Models ({{LLMs}})},
  author = {Kumar, Pranjal},
  date = {2024-08-18},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {57},
  number = {10},
  pages = {260},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-10888-y},
  url = {https://doi.org/10.1007/s10462-024-10888-y},
  urldate = {2025-09-09},
  abstract = {Artificial intelligence (AI) has significantly impacted various fields. Large language models (LLMs) like GPT-4, BARD, PaLM, Megatron-Turing NLG, Jurassic-1 Jumbo etc., have contributed to our understanding and application of AI in these domains, along with natural language processing (NLP) techniques. This work provides a comprehensive overview of LLMs in the context of language modeling, word embeddings, and deep learning. It examines the application of LLMs in diverse fields including text generation, vision-language models, personalized learning, biomedicine, and code generation. The paper offers a detailed introduction and background on LLMs, facilitating a clear understanding of their fundamental ideas and concepts. Key language modeling architectures are also discussed, alongside a survey of recent works employing LLM methods for various downstream tasks across different domains. Additionally, it assesses the limitations of current approaches and highlights the need for new methodologies and potential directions for significant advancements in this field.},
  langid = {english}
}

@inproceedings{Kuo2010BridgingCommonSense,
  title = {Bridging Common Sense Knowledge Bases with Analogy by Graph Similarity},
  booktitle = {Proceedings of the 2nd {{AAAI Conference}} on {{Collaboratively-Built Knowledge Sources}} and {{Artificial Intelligence}}},
  author = {Kuo, Yen-Ling and Hsu, Jane Yung-Jen},
  date = {2010-01-01},
  series = {{{AAAIWS}}'10-02},
  pages = {22--27},
  publisher = {AAAI Press},
  abstract = {Present-day programs are brittle as computers are notoriously lacking in common sense. While significant progress has been made in building large common sense knowledge bases, they are intrinsically incomplete and inconsistent. This paper presents a novel approach to bridging the gaps between multiple knowledge bases, making it possible to answer queries based on knowledge collected from multiple sources without a common ontology. New assertions are found by computing graph similarity with principle component analysis to draw analogies across multiple knowledge bases. Experiments are designed to find new assertions for a Chinese commonsense knowledge base using the OMCS ConceptNet and similarly for WordNet. The assertions are voted by online users to verify that 75.77\% / 77.59\% for Chinese ConceptNet / WordNet respectively are good, despite the low overlap in coverage among the knowledge bases.}
}

@inproceedings{Kurtic2025GiveMeBF16,
  title = {“{{Give Me BF16}} or {{Give Me Death}}”? {{Accuracy-Performance Trade-Offs}} in {{LLM Quantization}}},
  shorttitle = {“{{Give Me BF16}} or {{Give Me Death}}”?},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kurtic, Eldar and Marques, Alexandre Noll and Pandit, Shubhra and Kurtz, Mark and Alistarh, Dan},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {26872--26886},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.1304/},
  urldate = {2025-08-01},
  abstract = {Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3\% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the “best” format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous deployment of mid and large-size models on high-end GPUs. Our results provide a first set of practical guidelines for deploying quantized LLMs across different scales and performance requirements.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Kusner2015WordEmbeddingsDocument,
  title = {From {{Word Embeddings To Document Distances}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}, {{ICML}} 2015, {{Lille}}, {{France}}, 6-11 {{July}} 2015},
  author = {Kusner, Matt J and Sun, Yu and Kolkin, Nicholas I and Weinberger, Kilian Q},
  date = {2015-01-01},
  pages = {957--966},
  url = {http://jmlr.org/proceedings/papers/v37/kusnerb15.html},
  urldate = {2018-09-01},
  abstract = {We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representatio...},
  eventtitle = {{{ICML}}}
}

@inproceedings{Lachenmaier2025CanLLMsGround,
  title = {Can {{LLMs Ground}} When They ({{Don}}'t) {{Know}}: {{A Study}} on {{Direct}} and {{Loaded Political Questions}}},
  shorttitle = {Can {{LLMs Ground}} When They ({{Don}}'t) {{Know}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lachenmaier, Clara and Sieker, Judith and Zarrieß, Sina},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {14956--14975},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.728/},
  urldate = {2025-07-29},
  abstract = {Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine LLMs' ability to answer direct knowledge questions and loaded questions that presuppose misinformation.We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias.Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@article{Landis1977MeasurementObserverAgreement,
  title = {The {{Measurement}} of {{Observer Agreement}} for {{Categorical Data}}},
  author = {Landis, J. Richard and Koch, Gary G.},
  date = {1977},
  journaltitle = {Biometrics},
  volume = {33},
  number = {1},
  eprint = {2529310},
  eprinttype = {jstor},
  pages = {159--174},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2529310},
  url = {https://www.jstor.org/stable/2529310},
  urldate = {2021-03-12},
  abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.}
}

@article{Lao2010RelationalRetrievalUsing,
  title = {Relational Retrieval Using a Combination of~Path-Constrained Random Walks},
  author = {Lao, Ni and Cohen, William W.},
  date = {2010-10-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {81},
  number = {1},
  pages = {53--67},
  issn = {1573-0565},
  doi = {10.1007/s10994-010-5205-8},
  url = {https://doi.org/10.1007/s10994-010-5205-8},
  urldate = {2020-06-16},
  abstract = {Scientific literature with rich metadata can be represented as a labeled directed graph. This graph representation enables a number of scientific tasks such as ad hoc retrieval or named entity recognition (NER) to be formulated as typed proximity queries in the graph. One popular proximity measure is called Random Walk with Restart (RWR), and much work has been done on the supervised learning of RWR measures by associating each edge label with a parameter. In this paper, we describe a novel learnable proximity measure which instead uses one weight per edge label sequence: proximity is defined by a weighted combination of simple “path experts”, each corresponding to following a particular sequence of labeled edges. Experiments on eight tasks in two subdomains of biology show that the new learning method significantly outperforms the RWR model (both trained and untrained). We also extend the method to support two additional types of experts to model intrinsic properties of entities: query-independent experts, which generalize the PageRank measure, and popular entity experts which allow rankings to be adjusted for particular entities that are especially important.},
  langid = {english}
}

@inproceedings{Lawrence2015CombiningArgumentMining,
  title = {Combining {{Argument Mining Techniques}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Argumentation Mining}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2015-06},
  pages = {127--136},
  publisher = {Association for Computational Linguistics},
  location = {Denver, CO},
  doi = {10.3115/v1/W15-0516},
  url = {https://www.aclweb.org/anthology/W15-0516},
  urldate = {2019-08-20}
}

@article{Lawrence2016ArgumentMiningUsing,
  title = {Argument {{Mining Using Argumentation Scheme Structures}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2016},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  pages = {379--390},
  issn = {0922-6389},
  doi = {10.3233/978-1-61499-686-6-379},
  url = {http://www.medra.org/servlet/aliasResolver?alias=iospressISBN&isbn=978-1-61499-685-9&spage=379&doi=10.3233/978-1-61499-686-6-379},
  urldate = {2019-08-20},
  abstract = {Argumentation schemes are patterns of human reasoning which have been detailed extensively in philosophy and psychology. In this paper we demonstrate that the structure of such schemes can provide rich information to the task of automatically identify complex argumentative structures in natural language text. By training a range of classifiers to identify the individual proposition types which occur in these schemes, it is possible not only to determine where a scheme is being used, but also the roles played by its component parts. Furthermore, this task can be performed on segmented natural language, with no prior knowledge of the text’s argumentative structure.},
  langid = {english}
}

@inproceedings{Lawrence2017MiningArgumentativeStructure,
  title = {Mining {{Argumentative Structure}} from {{Natural Language}} Text Using {{Automatically Generated Premise-Conclusion Topic Models}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Argument Mining}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2017-09},
  pages = {39--48},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/W17-5105},
  url = {https://www.aclweb.org/anthology/W17-5105},
  urldate = {2019-08-20},
  abstract = {This paper presents a method of extracting argumentative structure from natural language text. The approach presented is based on the way in which we understand an argument being made, not just from the words said, but from existing contextual knowledge and understanding of the broader issues. We leverage high-precision, low-recall techniques in order to automatically build a large corpus of inferential statements related to the text's topic. These statements are then used to produce a matrix representing the inferential relationship between different aspects of the topic. From this matrix, we are able to determine connectedness and directionality of inference between statements in the original text. By following this approach, we obtain results that compare favourably to those of other similar techniques to classify premise-conclusion pairs (with results 22 points above baseline), but without the requirement of large volumes of annotated, domain specific data.}
}

@article{Lawrence2019ArgumentMiningSurvey,
  title = {Argument {{Mining}}: {{A Survey}}},
  shorttitle = {Argument {{Mining}}},
  author = {Lawrence, John and Reed, Chris},
  date = {2019-10-08},
  journaltitle = {Computational Linguistics},
  volume = {45},
  number = {4},
  pages = {765--818},
  publisher = {MIT Press},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00364},
  url = {https://doi.org/10.1162/coli_a_00364},
  urldate = {2020-08-12},
  abstract = {Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.}
}

@inproceedings{Lawrence2019OnlineAnnotationAssistant,
  title = {An {{Online Annotation Assistant}} for {{Argument Schemes}}},
  booktitle = {Proceedings of the 13th {{Linguistic Annotation Workshop}}},
  author = {Lawrence, John and Visser, Jacky and Reed, Chris},
  date = {2019-08},
  pages = {100--107},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/W19-4012},
  url = {https://aclanthology.org/W19-4012},
  urldate = {2022-04-21},
  abstract = {Understanding the inferential principles underpinning an argument is essential to the proper interpretation and evaluation of persuasive discourse. Argument schemes capture the conventional patterns of reasoning appealed to in persuasion. The empirical study of these patterns relies on the availability of data about the actual use of argumentation in communicative practice. Annotated corpora of argument schemes, however, are scarce, small, and unrepresentative. Aiming to address this issue, we present one step in the development of improved datasets by integrating the Argument Scheme Key – a novel annotation method based on one of the most popular typologies of argument schemes – into the widely used OVA software for argument analysis.}
}

@online{LCMTeam2024LargeConceptModels,
  title = {Large {{Concept Models}}: {{Language Modeling}} in a {{Sentence Representation Space}}},
  shorttitle = {Large {{Concept Models}}},
  author = {{LCM Team} and Barrault, Loïc and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-jussà, Marta R. and Dale, David and Elsahar, Hady and Heffernan, Kevin and Janeiro, João Maria and Tran, Tuan and Ropers, Christophe and Sánchez, Eduardo and Roman, Robin San and Mourachko, Alexandre and Saleem, Safiyyah and Schwenk, Holger},
  date = {2024-12-15},
  eprint = {2412.08821},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.08821},
  url = {http://arxiv.org/abs/2412.08821},
  urldate = {2025-01-02},
  abstract = {LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content. In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, we build a "Large Concept Model". In this study, as proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities. The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation, and models operating in a quantized SONAR space. These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. We then scale one architecture to a model size of 7B parameters and training data of about 2.7T tokens. We perform an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion. Finally, we show that our model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size. The training code of our models is freely available.},
  pubstate = {prepublished}
}

@inproceedings{Leake2011HowManyCases,
  title = {How {{Many Cases Do You Need}}? {{Assessing}} and {{Predicting Case-Base Coverage}}},
  shorttitle = {How {{Many Cases Do You Need}}?},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Leake, David and Wilson, Mark},
  date = {2011-09-12},
  pages = {92--106},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23291-6_9},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-23291-6_9},
  urldate = {2021-05-27},
  abstract = {Case acquisition is the primary learning method for case-based reasoning (CBR), and the ability of a CBR system’s case-base to cover the problems it encounters is a crucial factor in its performance....},
  eventtitle = {International {{Conference}} on {{Case-Based Reasoning}}},
  langid = {english}
}

@inproceedings{Leake2019CombiningCaseAdaptation,
  title = {On {{Combining Case Adaptation Rules}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Leake, David and Ye, Xiaomeng},
  editor = {Bach, Kerstin and Marling, Cindy},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {204--218},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-29249-2_14},
  abstract = {The case adaptation process in case-based reasoning is often modeled as having two steps: enumerating differences between a new problem and the problem part of a retrieved case and then applying an adaptation rule for each difference. This model is sufficient when (1) predefined adaptation rules exist for all differences the system encounters, and (2) adaptation rules are sufficiently independent that interactions are not a major issue. This paper presents an approach to handling case adaptation when these assumptions fail. It proposes an approach, RObust ADaptation (ROAD), that uses heuristics to guide multi-step adaptations, with each adaptation chosen in the context of adaptations applied previously. To reduce the potential for accumulated degradation of solution quality from long adaptation chains, it performs incremental retrieval of new source cases along the adaptation path, resetting the partially modified case to the “ground truth” of existing cases when an existing case is nearby. An evaluation supports the benefits of the model and illuminates some tradeoffs.},
  isbn = {978-3-030-29249-2},
  langid = {english}
}

@inproceedings{Leake2020BringingCaseBasedReasoning,
  title = {On {{Bringing Case-Based Reasoning Methodology}} to {{Deep Learning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Leake, David and Crandall, David},
  editor = {Watson, Ian and Weber, Rosina},
  date = {2020},
  pages = {343--348},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58342-2_22},
  abstract = {The case-based reasoning community is successfully pursuing multiple approaches for applying deep learning methods to advance case-based reasoning. This “Challenges and Promises” paper argues for a complementary endeavor: pursuing ways that the case-based reasoning methodology can advance deep learning. Starting from challenges in deep learning and proposed neural-symbolic integrations based on specific technologies, it proposes studying how CBR ideas can inform choices of components for a new reasoning pipeline.},
  isbn = {978-3-030-58342-2},
  langid = {english}
}

@inproceedings{Lee2025HybGRAGHybridRetrievalAugmented,
  title = {{{HybGRAG}}: {{Hybrid Retrieval-Augmented Generation}} on {{Textual}} and {{Relational Knowledge Bases}}},
  shorttitle = {{{HybGRAG}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lee, Meng-Chieh and Zhu, Qi and Mavromatis, Costas and Han, Zhen and Adeshina, Soji and Ioannidis, Vassilis N. and Rangwala, Huzefa and Faloutsos, Christos},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {879--893},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.43/},
  urldate = {2025-07-29},
  abstract = {Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions?Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source.However, many questions require both textual and relational information from SKB — referred to as “hybrid” questions — which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information.In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA, consisting of a retriever bank and a critic module, with the following advantages:1. Agentic, it automatically refines the output by incorporating feedback from the critic module, 2. Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank,3. Interpretable, it justifies decision making with intuitive refinement path, and4. Effective, it surpasses all baselines on HQA benchmarks.In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51\%.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@book{Lee2025NaturalLanguageProcessing,
  title = {Natural {{Language Processing}}: {{A Textbook}} with {{Python Implementation}}},
  shorttitle = {Natural {{Language Processing}}},
  author = {Lee, Raymond},
  date = {2025},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-96-3208-4},
  url = {https://link.springer.com/10.1007/978-981-96-3208-4},
  urldate = {2025-05-13},
  isbn = {978-981-96-3207-7 978-981-96-3208-4},
  langid = {english}
}

@inproceedings{Lee2025ShiftingRankingSet,
  title = {Shifting from {{Ranking}} to {{Set Selection}} for {{Retrieval Augmented Generation}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lee, Dahyun and Jo, Yongrae and Park, Haeju and Lee, Moontae},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {17606--17619},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.861/},
  urldate = {2025-08-01},
  abstract = {Retrieval in Retrieval-Augmented Generation (RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set.Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering.In this work, we propose a set-wise passage selection approach and introduce SetR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements.Experiments on multi-hop RAG benchmarks show that SetR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems.The code is available at https://github.com/LGAI-Research/SetR},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@online{Lehnert2024BetterPlanningTransformers,
  title = {Beyond {{A}}*: {{Better Planning}} with {{Transformers}} via {{Search Dynamics Bootstrapping}}},
  shorttitle = {Beyond {{A}}*},
  author = {Lehnert, Lucas and Sukhbaatar, Sainbayar and Mcvay, Paul and Rabbat, Michael and Tian, Yuandong},
  date = {2024-02-21},
  eprint = {2402.14083},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.14083},
  url = {http://arxiv.org/abs/2402.14083},
  urldate = {2024-02-24},
  abstract = {While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7\% of the time, while using up to 26.8\% fewer search steps than standard \$A\textasciicircum *\$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of \$A\textasciicircum *\$. This model is then fine-tuned via expert iterations to perform fewer search steps than \$A\textasciicircum *\$ search while still generating an optimal plan. In our training method, \$A\textasciicircum *\$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10\$\textbackslash times\$ smaller model size and a 10\$\textbackslash times\$ smaller training dataset. We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics.},
  pubstate = {prepublished}
}

@book{Lenat1989BuildingLargeKnowledgebased,
  title = {Building {{Large Knowledge-based Systems}}: {{Representation}} and {{Inference}} in the {{Cyc Project}}},
  shorttitle = {Building {{Large Knowledge-based Systems}}},
  author = {Lenat, Douglas B. and Guha, R. V.},
  date = {1989},
  eprint = {55NQAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {Addison-Wesley Publishing Company},
  abstract = {Chapter one presents the Cyc "philosophy" or paradigm. Chapter 2 presents a global overview of Cyc, including its representation language, the ontology f its knowledge base, and teh environment which it functions. Chapter 3 goes into much more detail on the representation language, including the structure and function of Cyc's metalevel agenda mechanism. Chapter 4 presents heuristics for ontological engineering, the pricnples upon whcihc Cyc's ontology is based. Chapter 5 the provides a glimpse into the global ontology of knowledge. Chapter 6 explains how we "solve" (i.e., adequately handle) the various tough representation thorns (substances, time, space, structures, composite mental/physical objects, beliefs, uncertainty, etc. ). Chapter 7 surveys the mistakes that new knowledge tnereres most often commit. Chapter 8, the concluding chapter, includes a brief status report on the project, and a statement of goals and a timetable for the coming five years.},
  isbn = {978-0-201-51752-1},
  langid = {english},
  pagetotal = {408}
}

@inproceedings{Lendvai2016MonolingualSocialMedia,
  title = {Monolingual {{Social Media Datasets}} for {{Detecting Contradiction}} and {{Entailment}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Lendvai, Piroska and Augenstein, Isabelle and Bontcheva, Kalina and Declerck, Thierry},
  date = {2016-05},
  pages = {4602--4605},
  publisher = {European Language Resources Association (ELRA)},
  location = {Portorož, Slovenia},
  url = {https://aclanthology.org/L16-1729},
  urldate = {2022-01-03},
  abstract = {Entailment recognition approaches are useful for application domains such as information extraction, question answering or summarisation, for which evidence from multiple sentences needs to be combined. We report on a new 3-way judgement Recognizing Textual Entailment (RTE) resource that originates in the Social Media domain, and explain our semi-automatic creation method for the special purpose of information verification, which draws on manually established rumourous claims reported during crisis events. From about 500 English tweets related to 70 unique claims we compile and evaluate 5.4k RTE pairs, while continue automatizing the workflow to generate similar-sized datasets in other languages.},
  eventtitle = {{{LREC}} 2016}
}

@incollection{Lenz1998TextualCBR,
  title = {Textual {{CBR}}},
  booktitle = {Case-{{Based Reasoning Technology}}: {{From Foundations}} to {{Applications}}},
  author = {Lenz, Mario and Hübner, André and Kunze, Mirjam},
  editor = {Lenz, Mario and Burkhard, Hans-Dieter and Bartsch-Spörl, Brigitte and Wess, Stefan},
  date = {1998},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {115--137},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-69351-3_5},
  url = {https://doi.org/10.1007/3-540-69351-3_5},
  urldate = {2021-02-19},
  abstract = {In this chapter, we will explore a fairly new direction of research in the case-based reasoning community, namely the handling of textual documents. We will explain first why the ability to deal with natural language texts is crucial. We will then discuss more traditional methods for these tasks. Following this, we present the approach of Textual CBR and, in particular, the CBR-Answers project.},
  isbn = {978-3-540-69351-2},
  langid = {english}
}

@inproceedings{Lenz1998TextualCBRInformation,
  title = {Textual {{CBR}} and {{Information Retrieval}} - {{A Comparison}}},
  booktitle = {In {{Proceedings}} 6th {{German Workshop}} on {{CBR}}},
  author = {Lenz, Mario},
  date = {1998},
  abstract = {In recent years, quite a number of projects started to apply case-based reasoning technology to textual documents instead of highly structured cases. For this the term Textual CBR has been coined. In this paper, we give an overview over the main ideas of Textual CBR and compare it with Information Retrieval techniques. We also present some preliminary results obtained from three projects performed which further demonstrate major advantages of Textual CBR.  Keywords: Textual case-based reasoning, document management, knowledge acquisition. 1 Introduction  In recent years, case-based reasoning (CBR) researchers started to address tasks that have traditionally been coped with by the Information Retrieval community, namely the handling of textual documents. When considering the roots of CBR, this development is not surprising at all: CBR tries to solve problems by explicitly reusing experiences collected during earlier problem solving situations. In practice, however, many of these experie...}
}

@thesis{Lenz2018RetrievalArgumentationGraphs,
  type = {bathesis},
  title = {Retrieval of {{Argumentation Graphs}} Using {{Concatenated Word Embeddings}} and {{Argumentation Schemes}}},
  author = {Lenz, Mirko},
  date = {2018-07-09},
  institution = {Trier University},
  location = {Trier},
  abstract = {This thesis will outline an approach to retrieve arguments from a set of stored ones using both structural and semantic similarity computations. The novelty of the approach is the use of multiple, concatenated word embeddings and aggregation functions to increase the amount of contextual information. In existing works, this method has only been used for classification tasks. This thesis shows that the concatenation combines the benefits of multiple embeddings while not having the same weaknesses. Furthermore, argumentation schemes will be used to enhance the structural retrieval even further. These schemes provide a detailed description of the relation between two arguments. The use of them also aims at incorporating more information into the retrieval process. This thesis shows that the use of argumentation schemes provides further benefits to the retrieval process. The retrieval is performed on German texts and it will be shown that the publicly available embeddings for this language are not as detailed as their English counterparts.},
  langid = {english},
  pagetotal = {88}
}

@inproceedings{Lenz2019SemanticTextualSimilarity,
  title = {Semantic {{Textual Similarity Measures}} for {{Case-Based Retrieval}} of {{Argument Graphs}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Lenz, Mirko and Ollinger, Stefan and Sahitaj, Premtim and Bergmann, Ralph},
  editor = {Bach, Kerstin and Marling, Cindy},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11680},
  pages = {219--234},
  publisher = {Springer International Publishing},
  location = {Otzenhausen, Germany},
  doi = {10.1007/978-3-030-29249-2_15},
  abstract = {Argumentation is an important sub-field of Artificial Intelligence, which involves computational methods for reasoning and decision making based on argumentative structures. This paper contributes to case-based reasoning with argument graphs in the standardized Argument Interchange Format by improving the similarity-based retrieval phase. We explore a large range of novel approaches for semantic textual similarity measures (both supervised and unsupervised) and use them in the context of a graph-based similarity measure for argument graphs. In addition, the use of an ontology-based semantic similarity measure for argumentation schemes is investigated. With a range of experiments we demonstrate the strengths and weaknesses of the various methods and show that our methods can improve over our previous work. Our code is publicly available on GitHub.},
  eventtitle = {{{ICCBR}} 2019},
  isbn = {978-3-030-29249-2},
  langid = {english}
}

@inproceedings{Lenz2020ArgumentMiningPipeline,
  title = {Towards an {{Argument Mining Pipeline Transforming Texts}} to {{Argument Graphs}}},
  booktitle = {Computational {{Models}} of {{Argument}}},
  author = {Lenz, Mirko and Sahitaj, Premtim and Kallenberg, Sean and Coors, Christopher and Dumani, Lorik and Schenkel, Ralf and Bergmann, Ralph},
  date = {2020},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {326},
  pages = {263--270},
  publisher = {IOS Press},
  location = {Virtual Event},
  doi = {10.3233/FAIA200510},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200510},
  abstract = {This paper tackles the automated extraction of components of argumentative information and their relations from natural language text. Moreover, we address a current lack of systems to provide a complete argumentative structure from arbitrary natural language text for general usage. We present an argument mining pipeline as a universally applicable approach for transforming German and English language texts to graph-based argument representations. We also introduce new methods for evaluating the performance based on existing benchmark argument structures. Our results show that the generated argument graphs can be beneficial to detect new connections between different statements of an argumentative text.},
  eventtitle = {International {{Conference}} on {{Computational Models}} of {{Argument}}},
  langid = {english}
}

@thesis{Lenz2021GeneralizationArgumentGraphs,
  type = {mathesis},
  title = {Generalization of {{Argument Graphs}} Based on {{User Queries}}},
  author = {Lenz, Mirko},
  date = {2021-03-22},
  institution = {Trier University},
  location = {Trier},
  abstract = {Argumentation is central to many aspects of human interaction. Finding the best arguments is possible with established web search engines, even though they are only able to process them in their textual form. Argumentation machines on the other hand are able to consider structural elements as well—for instance, when representing relations between arguments as graphs. Operating directly on graphs eliminates the user’s task of manually structuring the collected information. However, the found argument may not properly fulfill the information need expressed via the query. It may happen that the topic of the found argument deals with a special case of the query—for instance, asking for arguments about “animals”, but being presented with the special case “dogs”. To tackle this task, this thesis investigates the generalization of argument graphs using a text-based approach. By identifying words that could benefit from being generalized, I propose two methods of determining appropriate replacements. Both techniques make extensive use of background knowledge in the form of commonsense knowledge (e.g., dog is an animal). A current limitation is that the user has to provide a “starting point” for this generalization, making the process not fully-automatic. The proposed approach is complemented by a fully featured implementation, a corpus containing generalizations crafted by experts, and a detailed evaluation of the techniques. I come to the conclusion that the generalization of argument graphs is a difficult task to solve, mainly caused by the high amount of subjectivity involved in the whole process. While trying to approximate the assessments of the experts does not seem to be a feasible strategy, I was able to achieve better results by allowing the system more freedom during the generalization process.},
  langid = {english},
  pagetotal = {96}
}

@inproceedings{Lenz2022ComparingUnsupervisedAlgorithms,
  title = {Comparing {{Unsupervised Algorithms}} to {{Construct Argument Graphs}}},
  booktitle = {Joint {{Proceedings}} of {{Workshops}}, {{Tutorials}} and {{Doctoral Consortium}} Co-Located with the 45th {{German Conference}} on {{Artificial Intelligence}}},
  author = {Lenz, Mirko and Dumani, Lorik and Sahitaj, Premtim},
  editor = {Koert, Dorothea and Minor, Mirjam},
  date = {2022-09-19},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3457},
  publisher = {CEUR},
  location = {Virtual Event, Trier},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3457/#paper3tmg},
  abstract = {Computational argumentation has gained considerable attention in recent years. Various areas have been addressed, such as extracting arguments from natural language texts into a structured form in order to store them in an argument base, determining stances for arguments with respect to topics, determination of inferences from statements, and much more. After so much progress has been made in the isolated tasks, in this paper we address the next level and aim to advance the automatic generation of argument graphs. To this end, we investigate various unsupervised methods for constructing the graphs and measure the performance with different metrics on three different datasets. Our implementation is publicly available on GitHub under the permissive MIT license.},
  eventtitle = {{{TMG}} 2022},
  langid = {english}
}

@inproceedings{Lenz2022UserCentricArgumentMining,
  title = {User-{{Centric Argument Mining}} with {{ArgueMapper}} and {{Arguebuf}}},
  booktitle = {Computational {{Models}} of {{Argument}}},
  author = {Lenz, Mirko and Bergmann, Ralph},
  date = {2022},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {353},
  pages = {367--368},
  publisher = {IOS Press},
  location = {Cardiff, Wales},
  doi = {10.3233/FAIA220176},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA220176},
  abstract = {Existing tools to create argument graphs are tailored for experts in the domain of argumentation. By taking into account the needs of experts, laymen, and developers, we propose ArgueMapper as a novel argument diagramming tool and Arguebuf as its underlying format. ArgueMapper is the first of its kind to be optimized for mobile devices and provide a discoverable interface suitable for novice users. Arguebuf provides native implementations for all major programming languages via a code generation approach. To complement Arguebuf, we provide a supercharged Python implementation that enables advanced analysis. All of our contributions support AIF and are publicly available on GitHub under the MIT license.},
  eventtitle = {International {{Conference}} on {{Computational Models}} of {{Argument}}},
  langid = {english}
}

@inproceedings{Lenz2022WorkshopTextMining,
  title = {Workshop on {{Text Mining}} and {{Generation}} ({{TMG}}): {{Preface}}},
  booktitle = {Joint {{Proceedings}} of {{Workshops}}, {{Tutorials}} and {{Doctoral Consortium}} Co-Located with the 45th {{German Conference}} on {{Artificial Intelligence}}},
  author = {Lenz, Mirko and Dumani, Lorik and Bondarenko, Alexander and Syed, Shahbaz},
  editor = {Koert, Dorothea and Minor, Mirjam},
  date = {2022-09-19},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3457},
  publisher = {CEUR},
  location = {Virtual Event, Trier},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3457/#preface_tmg},
  abstract = {This paper is a report on the first Text Mining and Generation Workshop (TMG), which was a one-day virtual event hosted at the German Conference on Artificial Intelligence (KI 2022) in Trier, Germany. In addition to four accepted original papers, there were three invited talks by speakers who presented their works already published at high-ranked conferences as well as one keynote by a pioneer in the two research fields relevant to the workshop.},
  eventtitle = {{{TMG}} 2022},
  langid = {english}
}

@inproceedings{Lenz2023CaseBasedAdaptationArgument,
  title = {Case-{{Based Adaptation}} of~{{Argument Graphs}} with~{{WordNet}} and~{{Large Language Models}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Lenz, Mirko and Bergmann, Ralph},
  editor = {Massie, Stewart and Chakraborti, Sutanu},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14141},
  pages = {263--278},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-40177-0_17},
  abstract = {Finding information online is hard, even more so once you get into the domain of argumentation. There have been developments around the specialized argumentation machines that incorporate structural features of arguments, but all current approaches share one pitfall: They operate on a corpora of limited sizes. Consequently, it may happen that a user searches for a rather general term like cost increases, but the machine is only able to serve arguments concerned with rent increases. We aim to bridge this gap by introducing approaches to generalize/specialize a found argument using a combination of WordNet and Large Language Models. The techniques are evaluated on a new benchmark dataset with diverse queries using our fully featured implementation. Both the dataset and the code are publicly available on GitHub.},
  eventtitle = {{{ICCBR}} 2023},
  isbn = {978-3-031-40177-0},
  langid = {english},
  annotation = {Best Student Paper Award at ICCBR 2023}
}

@inproceedings{Lenz2024ArgServicesMicroserviceBasedArchitecture,
  title = {{{ArgServices}}: {{A Microservice-Based Architecture}} for~{{Argumentation Machines}}},
  shorttitle = {{{ArgServices}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Lenz, Mirko and Dumani, Lorik and Schenkel, Ralf and Bergmann, Ralph},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14638},
  pages = {352--369},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_21},
  abstract = {Argumentation is ubiquitous, and the development of argumentation machines could greatly assist humans in managing and navigating argumentation. However, the development of such systems is hindered by the lack of common standards and suitable tools, leading to ad-hoc solutions with little reuse value. Towards a more unified approach, we present an extensible microservice-based architecture for argumentation machines. Being built on the established gRPC framework, it provides strongly typed interfaces for the following services: (i) Argument Mining, (ii) Case-Based Reasoning on Arguments, (iii) Argument Retrieval and Ranking, and (iv) Quality Assessment of Arguments. Our system is designed to be extensible, allowing for easy integration of new tasks. We demonstrate the feasibility of our architecture via a proof-of-concept implementation and provide additional supplementary resources, such as a REST API gateway. Our contributions are publicly available on GitHub under the permissive MIT license.},
  eventtitle = {{{RATIO}} 2024},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Lenz2024CBRkitIntuitiveCaseBased,
  title = {{{CBRkit}}: {{An Intuitive Case-Based Reasoning Toolkit}} for~{{Python}}},
  shorttitle = {{{CBRkit}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Lenz, Mirko and Malburg, Lukas and Bergmann, Ralph},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14775},
  pages = {289--304},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_19},
  abstract = {Developing Case-Based Reasoning (CBR) applications is a complex and demanding task that requires a lot of experience and a deep understanding of users. Additionally, current CBR frameworks are not as usable as Machine Learning (ML) frameworks that can be deployed with only a few lines of code. To address these problems and allow users to easily build hybrid Artificial Intelligence (AI) systems by combining CBR with techniques such as ML, we present the CBRkit library in this paper. CBRkit is a Python-based framework that provides generic and easily extensible functions to simplify the creation of CBR applications with advanced similarity measures and case representations. The framework is available from GitHub and PyPI under the permissive MIT license. An initial user study indicates that it is easily possible even for non-CBR experts and users who only have limited Python programming skills to develop their own customized CBR application.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english},
  annotation = {Best Student Paper Award at ICCBR 2024}
}

@inproceedings{Lenz2024PolArgUnsupervisedPolarity,
  title = {{{PolArg}}: {{Unsupervised Polarity Prediction}} of~{{Arguments}} in~{{Real-Time Online Conversations}}},
  shorttitle = {{{PolArg}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Lenz, Mirko and Bergmann, Ralph},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14638},
  pages = {108--126},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_7},
  abstract = {The increasing usage of social networks has led to a growing number of discussions on the Internet that are a valuable source of argumentation that occurs in real time. Such conversations are often made up of a large number of participants and are characterized by a fast pace. Platforms like X/Twitter and Hacker News (HN) allow users to respond to other users’ posts, leading to a tree-like structure. Previous work focused on training supervised models on datasets obtained from debate portals like Kialo where authors provide polarity labels (i.e., support/attack) together with their posts. Such classifiers may yield suboptimal predictions for the noisier posts from X or HN, so we propose unsupervised prompting strategies for large language models instead. Our experimental evaluation found this approach to be more effective for X conversations than a model fine-tuned on Kialo debates, but less effective for HN posts (which are more technical and less argumentative). Finally, we provide an open-source application for converting discussions on these platforms into argument graphs.},
  eventtitle = {{{RATIO}} 2024},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Lenz2025ArgueMapperAssistantInteractive,
  title = {{{ArgueMapper Assistant}}: {{Interactive Argument Mining Using Generative Language Models}}},
  shorttitle = {{{ArgueMapper Assistant}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Lenz, Mirko and Bergmann, Ralph},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  volume = {15446},
  pages = {189--203},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77915-2_14},
  abstract = {Structured arguments are a valuable resource for analyzing and understanding complex topics. However, manual annotation is time-consuming and often not feasible for large datasets, and automated approaches are less accurate. To address this issue, we propose an interactive argument mining system that takes advantage of generative language models to support humans in the creation of argument graphs. We present the open source ArgueMapper Assistant featuring two prompting strategies and evaluate it on a real-world news dataset. The resulting corpus containing 88 argument graphs is publicly available as well. With generative models, the annotation time is reduced by about 20\% while the number of errors is slightly increased (mostly due to missing argumentative units and wrong relation types). A survey provides insights into the usefulness and reliability of the assistant features and shows that participants prefer to use the assistant in the future.},
  eventtitle = {{{SGAI}} 2024},
  isbn = {978-3-031-77915-2},
  langid = {english}
}

@inproceedings{Lenz2025LLsiMLargeLanguage,
  title = {{{LLsiM}}: {{Large Language Models}} for~{{Similarity Assessment}} in~{{Case-Based Reasoning}}},
  shorttitle = {{{LLsiM}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Lenz, Mirko and Hoffmann, Maximilian and Bergmann, Ralph},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15662},
  pages = {126--141},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_9},
  abstract = {In Case-Based Reasoning (CBR), past experience is used to solve new problems. Determining the most relevant cases is a crucial aspect of this process and is typically based on one or multiple manually-defined similarity measures, requiring deep domain knowledge. To overcome the knowledge-acquisition bottleneck, we propose the use of Large Language Models (LLMs) to automatically assess similarities between cases. We present three distinct approaches where the model is used for different tasks: (i) to predict similarity scores, (ii) to assess pairwise preferences, and (iii) to automatically configure similarity measures. Our conceptual work is accompanied by an open-source Python implementation that we use to evaluate the approaches on three different domains by comparing them to manually crafted similarity measures. Our results show that directly using LLM-based scores does not align well with the baseline rankings, but letting the LLM automatically configure the measures yields rankings that closely resemble the expert-defined ones.},
  eventtitle = {{{ICCBR}} 2025},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@inproceedings{Lenz2026NumbersDontLie,
  title = {Numbers {{Don}}’t {{Lie}}: {{Hybrid Extraction}} and~{{Validation}} of~{{Quantitative Statements}} in~{{Arguments}} with~{{Semi-structured Information}}},
  shorttitle = {Numbers {{Don}}’t {{Lie}}},
  booktitle = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}},
  author = {Lenz, Mirko and Dumani, Lorik and Schenkel, Ralf and Bergmann, Ralph},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15956},
  pages = {77--90},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6_6},
  abstract = {Evidence in arguments may be stated in various forms, including quantitative statements (i.e., numerical relations between entities). This measurable information can be validated against reliable sources like Wikipedia to combat the spread of misinformation. In this paper, we propose a four-step pipeline that combines rule-based techniques with prompting strategies for generative language models in a hybrid fashion. We use regular expressions to identify candidates in claim-premise structures, extract statements using GPT-4o, augment the data with tables from Wikipedia, and validate statements through retrieval-augmented generation (RAG). The pipeline is evaluated on two existing argumentation corpora and the generated dataset is manually annotated to assess the quality of our predictions, showing promising results for extraction and mixed results for validation. Our code and data are available to foster further research in this area.},
  eventtitle = {German {{Conference}} on {{Artificial Intelligence}}},
  isbn = {978-3-032-02813-6},
  langid = {english}
}

@article{Levenshtein1966BinaryCodesCapable,
  title = {Binary {{Codes Capable}} of {{Correcting Deletions}}, {{Insertions}} and {{Reversals}}},
  author = {Levenshtein, V. I.},
  date = {1966-02-01},
  journaltitle = {Soviet Physics Doklady},
  shortjournal = {Soviet Physics Doklady},
  volume = {10},
  pages = {707},
  url = {http://adsabs.harvard.edu/abs/1966SPhD...10..707L},
  urldate = {2018-09-01},
  abstract = {Not Available}
}

@article{Lewinski2025OneConceptArgument,
  title = {One {{Concept}} of {{Argument}}},
  author = {Lewiński, Marcin},
  date = {2025-05-14},
  journaltitle = {Argumentation},
  shortjournal = {Argumentation},
  issn = {1572-8374},
  doi = {10.1007/s10503-025-09654-3},
  url = {https://doi.org/10.1007/s10503-025-09654-3},
  urldate = {2025-09-08},
  abstract = {Part of the business of argumentation theory involves resolving a conceptual dispute over what argumentation and argument are in the first place. This dispute has produced various “concepts of argument.” The goal of this paper is twofold: (1) to develop a complete ontology of argumentative phenomena, capable of accounting for various conceptions of argument—something, as I argue, that is badly wanting in argumentation theory; and, within this ontology, (2) to defend a position that there is but one concept of argument needed to grasp these diverse phenomena and conceptions of argument and argumentation. I move in four steps. First, I briefly sketch the discussion over arguments-as-activities and arguments-as-products. Second, I go back to the classic work of Twardowski on actions and products and adapt it for argumentation theory, producing a complex yet systematically organized conceptual ontology of argument and argumentation. This conceptual housekeeping allows me, third, to critically engage some of the recent, Frege-inspired philosophical literature on the concept of argument, while defending act-based approaches to argument(ation). Fourth, I present a positive proposal of a minimal, contrastivist concept of argument as a set of reasons advanced to support a conclusion C1 rather than another conclusion Cn.},
  langid = {english}
}

@inproceedings{Lewis2020RetrievalAugmentedGenerationKnowledgeIntensive,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  date = {2020},
  volume = {33},
  pages = {9459--9474},
  publisher = {Curran Associates, Inc.},
  url = {https://dl.acm.org/doi/abs/10.5555/3495724.3496517},
  urldate = {2024-04-01},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.}
}

@online{Li2025LargeLanguageModels,
  title = {Large {{Language Models}} in {{Argument Mining}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} in {{Argument Mining}}},
  author = {Li, Hao and Schlegel, Viktor and Sun, Yizheng and Batista-Navarro, Riza and Nenadic, Goran},
  date = {2025-08-04},
  eprint = {2506.16383},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.16383},
  url = {http://arxiv.org/abs/2506.16383},
  urldate = {2025-09-03},
  abstract = {Argument Mining (AM), a critical subfield of Natural Language Processing (NLP), focuses on extracting argumentative structures from text. The advent of Large Language Models (LLMs) has profoundly transformed AM, enabling advanced in-context learning, prompt-based generation, and robust cross-domain adaptability. This survey systematically synthesizes recent advancements in LLM-driven AM. We provide a concise review of foundational theories and annotation frameworks, alongside a meticulously curated catalog of datasets. A key contribution is our comprehensive taxonomy of AM subtasks, elucidating how contemporary LLM techniques -- such as prompting, chain-of-thought reasoning, and retrieval augmentation -- have reconfigured their execution. We further detail current LLM architectures and methodologies, critically assess evaluation practices, and delineate pivotal challenges including long-context reasoning, interpretability, and annotation bottlenecks. Conclusively, we highlight emerging trends and propose a forward-looking research agenda for LLM-based computational argumentation, aiming to strategically guide researchers in this rapidly evolving domain.},
  pubstate = {prepublished}
}

@inproceedings{Liao2018MachineLearningApproach,
  title = {A {{Machine Learning Approach}} to {{Case Adaptation}}},
  booktitle = {2018 {{IEEE First International Conference}} on {{Artificial Intelligence}} and {{Knowledge Engineering}} ({{AIKE}})},
  author = {Liao, Chieh-Kang and Liu, Alan and Chao, Yu-Sheng},
  date = {2018-09},
  pages = {106--109},
  doi = {10.1109/AIKE.2018.00023},
  url = {https://ieeexplore.ieee.org/document/8527455},
  urldate = {2025-07-07},
  abstract = {The idea of case-based reasoning (CBR) is based on experts, who prefer to rely on their experience in solving similar problems which have been solved before. Each successful experience of problem solving is stored as a case, and a case can be reused in solving a similar problem in the future. However, experience may not be exactly the same as the target problem that they are facing. To make use of experience, case adaptation is necessary. There are several issues to consider when implementing case adaptation in a CBR system, including the comprehension of each case and design of a case adaptation method. The system retrieves the most similar case depending on attributes of the target problem, and the solution part of the retrieved case will then be refined with case adaptation. The goal of this study is to design and implement case adaptation with the aide of artificial neural networks and the concept of heuristics. The experiments show that the proposed approach is efficient in adjusting the solution to fit the needs of the target problem.},
  eventtitle = {2018 {{IEEE First International Conference}} on {{Artificial Intelligence}} and {{Knowledge Engineering}} ({{AIKE}})}
}

@inproceedings{Lieber2018MakingBestCases,
  title = {Making the {{Best}} of {{Cases}} by {{Approximation}}, {{Interpolation}} and {{Extrapolation}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Lieber, Jean and Nauer, Emmanuel and Prade, Henri and Richard, Gilles},
  editor = {Cox, Michael T. and Funk, Peter and Begum, Shahina},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {580--596},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01081-2_38},
  abstract = {Case-based reasoning usually exploits source cases (consisting of a source problem and its solution) individually, on the basis of the similarity between the target problem and a particular source problem. This corresponds to approximation. Then the solution of the source case has to be adapted to the target. We advocate in this paper that it is also worthwhile to consider source cases by two, or by three. Handling cases by two allows for a form of interpolation, when the target problem is between two similar source problems. When cases come by three, it offers a basis for extrapolation. Namely the solution of the target problem is obtained, when possible, as the fourth term of an analogical proportion linking the three source cases with the target, where the analogical proportion handles both similarity and dissimilarity between cases. Experiments show that interpolation and extrapolation techniques are of interest for reusing cases, either in an independent or in a combined way.},
  isbn = {978-3-030-01081-2},
  langid = {english}
}

@article{Likert1932TechniqueMeasurementAttitudes,
  title = {A Technique for the Measurement of Attitudes},
  author = {Likert, R.},
  date = {1932},
  journaltitle = {Archives of Psychology},
  volume = {22  140},
  pages = {55--55},
  abstract = {The project conceived in 1929 by Gardner Murphy and the writer aimed first to present a wide array of problems having to do with five major "attitude areas"—international relations, race relations, economic conflict, political conflict, and religion. The kind of questionnaire material falls into four classes: yes-no, multiple choice, propositions to be responded to by degrees of approval, and a series of brief newspaper narratives to be approved or disapproved in various degrees. The monograph aims to describe a technique rather than to give results. The appendix, covering ten pages, shows the method of constructing an attitude scale. A bibliography is also given. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@inproceedings{Lin2015ModelingRelationPaths,
  title = {Modeling {{Relation Paths}} for {{Representation Learning}} of {{Knowledge Bases}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong and Rao, Siwei and Liu, Song},
  date = {2015-09},
  pages = {705--714},
  publisher = {Association for Computational Linguistics},
  location = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1082},
  url = {https://www.aclweb.org/anthology/D15-1082},
  urldate = {2020-05-31},
  eventtitle = {{{EMNLP}} 2015}
}

@inproceedings{Lin2023ArgueMeTersely,
  title = {Argue with {{Me Tersely}}: {{Towards Sentence-Level Counter-Argument Generation}}},
  shorttitle = {Argue with {{Me Tersely}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lin, Jiayu and Ye, Rong and Han, Meng and Zhang, Qi and Lai, Ruofei and Zhang, Xinyu and Cao, Zhao and Huang, Xuanjing and Wei, Zhongyu},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {16705--16720},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.1039},
  url = {https://aclanthology.org/2023.emnlp-main.1039/},
  urldate = {2025-09-03},
  abstract = {Counter-argument generation—a captivating area in computational linguistics—seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused challenges. Furthermore, the diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics. In this paper, we present the ArgTersely benchmark for sentence-level counter-argument generation, drawing from a manually annotated dataset from the ChangeMyView debate forum. We also propose Arg-LlaMA for generating high-quality counter-argument. For better evaluation, we trained a BERT-based evaluator Arg-Judge with human preference data. We conducted comparative experiments involving various baselines such as LlaMA, Alpaca, GPT-3, and others. The results show the competitiveness of our proposed framework and evaluator in counter-argument generation tasks. Code and data are available at https://github.com/amazingljy1206/ArgTersely.},
  eventtitle = {{{EMNLP}} 2023}
}

@article{Lippi2016ArgumentationMiningState,
  title = {Argumentation {{Mining}}: {{State}} of the {{Art}} and {{Emerging Trends}}},
  shorttitle = {Argumentation {{Mining}}},
  author = {Lippi, Marco and Torroni, Paolo},
  date = {2016-03-30},
  journaltitle = {ACM Trans. Internet Technol.},
  volume = {16},
  number = {2},
  pages = {10:1--10:25},
  issn = {1533-5399},
  doi = {10.1145/2850417},
  url = {https://dl.acm.org/doi/10.1145/2850417},
  urldate = {2025-09-18},
  abstract = {Argumentation mining aims at automatically extracting structured arguments from unstructured textual documents. It has recently become a hot topic also due to its potential in processing information originating from the Web, and in particular from social media, in innovative ways. Recent advances in machine learning methods promise to enable breakthrough applications to social and economic sciences, policy making, and information technology: something that only a few years ago was unthinkable. In this survey article, we introduce argumentation models and methods, review existing systems and applications, and discuss challenges and perspectives of this exciting new research area.}
}

@article{Liu2004ConceptNetPracticalCommonsense,
  title = {{{ConceptNet}} — {{A Practical Commonsense Reasoning Tool-Kit}}},
  author = {Liu, H. and Singh, P.},
  date = {2004-10-01},
  journaltitle = {BT Technology Journal},
  shortjournal = {BT Technology Journal},
  volume = {22},
  number = {4},
  pages = {211--226},
  issn = {1573-1995},
  doi = {10.1023/B:BTTJ.0000047600.45421.6d},
  url = {https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d},
  urldate = {2021-02-10},
  abstract = {ConceptNet is a freely available commonsense knowledge base and natural-language-processing tool-kit which supports many practical textual-reasoning tasks over real-world documents including topic-gisting, analogy-making, and other context oriented inferences. The knowledge base is a semantic network presently consisting of over 1.6 million assertions of commonsense knowledge encompassing the spatial, physical, social, temporal, and psychological aspects of everyday life. ConceptNet is generated automatically from the 700 000 sentences of the Open Mind Common Sense Project — a World Wide Web based collaboration with over 14 000 authors.},
  langid = {english}
}

@inproceedings{Liu2021SwinTransformerHierarchical,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer Using Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021},
  pages = {10012--10022},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html},
  urldate = {2025-02-10},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english}
}

@inproceedings{Liu2022SwinTransformerV2,
  title = {Swin {{Transformer V2}}: {{Scaling Up Capacity}} and {{Resolution}}},
  shorttitle = {Swin {{Transformer V2}}},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
  date = {2022},
  pages = {12009--12019},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html},
  urldate = {2025-02-10},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english}
}

@software{Liu2024InstructorLibraryStructured,
  title = {Instructor: {{A Library}} for {{Structured Outputs}} from {{Large Language Models}}},
  shorttitle = {Instructor},
  author = {Liu, Jason and {Contributors}},
  date = {2024-03},
  origdate = {2023-06-14T10:42:23Z},
  url = {https://github.com/instructor-ai/instructor},
  urldate = {2025-03-17},
  abstract = {Instructor is the most popular Python library for working with structured outputs from large language models (LLMs), boasting over 1 million monthly downloads. Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses. Get ready to supercharge your LLM workflows with the community's top choice!},
  organization = {Instructor AI},
  version = {1}
}

@article{Livi2013GraphMatchingProblem,
  title = {The Graph Matching Problem},
  author = {Livi, Lorenzo and Rizzi, Antonello},
  date = {2013-08-01},
  journaltitle = {Pattern Analysis and Applications},
  shortjournal = {Pattern Anal Applic},
  volume = {16},
  number = {3},
  pages = {253--283},
  issn = {1433-755X},
  doi = {10.1007/s10044-012-0284-8},
  url = {https://doi.org/10.1007/s10044-012-0284-8},
  urldate = {2025-05-14},
  abstract = {In this paper, we propose a survey concerning the state of the art of the graph matching problem, conceived as the most important element in the definition of inductive inference engines in graph-based pattern recognition applications. We review both methodological and algorithmic results, focusing on inexact graph matching procedures. We consider different classes of graphs that are roughly differentiated considering the complexity of the defined labels for both vertices and edges. Emphasis will be given to the understanding of the underlying methodological aspects of each identified research branch. A selection of inexact graph matching algorithms is proposed and synthetically described, aiming at explaining some significant instances of each graph matching methodology mainly considered in the technical literature.},
  langid = {english}
}

@article{Lloyd1982LeastSquaresQuantization,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  date = {1982-03},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2\textasciicircum bquanta,b=1,2, \textbackslash cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}}
}

@inproceedings{Loffler2026HybridConstraintBasedGreedy,
  title = {A {{Hybrid Constraint-Based}}, {{Greedy}}, and~{{Local Search Approach}} for~the~{{Transshipment Problem}}},
  booktitle = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}},
  author = {Löffler, Sven and Abbenhaus, Viktoria and Assaf, George and Hofstedt, Petra},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15956},
  pages = {91--103},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6_7},
  abstract = {The efficient resolution of logistics problems, particularly those aimed at minimizing costs and reducing environmental impact, represents a critical challenge in our globalized world. A prominent example of such problems is the Transshipment Problem, which seeks to determine the most cost-effective paths from sources (e.g., producers) through transshipment points to sinks (e.g., customers). Approaches to addressing this problem range from greedy algorithms, which may rapidly yield locally optimal solutions, to constraint-based methods that, given sufficient resources and computation time, can identify globally optimal solutions. In this study, we propose a hybrid approach that integrates greedy strategies into the solution process of constraint modeling for the Transshipment Problem. This integration aims to expedite the discovery of high-quality initial solutions while preserving the global optimization capabilities inherent in constraint-based search methods. To validate the effectiveness of this new hybrid approach, we conducted an extensive series of experiments, which demonstrate its significant advantages in solving the Transshipment Problem compared to both a conventional constraint model and pure greedy methods.},
  eventtitle = {German {{Conference}} on {{Artificial Intelligence}}},
  isbn = {978-3-032-02813-6},
  langid = {english}
}

@inproceedings{Long2025WhatMakesGood,
  title = {What {{Makes}} a {{Good Natural Language Prompt}}?},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Long, Do Xuan and Dinh, Duy and Nguyen, Ngoc-Hai and Kawaguchi, Kenji and Chen, Nancy F. and Joty, Shafiq and Kan, Min-Yen},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {5835--5873},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.292/},
  urldate = {2025-07-29},
  abstract = {As large language models (LLMs) have progressed towards more human-like and human–AI communications prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying 150+ prompting-related papers from leading NLP and AI conferences (2022–2024), and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. Finally, we explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human–AI communication and opening new prompting research directions.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@unpublished{Lu2012FastProjectedFixedPoint,
  title = {A {{Fast Projected Fixed-Point Algorithm}} for {{Large Graph Matching}}},
  author = {Lu, Yao and Huang, Kaizhu and Liu, Cheng-Lin},
  date = {2012-07-03},
  eprint = {1207.1114},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1207.1114},
  urldate = {2019-01-07},
  abstract = {We propose a fast approximate algorithm for large graph matching. A new projected fixed-point method is defined and a new doubly stochastic projection is adopted to derive the algorithm. Previous graph matching algorithms suffer from high computational complexity and therefore do not have good scalability with respect to graph size. For matching two weighted graphs of \$n\$ nodes, our algorithm has time complexity only \$O(n\textasciicircum 3)\$ per iteration and space complexity \$O(n\textasciicircum 2)\$. In addition to its scalability, our algorithm is easy to implement, robust, and able to match undirected weighted attributed graphs of different sizes. While the convergence rate of previous iterative graph matching algorithms is unknown, our algorithm is theoretically guaranteed to converge at a linear rate. Extensive experiments on large synthetic and real graphs (more than 1,000 nodes) were conducted to evaluate the performance of various algorithms. Results show that in most cases our proposed algorithm achieves better performance than previous state-of-the-art algorithms in terms of both speed and accuracy in large graph matching. In particular, with high accuracy, our algorithm takes only a few seconds (in a PC) to match two graphs of 1,000 nodes.}
}

@book{Lukaszewicz1990NonmonotonicReasoningFormalization,
  title = {Non-Monotonic {{Reasoning}}: {{Formalization}} of {{Commonsense Reasoning}}},
  shorttitle = {Non-Monotonic {{Reasoning}}},
  author = {Łukaszewicz, Witold},
  date = {1990},
  eprint = {ZnZQAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {Ellis Horwood},
  isbn = {978-0-13-624446-2},
  langid = {english},
  pagetotal = {336}
}

@online{Luo2025CausalGraphsMeet,
  title = {Causal {{Graphs Meet Thoughts}}: {{Enhancing Complex Reasoning}} in {{Graph-Augmented LLMs}}},
  shorttitle = {Causal {{Graphs Meet Thoughts}}},
  author = {Luo, Hang and Zhang, Jian and Li, Chujun},
  date = {2025-03-17},
  eprint = {2501.14892},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.14892},
  url = {http://arxiv.org/abs/2501.14892},
  urldate = {2025-03-24},
  abstract = {In knowledge-intensive tasks, especially in high-stakes domains like medicine and law, it is critical not only to retrieve relevant information but also to provide causal reasoning and explainability. Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, integrating knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has emerged as an effective solution. Traditional Graph RAG methods often rely on simple graph traversal or semantic similarity, which do not capture causal relationships or align well with the model's internal reasoning steps. This paper proposes a novel pipeline that filters large knowledge graphs to emphasize cause-effect edges, aligns the retrieval process with the model's chain-of-thought (CoT), and enhances reasoning through multi-stage path improvements. Experiments on medical question-answering tasks show consistent gains, with up to a 10\textbackslash\% absolute improvement across multiple large language models (LLMs). This approach demonstrates the value of combining causal reasoning with stepwise retrieval, leading to more interpretable and logically grounded solutions for complex queries.},
  pubstate = {prepublished}
}

@inproceedings{Luong2015EffectiveApproachesAttentionbased,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
  date = {2015-09},
  pages = {1412--1421},
  publisher = {Association for Computational Linguistics},
  location = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1166},
  url = {https://aclanthology.org/D15-1166/},
  urldate = {2025-07-30},
  eventtitle = {{{EMNLP}} 2015}
}

@inproceedings{Ma2025PragmaticsEraLarge,
  title = {Pragmatics in the {{Era}} of {{Large Language Models}}: {{A Survey}} on {{Datasets}}, {{Evaluation}}, {{Opportunities}} and {{Challenges}}},
  shorttitle = {Pragmatics in the {{Era}} of {{Large Language Models}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ma, Bolei and Li, Yuting and Zhou, Wei and Gong, Ziwei and Liu, Yang Janet and Jasinskaja, Katja and Friedrich, Annemarie and Hirschberg, Julia and Kreuter, Frauke and Plank, Barbara},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {8679--8696},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.425},
  url = {https://aclanthology.org/2025.acl-long.425/},
  urldate = {2025-09-09},
  abstract = {Understanding pragmatics—the use of language in context—is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatic phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{MacQueen1967MethodsClassificationAnalysis,
  title = {Some Methods for Classification and Analysis of Multivariate Observations},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  author = {MacQueen, J.},
  date = {1967},
  volume = {1},
  pages = {281--297},
  location = {Berkeley, USA},
  url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Some-methods-for-classification-and-analysis-of-multivariate-observations/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992?tab=ChapterArticleLink},
  eventtitle = {Berkeley {{Symposium}} on {{Mathematical Statistics}} and {{Probability}}}
}

@thesis{Maggiore2022SurveySemanticSimilarity,
  type = {mathesis},
  title = {Survey on {{Semantic Similarity Measures}} for {{Argument Graphs}}},
  author = {Maggiore, Federico},
  date = {2022-09-10},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {english}
}

@article{Mailly2025PygargPythonEngine,
  title = {Pygarg: {{A Python}} Engine for Argumentation},
  shorttitle = {Pygarg},
  author = {Mailly, Jean-Guy},
  date = {2025-10-01},
  journaltitle = {Argument \& Computation},
  volume = {16},
  number = {3},
  pages = {425--430},
  publisher = {SAGE Publications},
  issn = {1946-2166},
  doi = {10.3233/AAC-230019},
  url = {https://doi.org/10.3233/AAC-230019},
  urldate = {2025-09-15},
  abstract = {Recent advancements in algorithms for abstract argumentation make it possible now to solve reasoning problems even with argumentation frameworks of large size, as demonstrated by the results of the various editions of the International Competition on Computational Models of Argumentation (ICCMA). However, the solvers participating to the competition may be hard to use for non-expert programmers, especially if they need to incorporate these algorithms in their own code instead of simply using the command-line interface. Moreover, some ICCMA solvers focus on the ICCMA tracks, and do not implement algorithms for other problems. In this paper we describe pygarg, a Python implementation of the SAT-based approach used in the argumentation solver CoQuiAAS. Contrary to CoQuiAAS and most of the participants to the various editions of ICCMA, pygarg incorporates all problems that have been considered in the main track of any edition of ICCMA. We show how to easily use pygarg via a command-line interface inspired by ICCMA competitions, and then how it can be used in other Python scripts as a third-party library.},
  langid = {english}
}

@software{Maintainers2016TorchVisionPyTorchsComputer,
  title = {{{TorchVision}}: {{PyTorch}}'s {{Computer Vision}} Library},
  shorttitle = {{{TorchVision}}},
  author = {{maintainers}, TorchVision and {contributors}},
  date = {2016-11},
  origdate = {2016-11-09T23:11:43Z},
  url = {https://github.com/pytorch/vision},
  urldate = {2025-07-15},
  abstract = {Datasets, Transforms and Models specific to Computer Vision}
}

@article{Majumder2021InterpretableSemanticTextual,
  title = {Interpretable Semantic Textual Similarity of Sentences Using Alignment of Chunks with Classification and Regression},
  author = {Majumder, Goutam and Pakray, Partha and Das, Ranjita and Pinto, David},
  date = {2021-03-08},
  journaltitle = {Applied Intelligence},
  shortjournal = {Appl Intell},
  pages = {1--28},
  publisher = {Springer US},
  issn = {1573-7497},
  doi = {10.1007/s10489-020-02144-x},
  url = {https://link.springer.com/article/10.1007/s10489-020-02144-x},
  urldate = {2021-03-13},
  abstract = {The proposed work is focused on establishing an interpretable Semantic Textual Similarity (iSTS) method for a pair of sentences, which can clarify why two sentences are completely or partially similar or have some variations. This proposed interpretable approach is a pipeline of five modules that begins with the pre-processing and chunking of text. Further chunks of two sentences are aligned using a one–to–multi (1:M) chunk aligner. Thereafter, support vector, Gaussian Naive Bayes and k–Nearest Neighbours classifiers are then used to create a multiclass classification algorithm, and different class labels are used to define an alignment type. At last, a multivariate regression algorithm is developed to find the semantic equivalence of an alignment with a score (that ranges from 0 to 5). The efficiency of the proposed method is verified on three different datasets and also compared to other state–of–the–art interpretable STS (iSTS) methods. The evaluated results show that the proposed method performs better than other iSTS methods. Most importantly, the modules of the proposed iSTS method are used to develop a Textual Entailment (TE) method. It is found that, when we combined chunk level, alignment, and sentence level features the entailment results significantly improves.},
  langid = {english}
}

@inproceedings{Malburg2021ImprovingSimilarityBasedRetrieval,
  title = {Improving {{Similarity-Based Retrieval Efficiency}} by {{Using Graphic Processing Units}} in {{Case-Based Reasoning}}},
  booktitle = {The {{International FLAIRS Conference Proceedings}}},
  author = {Malburg, Lukas and Hoffmann, Maximilian and Trumm, Simon and Bergmann, Ralph},
  date = {2021-04-18},
  volume = {34},
  location = {Florida},
  doi = {10.32473/flairs.v34i1.128345},
  url = {https://journals.flvc.org/FLAIRS/article/view/128345},
  urldate = {2024-04-01},
  abstract = {The accelerated growth of available data causes case bases of increasing sizes and thus lowers efficiency during the case retrieval phase in Case-Based Reasoning (CBR) systems. Even though, many complex and data-intensive tasks are solved by using Graphic Processing Units (GPUs), its application in CBR research has yet to advance past the early stage phase. In this paper, we present an approach to use CUDA-compatible GPUs for similarity assessment of structural, feature vector based cases. Our approach supports several syntactic and semantic similarity measures and is implemented in the open-source case-based reasoning framework ProCAKE. When comparing to current retrieval techniques that calculate similarities on the CPU, our GPU-based approach outperforms them by a factor of up to 37. In addition, our evaluation indicates that the performance gains increase with higher case complexity.},
  eventtitle = {{{FLAIRS Conference}}}
}

@inproceedings{Malburg2024ImprovingComplexAdaptations,
  title = {Improving {{Complex Adaptations}} in {{Process-Oriented Case-Based Reasoning}} by {{Applying Rule-Based Adaptation}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Malburg, Lukas and Hotz, Maxim and Bergmann, Ralph},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {50--66},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_4},
  abstract = {Adaptation is a complex and error-prone task in Case-Based Reasoning (CBR), including the adaptation knowledge acquisition and modeling efforts required for performing adaptations. This is also evident for the subfield of Process-Oriented Case-Based Reasoning (POCBR) in which cases represent procedural experiential knowledge, making creation and maintaining adaptation knowledge even for domain experts exceedingly challenging. Current adaptation methods in POCBR address the adaptation knowledge bottleneck by learning adaptation knowledge based on cases in the case base. However, these approaches are based on proprietary representation formats, resulting in low usability and maintainability. Therefore, we present an approach of using adaptation rules and rule engines for complex adaptations in POCBR in this paper. The results of an experimental evaluation indicate that the rule-based adaptation approach leads to significantly better results during runtime than an already available POCBR adaptation method.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Mangalik2025CapturingAuthorSelf,
  title = {Capturing {{Author Self Beliefs}} in {{Social Media Language}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mangalik, Siddharth and V Ganesan, Adithya and Wheeler, Abigail B. and Kerry, Nicholas and Clifton, Jeremy D. W. and Schwartz, H. and Boyd, Ryan L.},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {1362--1376},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.69/},
  urldate = {2025-08-01},
  abstract = {Measuring the prevalence and dimensions of self beliefs is essential for understanding human self-perception and various psychological outcomes. In this paper, we develop a novel task for classifying language that contains explicit or implicit mentions of the author's self beliefs. We contribute a set of 2,000 human-annotated self beliefs, 100,000 LLM-labeled examples, and 10,000 surveyed self belief paragraphs. We then evaluate several encoder-based classifiers and training routines for this task. Our trained model, SelfAwareNet, achieved an AUC of 0.944, outperforming 0.839 from OpenAI's state-of-the-art GPT-4o model. Using this model we derive data-driven categories of self beliefs and demonstrate their ability to predict valence, depression, anxiety, and stress. We release the resulting self belief classification model and annotated datasets for use in future research.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@article{Manne2012FeatureTermsBased,
  title = {A {{Feature Terms}} Based {{Method}} for {{Improving Text Summarization}} with {{Supervised POS Tagging}}},
  author = {Manne, Suneetha and Sameen Fatima, S.},
  date = {2012-07-03},
  journaltitle = {International Journal of Computer Applications},
  shortjournal = {IJCA},
  volume = {47},
  number = {23},
  pages = {7--14},
  issn = {09758887},
  doi = {10.5120/7494-0541},
  url = {http://research.ijcaonline.org/volume47/number23/pxc3880541.pdf},
  urldate = {2023-10-26}
}

@book{Manning1999FoundationsStatisticalNatural,
  title = {Foundations of {{Statistical Natural Language Processing}}},
  author = {Manning, Christopher and Schütze, Hinrich},
  date = {1999-05-28},
  publisher = {MIT Press},
  location = {Cambridge, MA, USA},
  isbn = {978-0-262-13360-9},
  langid = {english},
  pagetotal = {718}
}

@book{Manning2008IntroductionInformationRetrieval,
  title = {Introduction to Information Retrieval},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  date = {2008},
  publisher = {Cambridge University Press},
  location = {New York},
  isbn = {978-0-521-86571-5},
  pagetotal = {482},
  annotation = {OCLC: ocn190786122}
}

@article{Manning2022HumanLanguageUnderstanding,
  title = {Human {{Language Understanding}} \& {{Reasoning}}},
  author = {Manning, Christopher D.},
  date = {2022-05-01},
  journaltitle = {Daedalus},
  shortjournal = {Daedalus},
  volume = {151},
  number = {2},
  pages = {127--138},
  issn = {0011-5266},
  doi = {10.1162/daed_a_01905},
  url = {https://doi.org/10.1162/daed_a_01905},
  urldate = {2023-04-01},
  abstract = {The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.}
}

@inproceedings{Mao2023EmbeddingToEmbeddingMethodBased,
  title = {Embedding-{{To-Embedding Method Based}} on {{Autoencoder}} for {{Solving Sentence Analogies}}},
  booktitle = {Proceedings of the {{Workshops}} at the 31st {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2023)},
  author = {Mao, Weihao and Lepage, Yves},
  editor = {Malburg, Lukas and Verma, Deepika},
  date = {2023-07-17},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3438},
  pages = {15--26},
  publisher = {CEUR},
  location = {Aberdeen, Scotland},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3438/#paper_02},
  urldate = {2023-07-31},
  eventtitle = {{{ICCBR}} 2023 {{Workshop Proceedings}}},
  langid = {english}
}

@article{Marcus1993BuildingLargeAnnotated,
  title = {Building a {{Large Annotated Corpus}} of {{English}}: {{The Penn Treebank}}},
  shorttitle = {Building a {{Large Annotated Corpus}} of {{English}}},
  author = {Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  date = {1993},
  journaltitle = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {313--330},
  url = {https://www.aclweb.org/anthology/J93-2004},
  urldate = {2021-02-09}
}

@article{Marji2025EvaluatingLargeLanguage,
  title = {Evaluating Large Language Models’ Ability to Generate Interpretive Arguments},
  author = {Marji, Zaid and Licato, John},
  date = {2025-10-01},
  journaltitle = {Argument \& Computation},
  volume = {16},
  number = {3},
  pages = {362--404},
  publisher = {SAGE Publications},
  issn = {1946-2166},
  doi = {10.3233/AAC-230014},
  url = {https://doi.org/10.3233/AAC-230014},
  urldate = {2025-09-15},
  abstract = {In natural language understanding, a crucial goal is correctly interpreting open-textured phrases. In practice, disagreements over the meanings of open-textured phrases are often resolved through the generation and evaluation of interpretive arguments, arguments designed to support or attack a specific interpretation of an expression within a document. In this paper, we discuss some of our work towards the goal of automatically generating and evaluating interpretive arguments. We have curated a set of rules from the code of ethics of various professional organizations and a set of associated scenarios that are ambiguous with respect to some open-textured phrase within the rule. We collected and evaluated arguments from both human annotators and state-of-the-art generative language models in order to determine the relative quality and persuasiveness of both sets of arguments. Finally, we performed a Turing test-inspired study in order to assess whether human annotators can tell the difference between human arguments and machine-generated arguments. The results show that machine-generated arguments, when prompted a certain way, can be consistently rated as more convincing than human-generated arguments, and to the untrained eye, the machine-generated arguments can convincingly sound human-like.},
  langid = {english}
}

@inproceedings{Marjieh2022PredictingHumanSimilarity,
  title = {Predicting {{Human Similarity Judgments Using Large Language Models}}},
  author = {Marjieh, Raja and Sucholutsky, Ilia and Sumers, Theodore and Jacoby, Nori and Griffiths, Thomas L.},
  date = {2022-07-23},
  url = {https://openreview.net/forum?id=XGrOjI_xnF},
  urldate = {2025-09-17},
  abstract = {Similarity judgments provide a well-established method for accessing mental representations, with applications in psychology, neuroscience and machine learning. However, collecting similarity judgments can be prohibitively expensive for naturalistic datasets as the number of comparisons grows quadratically in the number of stimuli. We leverage recent advances in language models and online recruitment, proposing an efficient domain-general procedure for predicting human similarity judgments based on text descriptions. Crucially, the number of descriptions required grows only linearly with the number of stimuli, drastically reducing the amount of data required. We test this procedure on six datasets of naturalistic images and show that our models outperform previous approaches based on visual information.},
  eventtitle = {First {{Workshop}} on {{Pre-training}}: {{Perspectives}}, {{Pitfalls}}, and {{Paths Forward}} at {{ICML}} 2022},
  langid = {english}
}

@inproceedings{Marquer2023LessBetterEnergyBased,
  title = {Less Is {{Better}}: {{An Energy-Based Approach}} to {{Case Base Competence}}},
  shorttitle = {Less Is {{Better}}},
  booktitle = {Proceedings of the {{Workshops}} at the 31st {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2023)},
  author = {Marquer, Esteban and Badra, Fadi and Lesot, Marie-Jeanne and Couceiro, Miguel and Leake, David},
  editor = {Malburg, Lukas and Verma, Deepika},
  date = {2023-07-17},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3438},
  pages = {27--42},
  publisher = {CEUR},
  location = {Aberdeen, Scotland},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3438/#paper_03},
  urldate = {2023-07-31},
  eventtitle = {{{ICCBR}} 2023 {{Workshop Proceedings}}},
  langid = {english}
}

@inproceedings{Marro2022GraphEmbeddingsArgumentation,
  title = {Graph {{Embeddings}} for {{Argumentation Quality Assessment}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Marro, Santiago and Cabrio, Elena and Villata, Serena},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  date = {2022-12},
  pages = {4154--4164},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.findings-emnlp.306},
  url = {https://aclanthology.org/2022.findings-emnlp.306/},
  urldate = {2025-05-31},
  abstract = {Argumentation is used by people both internally, by evaluating arguments and counterarguments to make sense of a situation and take a decision, and externally, e.g., in a debate, by exchanging arguments to reach an agreement or to promote an individual position. In this context, the assessment of the quality of the arguments is of extreme importance, as it strongly influences the evaluation of the overall argumentation, impacting on the decision making process. The automatic assessment of the quality of natural language arguments is recently attracting interest in the Argument Mining field. However, the issue of automatically assessing the quality of an argumentation largely remains a challenging unsolved task. Our contribution is twofold: first, we present a novel resource of 402 student persuasive essays, where three main quality dimensions (i.e., cogency, rhetoric, and reasonableness) have been annotated, leading to 1908 arguments tagged with quality facets; second, we address this novel task of argumentation quality assessment proposing a novel neural architecture based on graph embeddings, that combines both the textual features of the natural language arguments and the overall argument graph, i.e., considering also the support and attack relations holding among the arguments. Results on the persuasive essays dataset outperform state-of-the-art and standard baselines' performance.},
  eventtitle = {Findings 2022}
}

@book{Massie2023CaseBasedReasoningResearch,
  title = {Case-{{Based Reasoning Research}} and {{Development}}: 31st {{International Conference}}, {{ICCBR}} 2023, {{Aberdeen}}, {{UK}}, {{July}} 17–20, 2023, {{Proceedings}}},
  shorttitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  editor = {Massie, Stewart and Chakraborti, Sutanu},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14141},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-40177-0},
  url = {https://link.springer.com/10.1007/978-3-031-40177-0},
  urldate = {2024-06-26},
  isbn = {978-3-031-40176-3 978-3-031-40177-0},
  langid = {english}
}

@article{Mathet2015UnifiedHolisticMethod,
  title = {The {{Unified}} and {{Holistic Method Gamma}} for {{Inter-Annotator Agreement Measure}} and {{Alignment}}},
  author = {Mathet, Yann and Widlöcher, Antoine and Métivier, Jean-Philippe},
  date = {2015-09-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {41},
  number = {3},
  pages = {437--479},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00227},
  url = {https://doi.org/10.1162/COLI_a_00227},
  urldate = {2023-07-26},
  abstract = {Agreement measures have been widely used in computational linguistics for more than 15 years to check the reliability of annotation processes. Although considerable effort has been made concerning categorization, fewer studies address unitizing, and when both paradigms are combined even fewer methods are available and discussed. The aim of this article is threefold. First, we advocate that to deal with unitizing, alignment and agreement measures should be considered as a unified process, because a relevant measure should rely on an alignment of the units from different annotators, and this alignment should be computed according to the principles of the measure. Second, we propose the new versatile measure γ, which fulfills this requirement and copes with both paradigms, and we introduce its implementation. Third, we show that this new method performs as well as, or even better than, other more specialized methods devoted to categorization or segmentation, while combining the two paradigms at the same time.}
}

@article{Mathisen2020LearningSimilarityMeasures,
  title = {Learning Similarity Measures from Data},
  author = {Mathisen, Bjørn Magnus and Aamodt, Agnar and Bach, Kerstin and Langseth, Helge},
  date = {2020-06-01},
  journaltitle = {Progress in Artificial Intelligence},
  shortjournal = {Prog Artif Intell},
  volume = {9},
  number = {2},
  pages = {129--143},
  issn = {2192-6360},
  doi = {10.1007/s13748-019-00201-2},
  url = {https://doi.org/10.1007/s13748-019-00201-2},
  urldate = {2025-07-07},
  abstract = {Defining similarity measures is a requirement for some machine learning methods. One such method is case-based reasoning (CBR) where the similarity measure is used to retrieve the stored case or a set of cases most similar to the query case. Describing a similarity measure analytically is challenging, even for domain experts working with CBR experts. However, datasets are typically gathered as part of constructing a CBR or machine learning system. These datasets are assumed to contain the features that correctly identify the solution from the problem features; thus, they may also contain the knowledge to construct or learn such a similarity measure. The main motivation for this work is to automate the construction of similarity measures using machine learning. Additionally, we would like to do this while keeping training time as low as possible. Working toward this, our objective is to investigate how to apply machine learning to effectively learn a similarity measure. Such a learned similarity measure could be used for CBR systems, but also for clustering data in semi-supervised learning, or one-shot learning tasks. Recent work has advanced toward this goal which relies on either very long training times or manually modeling parts of the similarity measure. We created a framework to help us analyze the current methods for learning similarity measures. This analysis resulted in two novel similarity measure designs: The first design uses a pre-trained classifier as basis for a similarity measure, and the second design uses as little modeling as possible while learning the similarity measure from data and keeping training time low. Both similarity measures were evaluated on 14 different datasets. The evaluation shows that using a classifier as basis for a similarity measure gives state-of-the-art performance. Finally, the evaluation shows that our fully data-driven similarity measure design outperforms state-of-the-art methods while keeping training time low.},
  langid = {english}
}

@inproceedings{Maxwell2017StudySnippetLength,
  title = {A {{Study}} of {{Snippet Length}} and {{Informativeness}}: {{Behaviour}}, {{Performance}} and {{User Experience}}},
  shorttitle = {A {{Study}} of {{Snippet Length}} and {{Informativeness}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Maxwell, David and Azzopardi, Leif and Moshfeghi, Yashar},
  date = {2017-08-07},
  series = {{{SIGIR}} '17},
  pages = {135--144},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3077136.3080824},
  url = {https://dl.acm.org/doi/10.1145/3077136.3080824},
  urldate = {2025-01-13},
  abstract = {The design and presentation of a Search Engine Results Page (SERP) has been subject to much research. With many contemporary aspects of the SERP now under scrutiny, work still remains in investigating more traditional SERP components, such as the result summary. Prior studies have examined a variety of different aspects of result summaries, but in this paper we investigate the influence of result summary length on search behaviour, performance and user experience. To this end, we designed and conducted a within-subjects experiment using the TREC AQUAINT news collection with 53 participants. Using Kullback-Leibler distance as a measure of information gain, we examined result summaries of different lengths and selected four conditions where the change in information gain was the greatest: (i) title only; (ii) title plus one snippet; (iii) title plus two snippets; and (iv) title plus four snippets. Findings show that participants broadly preferred longer result summaries, as they were perceived to be more informative. However, their performance in terms of correctly identifying relevant documents was similar across all four conditions. Furthermore, while the participants felt that longer summaries were more informative, empirical observations suggest otherwise; while participants were more likely to click on relevant items given longer summaries, they also were more likely to click on non-relevant items. This shows that longer is not necessarily better, though participants perceived that to be the case - and second, they reveal a positive relationship between the length and informativeness of summaries and their attractiveness (i.e. clickthrough rates). These findings show that there are tensions between perception and performance when designing result summaries that need to be taken into account.},
  isbn = {978-1-4503-5022-8}
}

@inproceedings{Mayer2020TransformerBasedArgumentMining,
  title = {Transformer-{{Based Argument Mining}} for {{Healthcare Applications}}},
  booktitle = {Proceedings of the 24th {{European Conference}} on {{Artificial Intelligence}}},
  author = {Mayer, Tobias and Cabrio, Elena and Villata, Serena},
  date = {2020},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {325},
  pages = {2108--2115},
  publisher = {IOS Press},
  location = {Santiago de Compostela, Spain},
  doi = {10.3233/FAIA200334},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200334},
  urldate = {2025-07-15},
  eventtitle = {{{ECAI}} 2020},
  langid = {english}
}

@inproceedings{McCarthy1968ProgramsCommonSense,
  title = {Programs with {{Common Sense}}},
  booktitle = {Semantic {{Information Processing}}},
  author = {McCarthy, John},
  date = {1968},
  pages = {403--418},
  publisher = {MIT Press},
  abstract = {This paper will discuss programs to manipulate in a suitable formal lan- guage (most likely a part of the predicate calculus) common instrumental statements. The basic program will draw immediate conclusions from a list of premises. These conclusions will be either declarative or imperative sentences. When an imperative sentence is deduced the program takes a corresponding action. These actions may include printing sentences, moving sentences on lists. and reinitiating the basic deduction process on these lists. Facilities will be provided for communication with humans in the system via manual intervention and display devices connected to the computer}
}

@inproceedings{McInnes2017AcceleratedHierarchicalDensity,
  title = {Accelerated {{Hierarchical Density Based Clustering}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {McInnes, Leland and Healy, John},
  date = {2017-11},
  pages = {33--42},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2017.12},
  abstract = {We present an accelerated algorithm for hierarchical density based clustering. Our new algorithm improves upon HDBSCAN*, which itself provided a significant qualitative improvement over the popular DBSCAN algorithm. The accelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while supporting variable density clusters, and eliminating the need for the difficult to tune distance scale parameter epsilon. This makes accelerated HDBSCAN* the default choice for density based clustering.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})}
}

@article{McInnes2017HdbscanHierarchicalDensity,
  title = {Hdbscan: {{Hierarchical}} Density Based Clustering},
  shorttitle = {Hdbscan},
  author = {McInnes, Leland and Healy, John and Astels, Steve},
  date = {2017-03-21},
  journaltitle = {The Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {2},
  number = {11},
  pages = {205},
  issn = {2475-9066},
  doi = {10.21105/joss.00205},
  url = {http://joss.theoj.org/papers/10.21105/joss.00205},
  urldate = {2022-08-02}
}

@article{McNemar1947NoteSamplingError,
  title = {Note on the Sampling Error of the Difference between Correlated Proportions or Percentages},
  author = {McNemar, Quinn},
  date = {1947-06-01},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {12},
  number = {2},
  pages = {153--157},
  issn = {1860-0980},
  doi = {10.1007/BF02295996},
  url = {https://doi.org/10.1007/BF02295996},
  urldate = {2024-04-05},
  abstract = {Two formulas are presented for judging the significance of the difference between correlated proportions. The chi square equivalent of one of the developed formulas is pointed out.},
  langid = {english}
}

@inproceedings{Meilicke2015NewParadigmAlignment,
  title = {New Paradigm for Alignment Extraction.},
  booktitle = {{{OM}}},
  author = {Meilicke, Christian and Stuckenschmidt, Heiner},
  date = {2015},
  pages = {1--12}
}

@inproceedings{Meilicke2019AnytimeBottomUpRule,
  title = {Anytime {{Bottom-Up Rule Learning}} for {{Knowledge Graph Completion}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}})},
  author = {Meilicke, Christian and Chekol, Melisachew Wudage and Ruffinelli, Daniel and Stuckenschmidt, Heiner},
  date = {2019-07},
  pages = {3137--3143},
  doi = {10.24963/ijcai.2019/435},
  url = {https://www.ijcai.org/Proceedings/2019/435},
  urldate = {2020-05-11},
  abstract = {We propose an anytime bottom-up technique for learning logical rules from large knowledge graphs. We apply the learned rules to predict candidates in the context of knowledge graph completion. Our approach outperforms other rule-based approaches and it is competitive with current state of the art, which is based on latent representations. Besides, our approach is significantly faster, requires less computational resources, and yields an explanation in terms of the rules that propose a candidate.}
}

@article{Memoli2011GromovWassersteinDistances,
  title = {Gromov–{{Wasserstein Distances}} and the {{Metric Approach}} to {{Object Matching}}},
  author = {Mémoli, Facundo},
  date = {2011-08},
  journaltitle = {Foundations of Computational Mathematics},
  volume = {11},
  number = {4},
  pages = {417--487},
  issn = {1615-3375, 1615-3383},
  doi = {10.1007/s10208-011-9093-5},
  url = {http://link.springer.com/10.1007/s10208-011-9093-5},
  urldate = {2019-01-07},
  langid = {english}
}

@online{Merriam-Webster2021Argumentation,
  title = {Argumentation},
  author = {{Merriam-Webster}},
  date = {2021-01-17},
  url = {https://www.merriam-webster.com/thesaurus/argumentation},
  urldate = {2021-01-17},
  abstract = {Argumentation: an exchange of views for the purpose of exploring a subject or deciding an issue. Synonyms: argument, argy-bargy, back-and-forth… Find the right word.},
  langid = {english},
  organization = {Merriam-Webster Thesaurus}
}

@article{Metzinger1999TeachingPhilosophyArgumentation,
  title = {Teaching {{Philosophy}} with {{Argumentation Maps}}: {{Review}} of {{Can Computers Think}}? {{The Debate}} by {{Robert E}}. {{Horn}}},
  shorttitle = {Teaching {{Philosophy}} with {{Argumentation Maps}}},
  author = {Metzinger, Thomas},
  date = {1999},
  journaltitle = {PSYCHE: An Interdisciplinary Journal of Research On Consciousness},
  volume = {5},
  publisher = {Association for the Scientific Study of Consciousness}
}

@inproceedings{Mihalcea2004TextRankBringingOrder,
  title = {{{TextRank}}: {{Bringing Order}} into {{Text}}},
  shorttitle = {{{TextRank}}},
  booktitle = {Proceedings of the 2004 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mihalcea, Rada and Tarau, Paul},
  date = {2004-07},
  pages = {404--411},
  publisher = {Association for Computational Linguistics},
  location = {Barcelona, Spain},
  url = {https://www.aclweb.org/anthology/W04-3252},
  urldate = {2020-09-08}
}

@unpublished{Mikolov2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-01-16},
  eprint = {1301.3781},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {https://arxiv.org/abs/1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}
}

@article{Miller1990WordNetOnlineLexical,
  title = {{{WordNet}}: {{An}} on-Line Lexical Database},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A. and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine},
  date = {1990},
  journaltitle = {International Journal of Lexicography},
  volume = {3},
  pages = {235--244},
  abstract = {WordNet is an on-line lexical reference system whose design is inspired by current}
}

@article{Miller1995WordNetLexicalDatabase,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  date = {1995-11-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {0001-0782},
  doi = {10.1145/219717.219748},
  url = {https://doi.org/10.1145/219717.219748},
  urldate = {2021-01-06},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].}
}

@online{Minaee2025LargeLanguageModels,
  title = {Large {{Language Models}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}}},
  author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  date = {2025-03-23},
  eprint = {2402.06196},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.06196},
  url = {http://arxiv.org/abs/2402.06196},
  urldate = {2025-09-09},
  abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \textbackslash cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
  pubstate = {prepublished}
}

@inproceedings{Minor2010CaseBasedAdaptationWorkflows,
  title = {Towards {{Case-Based Adaptation}} of {{Workflows}}},
  booktitle = {Case-{{Based Reasoning}}. {{Research}} and {{Development}}},
  author = {Minor, Mirjam and Bergmann, Ralph and Görg, Sebastian and Walter, Kirstin},
  editor = {Bichindaritz, Isabelle and Montani, Stefania},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {421--435},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14274-1_31},
  abstract = {Creation and adaptation of workflows is a difficult and costly task that is currently performed by human workflow modeling experts. Our paper describes a new approach for the automatic adaptation of workflows, which makes use of a case base of former workflow adaptations. We propose a general framework for case-based adaptation of workflows and then focus on novel methods to represent and reuse previous adaptation episodes for workflows. An empirical evaluation demonstrates the feasibility of the approach and provides valuable insights for future research.},
  isbn = {978-3-642-14274-1},
  langid = {english}
}

@article{Minor2014CasebasedAdaptationWorkflows,
  title = {Case-Based Adaptation of Workflows},
  author = {Minor, Mirjam and Bergmann, Ralph and Görg, Sebastian},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {142--152},
  issn = {0306-4379},
  doi = {10.1016/j.is.2012.11.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437912001536},
  urldate = {2025-05-13},
  abstract = {This paper presents on a Case-based Reasoning approach for automated workflow adaptation by reuse of experience. Agile workflow technology allows structural adaptations of workflow instances at build time or at run time. The approach supports the expert in performing such adaptations by an automated method. The method employs workflow adaptation cases that record adaptation episodes from the past. The recorded changes can be automatically transferred to a new workflow that is in a similar situation of change. First, the notion of workflow adaptation cases is introduced. The sample workflow modeling language CFCN is presented, which has been developed by the University of Trier as a part of the agile workflow management system Cake. Then, the retrieval of adaptation cases is briefly discussed. The case-based adaptation method is explained including the so-called anchor mapping algorithm which identifies the parts of the target workflow where to apply the changes. A formative evaluation in two application domains compares different variants of the anchor mapping algorithm by means of experts assessing the results of the automated adaptation.}
}

@article{Minor2014ProcessorientedCasebasedReasoning,
  title = {Process-Oriented Case-Based Reasoning},
  author = {Minor, Mirjam and Montani, Stefania and Recio-García, Juan A.},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {103--105},
  issn = {0306-4379},
  doi = {10.1016/j.is.2013.06.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437913000835},
  urldate = {2025-05-13}
}

@inproceedings{Minor2016TransferabilityProcessOrientedCases,
  title = {On the {{Transferability}} of {{Process-Oriented Cases}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Minor, Mirjam and Bergmann, Ralph and Müller, Jan-Martin and Spät, Alexander},
  editor = {Goel, Ashok and Díaz-Agudo, M Belén and Roth-Berghofer, Thomas},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {281--294},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-47096-2_19},
  abstract = {This paper studies the feasibility of using transfer learning for process-oriented case-based reasoning. The work introduces a novel approach to transfer workflow cases from a loosely related source domain to a target domain. The idea is to develop a representation mapper based on workflow generalization, workflow abstraction, and structural analogy between the domain vocabularies. The approach is illustrated by a pair of sample domains in two sub-fields of customer relationship management that have similar process objectives but different tasks and data to fulfill them. An experiment with expert ratings of transferred cases is conducted to test the feasibility of the approach with promising results for workflow modeling support.},
  isbn = {978-3-319-47096-2},
  langid = {english}
}

@inproceedings{Minor2024RetrievalAugmentedGeneration,
  title = {Retrieval {{Augmented Generation}} with~{{LLMs}} for~{{Explaining Business Process Models}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Minor, Mirjam and Kaucher, Eduard},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {175--190},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_12},
  abstract = {Large language models (LLMs) and retrieval augmented generation (RAG) are undergoing rapid development. Considering a case base as a memory in a RAG system provides novel opportunities for text generation. In this paper, we investigate the role Case-Based Reasoning (CBR) could play for supporting RAG systems in generating accessible explanations of business process models. We experiment with two different case bases in a RAG system. Case base a) is dedicated to support prompt chaining by reusing index knowledge on the cases with the aim to deal with large process models that do not fit into the context window size of a recent LLM. Second, case base b) contains model-text pairs to serve as in-context examples to enhance prompt templates. Approach b) aims to improve the quality of generated text explanations for process models of normal size. Our contribution opens a novel application area for process-oriented CBR. Further, our case-based RAG system provides a contemporary alternative to traditional Natural Language Processing pipelines. The experimental results contribute to gain some insights on an inherent capability threshold of GPT-4 at which the performance decreases much earlier than having reached the given context window size, on the number of retrieved cases a recent RAG system should use as in-context examples, and on suitable prompt templates.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Mir2019EvaluatingStyleTransfer,
  title = {Evaluating {{Style Transfer}} for {{Text}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Mir, Remi and Felbo, Bjarke and Obradovich, Nick and Rahwan, Iyad},
  date = {2019-06},
  pages = {495--504},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1049},
  url = {https://www.aclweb.org/anthology/N19-1049},
  urldate = {2021-05-24},
  abstract = {Research in the area of style transfer for text is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment: direction-corrected Earth Mover's Distance, Word Mover's Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined models exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.},
  eventtitle = {{{NAACL-HLT}} 2019}
}

@inproceedings{Mirzakhmedova2024AreLargeLanguage,
  title = {Are {{Large Language Models Reliable Argument Quality Annotators}}?},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Mirzakhmedova, Nailia and Gohsen, Marcel and Chang, Chia Hao and Stein, Benno},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {129--146},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_8},
  abstract = {Evaluating the quality of arguments is a crucial aspect of any system leveraging argument mining. However, it is a challenge to obtain reliable and consistent annotations regarding argument quality, as this usually requires domain-specific expertise of the annotators. Even among experts, the assessment of argument quality is often inconsistent due to the inherent subjectivity of this task. In this paper, we study the potential of using state-of-the-art large language models (LLMs) as proxies for argument quality annotators. To assess the capability of LLMs in this regard, we analyze the agreement between model, human expert, and human novice annotators based on an established taxonomy of argument quality dimensions. Our findings highlight that LLMs can produce consistent annotations, with a moderately high agreement with human experts across most of the quality dimensions. Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators. These results suggest that LLMs can serve as a valuable tool for automated argument quality assessment, thus streamlining and accelerating the evaluation of large argument datasets.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@article{Modgil2014ASPICFrameworkStructured,
  title = {The {{ASPIC}}+ Framework for Structured Argumentation: A Tutorial},
  shorttitle = {The {{ASPIC}}+ Framework for Structured Argumentation},
  author = {Modgil, Sanjay and Prakken, Henry},
  date = {2014-01-02},
  journaltitle = {Argument \& Computation},
  volume = {5},
  number = {1},
  pages = {31--62},
  publisher = {SAGE Publications},
  issn = {1946-2166},
  doi = {10.1080/19462166.2013.869766},
  url = {https://doi.org/10.1080/19462166.2013.869766},
  urldate = {2025-09-09},
  abstract = {This article gives a tutorial introduction to the ASPIC+ framework for structured argumentation. The philosophical and conceptual underpinnings of ASPIC+ are discussed, the main definitions are illustrated with examples and several ways are discussed to instantiate the framework and to reconstruct other approaches as special cases of the framework. The ASPIC+ framework is based on two ideas: the first is that conflicts between arguments are often resolved with explicit preferences, and the second is that arguments are built with two kinds of inference rules: strict, or deductive rules, whose premises guarantee their conclusion, and defeasible rules, whose premises only create a presumption in favour of their conclusion. Accordingly, arguments can in ASPIC+ be attacked in three ways: on their uncertain premises, or on their defeasible inferences, or on the conclusions of their defeasible inferences. ASPIC+ is not a system but a framework for specifying systems. A main objective of the study of the ASPIC+ framework is to identify conditions under which instantiations of the framework satisfy logical consistency and closure properties.},
  langid = {english}
}

@online{Molnar2020LongitudinalEvaluationOpenSource,
  title = {Longitudinal {{Evaluation}} of {{Open-Source Software Maintainability}}},
  author = {Molnar, Arthur-Jozsef and Motogna, Simona},
  date = {2020-03-01},
  eprint = {2003.00447},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.00447},
  url = {http://arxiv.org/abs/2003.00447},
  urldate = {2023-10-05},
  abstract = {We present a longitudinal study on the long-term evolution of maintainability in open-source software. Quality assessment remains at the forefront of both software research and practice, with many models and assessment methodologies proposed and used over time. Some of them helped create and shape standards such as ISO 9126 and 25010, which are well established today. Both describe software quality in terms of characteristics such as reliability, security or maintainability. An important body of research exists linking these characteristics with software metrics, and proposing ways to automate quality assessment by aggregating software metric values into higher-level quality models. We employ the Maintainability Index, technical debt ratio and a maintainability model based on the ARiSA Compendium. Our study covers the entire 18 year development history and all released versions for three complex, open-source applications. We determine the maintainability for each version using the proposed models, we compare obtained results and use manual source code examination to put them into context. We examine the common development patterns of the target applications and study the relation between refactoring and maintainability. Finally, we study the strengths and weaknesses of each maintainability model using manual source code examination as the baseline.},
  pubstate = {prepublished}
}

@online{Molnar2020StudyMaintainabilityEvolving,
  title = {A {{Study}} of {{Maintainability}} in {{Evolving Open-Source Software}}},
  author = {Molnar, Arthur-Jozsef and Motogna, Simona},
  date = {2020-09-02},
  eprint = {2009.00959},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.00959},
  url = {http://arxiv.org/abs/2009.00959},
  urldate = {2023-10-05},
  abstract = {Our study is focused on an evaluation of the maintainability characteristic in the context of the long-term evolution of open-source software. According to well established software quality models such as the ISO 9126 and the more recent ISO 25010, maintainability remains among key quality characteristics alongside performance, security and reliability. To achieve our objective, we selected three complex, widely used target applications for which access to their entire development history and source code was available. To enable cross-application comparison, we restricted our selection to GUI-driven software developed on the Java platform. We focused our examination on released versions, resulting in 111 software releases included in our case study. These covered more than 10 years of development for each of the applications. For each version, we determined its maintainability using three distinct quantitative models of varying complexity. We examined the relation between software size and maintainability and studied the main drivers of important changes to software maintainability. We contextualized our findings using manual source code examination. We also carried out a finer grained evaluation at package level to determine the distribution of maintainability issues within application source code. Finally, we provided a cross-application analysis in order to identify common as well as application-specific patterns.},
  pubstate = {prepublished}
}

@article{Montani2014RetrievalClusteringSupporting,
  title = {Retrieval and Clustering for Supporting Business Process Adjustment and Analysis},
  author = {Montani, Stefania and Leonardi, Giorgio},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {128--141},
  issn = {0306-4379},
  doi = {10.1016/j.is.2012.11.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437912001482},
  urldate = {2025-05-13},
  abstract = {In this paper, we describe a framework able to support run-time adjustment and a posteriori analysis of business processes, which exploits the retrieval step of the Case-based Reasoning (CBR) methodology. In particular, our framework allows to retrieve traces of process execution similar to the current one. Moreover, it supports an automatic organization of the trace database content through the application of hierarchical clustering techniques. Results can provide help both to end users, in the process execution phase, and to process engineers, in (formal) process conformance evaluation and long term process schema redesign. Retrieval and clustering rely on a distance definition able to take into account temporal information in traces. This metric has outperformed simpler distance definitions in our experiments, which were conducted in a real-world application domain.}
}

@software{Montani2023SpaCyIndustrialstrengthNatural,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} ({{NLP}}) in {{Python}}},
  author = {Montani, Ines and Honnibal, Matthew and Boyd, Adriane and Landeghem, Sofie Van and Peters, Henning and McCann, Paul O'Leary and {geovedi}, jim and O'Regan, Jim and Samsonov, Maxim and family=Kok, given=Daniël, prefix=de, useprefix=true and Orosz, György and Blättermann, Marcus and Kannan, Madeesh and Altinok, Duygu and Mitsch, Raphael and Kristiansen, Søren Lind and {Edward} and Miranda, Lj and Baumgartner, Peter and Bournhonesque, Raphaël and Hudson, Richard and Bot, Explosion and {Roman} and Fiedler, Leander and Daniels, Ryn and {kadarakos} and Phatthiyaphaibun, Wannaphong and {Schero1994}},
  date = {2023-10},
  url = {https://doi.org/10.5281/zenodo.8398458},
  organization = {Zenodo},
  version = {v3.7.0}
}

@inproceedings{Moore1982RoleLogicKnowledge,
  title = {The Role of Logic in Knowledge Representation and Commonsense Reasoning},
  booktitle = {Proceedings of the {{Second AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Moore, Robert C.},
  date = {1982-08-18},
  series = {{{AAAI}}'82},
  pages = {428--433},
  publisher = {AAAI Press},
  location = {Pittsburgh, Pennsylvania},
  abstract = {This paper examines the role that formal logic ought to play in representing and reasoning with commonsense knowledge. We take issue with the commonly held view (as expressed by Newell [1980]) that the use of representations based on formal logic is inappropriate in most applications of artificial intelligence. We argue to the contrary that there is an important set of issues, involving incomplete knowledge of a problem situation, that so far have been addressed only by systems based on formal logic and deductive inference, and that, in some sense, probably can be dealt with only by systems based on logic and deduction. We further argue that the experiments of the late 1960s on problem-solving by theorem-proving did not show that the use of logic and deduction in AI systems was necessarily inefficient, but rather that what was needed was better control of the deduction process, combined with more attention to the computational properties of axioms.}
}

@unpublished{Mrksic2017SemanticSpecialisationDistributional,
  title = {Semantic {{Specialisation}} of {{Distributional Word Vector Spaces}} Using {{Monolingual}} and {{Cross-Lingual Constraints}}},
  author = {Mrkšić, Nikola and Vulić, Ivan and Séaghdha, Diarmuid Ó and Leviant, Ira and Reichart, Roi and Gašić, Milica and Korhonen, Anna and Young, Steve},
  date = {2017-06-01},
  eprint = {1706.00374},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {We present Attract-Repel, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of constraints from mono- and cross-lingual resources, yielding semantically specialised cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages. We next show that Attract-Repel-specialised vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our algorithm facilitate the training of multilingual DST models, which brings further performance improvements.}
}

@inproceedings{Muller2014WorkflowStreamsMeans,
  title = {Workflow {{Streams}}: {{A Means}} for {{Compositional Adaptation}} in {{Process-Oriented CBR}}},
  shorttitle = {Workflow {{Streams}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Müller, Gilbert and Bergmann, Ralph},
  editor = {Lamontagne, Luc and Plaza, Enric},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {315--329},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11209-1_23},
  abstract = {This paper presents a novel approach to compositional adaptation of workflows, thus addressing the adaptation step in processoriented case-based reasoning. Unlike previous approaches to adaptation, the proposed approach does not require additional adaptation knowledge. Instead, the available case base of workflows is analyzed and each case is decomposed into meaningful subcomponents, called workflow streams. During adaptation, deficiencies in the retrieved case are incrementally compensated by replacing fragments of the retrieved case by appropriate workflow streams. An empirical evaluation in the domain of cooking workflows demonstrates the feasibility of the approach and shows that the quality of adapted cases is very close to the quality of the original cases in the case base.},
  isbn = {978-3-319-11209-1},
  langid = {english}
}

@inproceedings{Muller2015GeneralizationWorkflowsProcessOriented,
  title = {Generalization of {{Workflows}} in {{Process-Oriented Case-Based Reasoning}}},
  booktitle = {The {{Twenty-Eighth International Flairs Conference}}},
  author = {Müller, Gilbert and Bergmann, Ralph},
  date = {2015-04-06},
  url = {https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS15/paper/view/10369},
  urldate = {2020-09-25},
  abstract = {In this paper, we introduce the concept of generalized cases into process-oriented case-based reasoning. We present the formal foundations for the generalization of workflow cases as well as a new algorithm for generalizing semantic workflows, guided by ontological knowledge of the domain. Further, the specialization of workflows w.r.t. a current query is addressed. An experimental evaluation demonstrates the capability of the approach for workflow adaptation showing that the adapted workflows have a similar quality compared to that of original workflows. Furthermore, the retrieval performance can be improved by a reduction of the case-base size while the coverage of cases is significantly increased.},
  eventtitle = {The {{Twenty-Eighth International Flairs Conference}}},
  langid = {english}
}

@inproceedings{Muller2015LearningApplyingAdaptation,
  title = {Learning and {{Applying Adaptation Operators}} in {{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Müller, Gilbert and Bergmann, Ralph},
  editor = {Hüllermeier, Eyke and Minor, Mirjam},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {259--274},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-24586-7_18},
  abstract = {This paper presents a novel approach to the operator-based adaptation of workflows, which is a specific type of transformational adaptation. We introduce the notion of workflow adaptation operators which are partial functions transforming a workflow into a successor workflow, specified by workflow fractions to be inserted and/or deleted. The adaptation process itself chains adaptation operators during a local search process aiming at fulfilling the query as best as possible. Further, the paper presents an algorithm that learns workflow adaptation operators from the case base automatically, thereby addressing the common problem of adaptation knowledge acquisition. An empirical evaluation in the domain of cooking workflows was conducted which demonstrates convincing adaptation capabilities without a significant reduction of the workflows’ quality.},
  isbn = {978-3-319-24586-7},
  langid = {english}
}

@article{Mumbaikar2013WebServicesBased,
  title = {Web Services Based on {{SOAP}} and {{REST}} Principles},
  author = {Mumbaikar, Snehal and Padiya, Puja},
  date = {2013-03},
  journaltitle = {International Journal of Scientific and Research Publications},
  volume = {3},
  number = {5},
  issn = {2250-3153},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.414.7936&rep=rep1&type=pdf},
  abstract = {Interest in Web services is rapidly increased from their start of use. To exchange information among the application in standard way is the main goal of web services. This communication between the applications is based on SOAP and REST principle. SOAP communications causes network traffic, higher latency and processing delays. To overcome this limitations the REST'ful architecture is used. REST is a lightweight, easy and better alternative for the SOAP. In this paper comparison on performance of SOAP based and REST'ful web services based on different metric for mobile environment and multimedia conference is taken into consideration.}
}

@article{Munkres1957AlgorithmsAssignmentTransportation,
  title = {Algorithms for the {{Assignment}} and {{Transportation Problems}}},
  author = {Munkres, James},
  date = {1957},
  journaltitle = {Journal of the Society for Industrial and Applied Mathematics},
  volume = {5},
  number = {1},
  eprint = {2098689},
  eprinttype = {jstor},
  pages = {32--38},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0368-4245},
  url = {https://www.jstor.org/stable/2098689},
  urldate = {2025-06-24}
}

@inproceedings{Nanhekhan2025FlashCheckExplorationEfficient,
  title = {{{FlashCheck}}: {{Exploration}} of~{{Efficient Evidence Retrieval}} for~{{Fast Fact-Checking}}},
  shorttitle = {{{FlashCheck}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Nanhekhan, Kevin and Venktesh, V. and Martin, Erik and Vatndal, Henrik and Setty, Vinay and Anand, Avishek},
  editor = {Hauff, Claudia and Macdonald, Craig and Jannach, Dietmar and Kazai, Gabriella and Nardini, Franco Maria and Pinelli, Fabio and Silvestri, Fabrizio and Tonellotto, Nicola},
  date = {2025},
  pages = {385--399},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-88717-8_28},
  abstract = {The advances in digital tools have led to the rampant spread of misinformation. While fact-checking aims to combat this, manual fact-checking is cumbersome and not scalable. It is essential for automated fact-checking to be efficient for aiding in combating misinformation in real-time and at the source. Fact-checking pipelines primarily comprise a knowledge retrieval component which extracts relevant knowledge to fact-check a claim from large knowledge sources like Wikipedia and a verification component. The existing works primarily focus on the fact-verification part rather than evidence retrieval from large data collections, which often face scalability issues for practical applications such as live fact-checking. In this study, we address this gap by exploring various methods for indexing a succinct set of factual statements from large collections like Wikipedia to enhance the retrieval phase of the fact-checking pipeline. We also explore the impact of vector quantization to further improve the efficiency of pipelines that employ dense retrieval approaches for first-stage retrieval.We study the efficiency and effectiveness of the approaches on fact-checking datasets such as HoVer and WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the real-world utility of the efficient retrieval approaches by fact-checking 2024 presidential debate and also open source the collection of claims with corresponding labels identified in the debate. Through a combination of indexed facts together with Dense retrieval and Index compression, we achieve up to a 10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the classical fact-checking pipelines over large collections.},
  isbn = {978-3-031-88717-8},
  langid = {english}
}

@online{Nassar2025SmolDoclingUltracompactVisionlanguage,
  title = {{{SmolDocling}}: {{An}} Ultra-Compact Vision-Language Model for End-to-End Multi-Modal Document Conversion},
  shorttitle = {{{SmolDocling}}},
  author = {Nassar, Ahmed and Marafioti, Andres and Omenetti, Matteo and Lysak, Maksym and Livathinos, Nikolaos and Auer, Christoph and Morin, Lucas and family=Lima, given=Rafael Teixeira, prefix=de, useprefix=false and Kim, Yusik and Gurbuz, A. Said and Dolfi, Michele and Farré, Miquel and Staar, Peter W. J.},
  date = {2025-03-14},
  eprint = {2503.11576},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.11576},
  url = {http://arxiv.org/abs/2503.11576},
  urldate = {2025-07-29},
  abstract = {We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.},
  pubstate = {prepublished}
}

@incollection{Nesetril2012BoundedHeightTrees,
  title = {Bounded {{Height Trees}} and {{Tree-Depth}}},
  booktitle = {Sparsity: {{Graphs}}, {{Structures}}, and {{Algorithms}}},
  author = {Nešetřil, Jaroslav and family=Mendez, given=Patrice Ossona, prefix=de, useprefix=true},
  editor = {Nešetřil, Jaroslav and Ossona de Mendez, Patrice},
  date = {2012},
  series = {Algorithms and {{Combinatorics}}},
  pages = {115--144},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-27875-4_6},
  url = {https://doi.org/10.1007/978-3-642-27875-4_6},
  urldate = {2022-08-02},
  abstract = {After treating graph classes and class resolutions we return to the basics: the structure of finite trees as the true measure of our things.},
  isbn = {978-3-642-27875-4},
  langid = {english}
}

@inproceedings{Neuhaus2006FastSuboptimalAlgorithms,
  title = {Fast {{Suboptimal Algorithms}} for the {{Computation}} of {{Graph Edit Distance}}},
  booktitle = {Structural, {{Syntactic}}, and {{Statistical Pattern Recognition}}},
  author = {Neuhaus, Michel and Riesen, Kaspar and Bunke, Horst},
  editor = {Yeung, Dit-Yan and Kwok, James T. and Fred, Ana and Roli, Fabio and family=Ridder, given=Dick, prefix=de, useprefix=true},
  date = {2006},
  pages = {163--172},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11815921_17},
  abstract = {Graph edit distance is one of the most flexible mechanisms for error-tolerant graph matching. Its key advantage is that edit distance is applicable to unconstrained attributed graphs and can be tailored to a wide variety of applications by means of specific edit cost functions. Its computational complexity, however, is exponential in the number of vertices, which means that edit distance is feasible for small graphs only. In this paper, we propose two simple, but effective modifications of a standard edit distance algorithm that allow us to suboptimally compute edit distance in a faster way. In experiments on real data, we demonstrate the resulting speedup and show that classification accuracy is mostly not affected. The suboptimality of our methods mainly results in larger inter-class distances, while intra-class distances remain low, which makes the proposed methods very well applicable to distance-based graph classification.},
  isbn = {978-3-540-37241-7},
  langid = {english}
}

@book{Newman2015BuildingMicroservicesDesigning,
  title = {Building {{Microservices}}: {{Designing Fine-Grained Systems}}},
  shorttitle = {Building {{Microservices}}},
  author = {Newman, Sam},
  date = {2015-02-02},
  eprint = {jjl4BgAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {O'Reilly Media},
  abstract = {Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You’ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.Discover how microservices allow you to align your system design with your organization’s goalsLearn options for integrating a service with the rest of your systemTake an incremental approach when splitting monolithic codebasesDeploy individual microservices through continuous integrationExamine the complexities of testing and monitoring distributed servicesManage security with user-to-service and service-to-service modelsUnderstand the challenges of scaling microservice architectures},
  isbn = {978-1-4919-5033-3},
  langid = {english},
  pagetotal = {280}
}

@inproceedings{Nguyen2018ArgumentMiningImproving,
  title = {Argument {{Mining}} for {{Improving}} the {{Automated Scoring}} of {{Persuasive Essays}}},
  booktitle = {Thirty-{{Second AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Nguyen, Huy V. and Litman, Diane J.},
  date = {2018-04-26},
  url = {https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16447},
  urldate = {2020-06-04},
  abstract = {End-to-end argument mining has enabled the development of new automated essay scoring (AES) systems that use argumentative features (e.g., number of claims, number of support relations) in addition to traditional legacy features (e.g., grammar, discourse structure) when scoring persuasive essays. While prior research has proposed different argumentative features as well as empirically demonstrated their utility for AES, these studies have all had important limitations.  In this paper we identify a set of desiderata for evaluating the use of argument mining for AES, introduce an end-to-end argument mining system and associated argumentative feature sets, and present the results of several studies that both satisfy the desiderata and demonstrate the value-added of argument mining for scoring persuasive essays.},
  eventtitle = {Thirty-{{Second AAAI Conference}} on {{Artificial Intelligence}}},
  langid = {english}
}

@online{NickBabich2020TipsImproveDiscoverability,
  title = {Tips to {{Improve Discoverability}} in {{UX}}},
  author = {{Nick Babich}},
  date = {2020-04-14},
  url = {https://xd.adobe.com/ideas/process/information-architecture/tips-to-improve-discoverability-in-ux/},
  urldate = {2022-05-03},
  abstract = {Help users find important features when interacting with your product with these simple but effective techniques. Read more at Adobe XD Ideas.},
  langid = {american},
  organization = {Adobe XD Ideas}
}

@book{Nielsen1994UsabilityEngineering,
  title = {Usability {{Engineering}}},
  author = {Nielsen, Jakob},
  date = {1994},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  abstract = {Written by the author of the best-selling HyperText \& HyperMedia, this book is an excellent guide to the methods of usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality. Step-by-step information on which method to use at various stages during the development lifecycle are included, along with detailed information on how to run a usability test and the unique issues relating to international usability. * Emphasizes cost-effective methods that developers can implement immediately * Instructs readers about which methods to use when, throughout the development lifecycle, which ultimately helps in cost-benefit analysis. * Shows readers how to avoid the four most frequently listed reasons for delay in software projects. * Includes detailed information on how to run a usability test. * Covers unique issues of international usability. * Features an extensive bibliography allowing readers to find additional information. * Written by an internationally renowned expert in the field and the author of the best-selling HyperText \& HyperMedia. Table of Contents Executive Summary. What is Usability Generations of User Interfaces. The Usability Engineering Lifecycle. Usability Heuristics. Usability Testing. Usability Assessment Methods Beyond Testing. Interface Standards. International User Interfaces. Future Developments. Appendix A: Exercises. Appendix B: Bibliography. Author Index. Subject Index.},
  isbn = {978-0-08-052029-2},
  pagetotal = {362}
}

@inproceedings{Nikishina2024ExtendingComparativeArgumentative,
  title = {Extending the~{{Comparative Argumentative Machine}}: {{Multilingualism}} and~{{Stance Detection}}},
  shorttitle = {Extending the~{{Comparative Argumentative Machine}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Nikishina, Irina and Bondarenko, Alexander and Zaczek, Sebastian and Haag, Onno Lander and Hagen, Matthias and Biemann, Chris},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {317--334},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_19},
  abstract = {The comparative argumentative machine~CAM can retrieve arguments that answer comparative questions—questions that ask which of several to-be-compared options should be favored in some scenario. In this paper, we describe how we equipped~CAM with a better answer stance detection (i.e., a better detection of which option “wins” a comparison) and with system variants to support non-English requests. As for the improved answer stance detection, we develop RoBERTa-based approaches and experimentally show them to be more effective than previous feature-based and LLM-based stance detectors. As for the multilingualism, in a proof of concept, we compare two approaches to support Russian requests and answers: (1)~translating the original English CAM~data and (2)~using an existing replica of CAM on native Russian data. Comparing the translation-based and the replica-based CAM~variants in a user study shows that combining their answers seems to be the most promising. For individual questions, the retrieved arguments of the two variants are often different and of quite diverse relevance and quality. As a demonstrator, we deploy a first multilingual CAM~version that combines translation-based and replica-based outputs for English and Russian and that can easily be extended to further languages.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Nikishina2025HowCompareThings,
  title = {How to {{Compare Things Properly}}? {{A Study}} of {{Argument Relevance}} in {{Comparative Question Answering}}},
  shorttitle = {How to {{Compare Things Properly}}?},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Nikishina, Irina and Anwar, Saba and Dolgov, Nikolay and Manina, Maria and Ignatenko, Daria and Shelmanov, Artem and Biemann, Chris},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {15702--15720},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.765/},
  urldate = {2025-07-29},
  abstract = {Comparative Question Answering (CQA) lies at the intersection of Question Answering, Argument Mining, and Summarization. It poses unique challenges due to the inherently subjective nature of many questions and the need to integrate diverse perspectives. Although the CQA task can be addressed using recently emerged instruction-following Large Language Models (LLMs), challenges such as hallucinations in their outputs and the lack of transparent argument provenance remain significant limitations.To address these challenges, we construct a manually curated dataset comprising arguments annotated with their relevance. These arguments are further used to answer comparative questions, enabling precise traceability and faithfulness. Furthermore, we define explicit criteria for an “ideal” comparison and introduce a benchmark for evaluating the outputs of various Retrieval-Augmented Generation (RAG) models with respect to argument relevance. All code and data are publicly released to support further research.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Nilles2021QuARkGUIQualityAware,
  title = {{{QuARk}}: {{A GUI}} for {{Quality-Aware Ranking}} of {{Arguments}}},
  shorttitle = {{{QuARk}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Nilles, Markus and Dumani, Lorik and Schenkel, Ralf},
  date = {2021-07-11},
  series = {{{SIGIR}} '21},
  pages = {2546--2549},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3404835.3462795},
  url = {https://doi.org/10.1145/3404835.3462795},
  urldate = {2022-09-26},
  abstract = {With the Web augmenting every day and computers increasingly getting more powerful, research in the field of computational argumentation becomes more and more important. One of its research branches is argument retrieval, which aims at finding and presenting users the best arguments for their queries. Several systems already exist for this purpose, all having the same goal but reaching it in different ways. In line with existing work, an argument consists of a claim supported or attacked by a premise. Now that argument retrieval has become a separate task in the CLEF lab Touché, displaying the ranking is becoming increasingly important. In this paper we present QuARk, a GUI that allows users to retrieve arguments from a focused debate collection for their queries. Since we strictly distinguished between frontend and backend and kept the communication between them simple, QuARk can be extended to integrate various argument retrieval systems, assuming some modifications are made. In order to demonstrate the GUI, we show the integration of a complex retrieval algorithm that we also presented in the CLEF lab Touché. Our retrieval process consists of two parts. In the first step, it finds the most similar claims to the query. Therefore, the user can select between different standard IR similarity methods. The second step ranks the premises directly related to the claims. Therefore, the user can choose to rank the arguments either by quantitative, qualitative, or a combined measure.},
  isbn = {978-1-4503-8037-9}
}

@inproceedings{Nilles2023TrustMeAm,
  title = {Trust Me, {{I}} Am an {{Expert}}: {{Predicting}} the {{Credibility}} of {{Experts}} for {{Statements}}},
  shorttitle = {Trust Me, {{I}} Am an {{Expert}}},
  booktitle = {Proceedings of the {{Workshops}} at the 31st {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2023)},
  author = {Nilles, Markus and Dumani, Lorik and Metzler, Björn and Schenkel, Ralf},
  editor = {Malburg, Lukas and Verma, Deepika},
  date = {2023-07-17},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3438},
  pages = {114--128},
  publisher = {CEUR},
  location = {Aberdeen, Scotland},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3438/#paper_09},
  urldate = {2023-07-31},
  abstract = {Nowadays, information on any topic can be researched on the Internet. However, in addition to reputable news sources, there is also a great deal of fake news that is disseminated, e.g., via social media or in established newspapers. Thus, the veracity must be assessed for each piece of information. People, parties, and organizations want to push through their interests and sometimes do not hesitate to spread fake news. For some time now, one popular means has been to quote (supposed) experts in a field. For example, —due to his authority— Albert Einstein is often quoted by believers in God although he was primarily concerned with physics while his quotes on God are taken out of context. In this paper, we define a new task of expert suitability prediction and evaluate methods to assess the credibility of a person with reference to a statement and its context and compare it to state-of-the-art approaches applying transformer-based embeddings. In an R4 cycle in CBR this approach could be used for the ranking. In this pilot study, we restrict our experiments to researchers, which allows us to derive their expertise from their publications. Furthermore, we make a manually labeled dataset consisting of 1,700 (statement,expert) pairs where suitable experts were tediously searched out together with valuable context information (such as convincing text parts of the experts’ contexts towards a statement) publicly available to stimulate further research in this very important, but up to now underrepresented area of fake news detection.},
  eventtitle = {{{ICCBR}} 2023 {{Workshop Proceedings}}},
  langid = {english}
}

@inproceedings{Nkisi-Orji2020CloodCBRMicroservices,
  title = {Clood {{CBR}}: {{Towards Microservices Oriented Case-Based Reasoning}}},
  shorttitle = {Clood {{CBR}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Nkisi-Orji, Ikechukwu and Wiratunga, Nirmalie and Palihawadana, Chamath and Recio-García, Juan A. and Corsar, David},
  editor = {Watson, Ian and Weber, Rosina},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {129--143},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58342-2_9},
  abstract = {CBR applications have been deployed in a wide range of sectors, from pharmaceuticals; to defence and aerospace to IoT and transportation, to poetry and music generation; for example. However, a majority of these have been built using monolithic architectures which impose size and complexity constraints. As such these applications have a barrier to adopting new technologies and remain prohibitively expensive in both time and cost because changes in frameworks or languages affect the application directly. To address this challenge, we introduce a distributed and highly scalable generic CBR system, Clood, which is based on a microservices architecture. This splits the application into a set of smaller, interconnected services that scale to meet varying demands. Experimental results show that our Clood implementation retrieves cases at a fairly consistent rate as the casebase grows by several orders of magnitude and was over 3,700 times faster than a comparable monolithic CBR system when retrieving from half a million cases. Microservices are cloud-native architectures and with the rapid increase in cloud-computing adoption, it is timely for the CBR community to have access to such a framework.},
  isbn = {978-3-030-58342-2},
  langid = {english}
}

@article{OKeefe1977TwoConceptsArgument,
  title = {Two {{Concepts}} of {{Argument}}},
  author = {O'Keefe, Daniel J.},
  date = {1977-03-30},
  journaltitle = {The Journal of the American Forensic Association},
  volume = {13},
  number = {3},
  pages = {121--128},
  publisher = {Routledge},
  issn = {0002-8533},
  doi = {10.1080/00028533.1977.11951098},
  url = {https://doi.org/10.1080/00028533.1977.11951098},
  urldate = {2025-09-08},
  abstract = {Students of argumentation rarely acknowledge that the term “argument” has two importantly different senses. This essay attempts to show the importance of distinguishing these senses, taking as a focus for analysis Wayne Brockriede's recent discussions of the concept of argument. It is argued that Brackriede's view suffers from a failure to heed the distinction noted, and that this failure signals important developments in the study of argument.}
}

@unpublished{Ollinger2020SameSideStance,
  title = {Same {{Side Stance Classification Task}}: {{Facilitating Argument Stance Classification}} by {{Fine-tuning}} a {{BERT Model}}},
  shorttitle = {Same {{Side Stance Classification Task}}},
  author = {Ollinger, Stefan and Dumani, Lorik and Sahitaj, Premtim and Bergmann, Ralph and Schenkel, Ralf},
  date = {2020-04-23},
  eprint = {2004.11163},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.11163},
  urldate = {2020-04-27},
  abstract = {Research on computational argumentation is currently being intensively investigated. The goal of this community is to find the best pro and con arguments for a user given topic either to form an opinion for oneself, or to persuade others to adopt a certain standpoint. While existing argument mining methods can find appropriate arguments for a topic, a correct classification into pro and con is not yet reliable. The same side stance classification task provides a dataset of argument pairs classified by whether or not both arguments share the same stance and does not need to distinguish between topic-specific pro and con vocabulary but only the argument similarity within a stance needs to be assessed. The results of our contribution to the task are build on a setup based on the BERT architecture. We fine-tuned a pre-trained BERT model for three epochs and used the first 512 tokens of each argument to predict if two arguments share the same stance.}
}

@inproceedings{Olmos2006InexactGraphMatching,
  title = {Inexact {{Graph Matching}}: {{A Case}} of {{Study}}},
  booktitle = {Proceedings of the {{Nineteenth International Florida Artificial Intelligence Research Society Conference}}},
  author = {Olmos, Ivan and Gonzalez, Jesus A. and Osorio, Mauricio},
  date = {2006},
  pages = {586--591},
  publisher = {AAAI Press},
  location = {Florida},
  url = {https://aaai.org/papers/flairs-2006-115/},
  abstract = {Inexact graph matching has become an important research area because it is used to find similarities among objects in several real domains such as chemical and biological compounds. Let G and G′ be input labeled graphs, we present an algorithm capable to find a graph S of G, where S is isomorphic to G′ and the corresponding labels between the vertices and edges of S and G′ are not the same (inexact matching). We use a listcode based representation without candidate generation, where a step by step expansion is implemented. The proposed approach is suitable to work with directed and undirected graphs. We conducted a set of experiments in a genome database in order to show the effectiveness of our algorithm. Our experiments show a promissing method to be used with scalable graph matching tools that can be applied to areas such as Machine Learning (ML) and Data Mining (DM).},
  eventtitle = {{{FLAIRS}}},
  langid = {english}
}

@online{OpenAI2024GPT4TechnicalReport,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  date = {2024-03-04},
  eprint = {2303.08774},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08774},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2024-06-24},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  pubstate = {prepublished}
}

@inproceedings{Opitz2019DissectingContentContext,
  title = {Dissecting {{Content}} and {{Context}} in {{Argumentative Relation Analysis}}},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Argument Mining}}},
  author = {Opitz, Juri and Frank, Anette},
  date = {2019-08},
  pages = {25--34},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/W19-4503},
  url = {https://www.aclweb.org/anthology/W19-4503},
  urldate = {2020-05-24},
  abstract = {When assessing relations between argumentative units (e.g., support or attack), computational systems often exploit disclosing indicators or markers that are not part of elementary argumentative units (EAUs) themselves, but are gained from their context (position in paragraph, preceding tokens, etc.). We show that this dependency is much stronger than previously assumed. In fact, we show that by completely masking the EAU text spans and only feeding information from their context, a competitive system may function even better. We argue that an argument analysis system that relies more on discourse context than the argument's content is unsafe, since it can easily be tricked. To alleviate this issue, we separate argumentative units from their context such that the system is forced to model and rely on an EAU's content. We show that the resulting classification system is more robust, and argue that such models are better suited for predicting argumentative relations across documents.}
}

@inproceedings{Opitz2021ExplainableUnsupervisedArgument,
  title = {Explainable {{Unsupervised Argument Similarity Rating}} with {{Abstract Meaning Representation}} and {{Conclusion Generation}}},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Argument Mining}}},
  author = {Opitz, Juri and Heinisch, Philipp and Wiesenbach, Philipp and Cimiano, Philipp and Frank, Anette},
  editor = {Al-Khatib, Khalid and Hou, Yufang and Stede, Manfred},
  date = {2021-11},
  pages = {24--35},
  publisher = {Association for Computational Linguistics},
  location = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.argmining-1.3},
  url = {https://aclanthology.org/2021.argmining-1.3/},
  urldate = {2025-05-15},
  abstract = {When assessing the similarity of arguments, researchers typically use approaches that do not provide interpretable evidence or justifications for their ratings. Hence, the features that determine argument similarity remain elusive. We address this issue by introducing novel argument similarity metrics that aim at high performance and explainability. We show that Abstract Meaning Representation (AMR) graphs can be useful for representing arguments, and that novel AMR graph metrics can offer explanations for argument similarity ratings. We start from the hypothesis that similar premises often lead to similar conclusions—and extend an approach for AMR-based argument similarity rating by estimating, in addition, the similarity of conclusions that we automatically infer from the arguments used as premises. We show that AMR similarity metrics make argument similarity judgements more interpretable and may even support argument quality judgements. Our approach provides significant performance improvements over strong baselines in a fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations.},
  eventtitle = {{{ArgMining}} 2021}
}

@inproceedings{Oshikawa2020SurveyNaturalLanguage,
  title = {A {{Survey}} on {{Natural Language Processing}} for {{Fake News Detection}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Oshikawa, Ray and Qian, Jing and Wang, William Yang},
  editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  date = {2020-05},
  pages = {6086--6093},
  publisher = {European Language Resources Association},
  location = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.747/},
  urldate = {2025-07-15},
  abstract = {Fake news detection is a critical yet challenging problem in Natural Language Processing (NLP). The rapid rise of social networking platforms has not only yielded a vast increase in information accessibility but has also accelerated the spread of fake news. Thus, the effect of fake news has been growing, sometimes extending to the offline world and threatening public safety. Given the massive amount of Web content, automatic fake news detection is a practical NLP problem useful to all online content providers, in order to reduce the human time and effort to detect and prevent the spread of fake news. In this paper, we describe the challenges involved in fake news detection and also describe related tasks. We systematically review and compare the task formulations, datasets and NLP solutions that have been developed for this task, and also discuss the potentials and limitations of them. Based on our insights, we outline promising research directions, including more fine-grained, detailed, fair, and practical detection models. We also highlight the difference between fake news detection and other related tasks, and the importance of NLP solutions for fake news detection.},
  eventtitle = {{{LREC}} 2020},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@inproceedings{Ottersen2024AutomaticAdjustingGlobal,
  title = {Automatic {{Adjusting Global Similarity Measures}} in~{{Learning CBR Systems}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Ottersen, Stuart G. and Bach, Kerstin},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {17--32},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_2},
  abstract = {This paper explores how learning case-based reasoning (CBR) systems are affected by updating similarity measures. We create CBR systems using the local-global principle and we investigate (1) how adding new cases changes the CBR system’s performance and (2) how this drift can be mitigated through updating the similarity measure, especially adapting feature weights for weighted sums. We aim to provide transparent measures to show when the knowledge containers drift apart to indicate when an update is necessary. We, therefore, explore the effect feature weight has on predictive performance and the knowledge containers in online learning CBR systems. Following this, we present a method to minimize updating feature weights while the case base grows while maintaining performance. The performance is compared to two baselines: never updating and always updating. Our experiments with public datasets show that a smart updating strategy catches the drifting of case base content and similarity measures well.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@article{Ouertatani2021ParsingArguedOpinion,
  title = {Parsing Argued Opinion Structure in {{Twitter}} Content},
  author = {Ouertatani, Asma and Gasmi, Ghada and Latiri, Chiraz},
  date = {2021-04-01},
  journaltitle = {Journal of Intelligent Information Systems},
  shortjournal = {J Intell Inf Syst},
  volume = {56},
  number = {2},
  pages = {327--353},
  issn = {1573-7675},
  doi = {10.1007/s10844-020-00620-x},
  url = {https://doi.org/10.1007/s10844-020-00620-x},
  urldate = {2023-10-20},
  abstract = {In this paper, we address the opinion argumentation mining issue from Twitter data with the objective of further analyzing Twitter users’ preferences and motivations. After introducing the argued opinion definition and its different elements, we propose an argued opinion mining system called TOMAS where we present an end-to-end approach to parse the structure of the argued opinion in order to identify its elements. Our suggested system consists of four consecutive sub-tasks, namely: (1) opinion-topic detection, (2) argumentative opinions identification, (3) argument components detection, and (4) argumentative relation recognition. The proposed system optimizes the argued opinion structure using different classification models. The experimental study is conducted on the MC2 Lab CLEF2017 tweets corpus while considering various comparative baselines. We highlight that our system significantly outperforms the majority baselines and significantly outperforms challenging existing approaches.},
  langid = {english}
}

@report{Page1999PageRankCitationRanking,
  type = {Technical Report},
  title = {The {{PageRank Citation Ranking}}: {{Bringing Order}} to the {{Web}}.},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  date = {1999-11},
  number = {1999--66},
  institution = {Stanford InfoLab / Stanford InfoLab},
  url = {http://ilpubs.stanford.edu:8090/422/},
  abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.}
}

@article{Pan2024UnifyingLargeLanguage,
  title = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}: {{A Roadmap}}},
  shorttitle = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}},
  author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  date = {2024-07},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  volume = {36},
  number = {7},
  pages = {3580--3599},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2024.3352100},
  url = {https://ieeexplore.ieee.org/document/10387715/},
  urldate = {2024-09-16}
}

@inproceedings{Parsodkar2025NavigatingLandscapeCase,
  title = {Navigating the~{{Landscape}} of~{{Case Fidelity}} and~{{Competence}} in~{{Case-Based Reasoning}}},
  booktitle = {Artificial {{Intelligence XLI}}},
  author = {Parsodkar, Adwait P. and P., Deepak and Chakraborti, Sutanu},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2025},
  pages = {235--249},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77915-2_17},
  abstract = {In this paper, we survey measures aimed at quantifying the intrinsic quality of cases’ contents, which we collectively refer to as case fidelity, and measures that capture their competence within the Case-Based Reasoning literature. We discuss how insights from the Truth Discovery and Item Response Theory literature can respectively inform advancements in estimating case fidelity and competence. Additionally, we highlight novel research directions that emerge from a deeper examination of case fidelity and competence.},
  isbn = {978-3-031-77915-2},
  langid = {english}
}

@inproceedings{Passonneau2006MeasuringAgreementSetvalued,
  title = {Measuring {{Agreement}} on {{Set-valued Items}} ({{MASI}}) for {{Semantic}} and {{Pragmatic Annotation}}},
  booktitle = {Proceedings of the {{Fifth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'06)},
  author = {Passonneau, Rebecca},
  date = {2006-05},
  publisher = {European Language Resources Association (ELRA)},
  location = {Genoa, Italy},
  url = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/636_pdf.pdf},
  urldate = {2021-03-07},
  abstract = {Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to.},
  eventtitle = {{{LREC}} 2006}
}

@inproceedings{Paul2020ArgumentativeRelationClassification,
  title = {Argumentative {{Relation Classification}} with {{Background Knowledge}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Computational Models}} of {{Argument}}},
  author = {Paul, Debjit and Opitz, Juri and Becker, Maria and Kobbe, Jonathan and Hirst, Graeme and Frank, Anette},
  editor = {Prakken, Henry and Bistarelli, Stefano and Santini, Francesco and Taticchi, Carlo},
  date = {2020},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {326},
  pages = {319--330},
  publisher = {IOS Press},
  location = {Perugia, Italy},
  doi = {10.3233/FAIA200515},
  abstract = {A common conception is that the understanding of relations that hold between argument units requires knowledge beyond the text. But to date, argument analysis systems that leverage knowledge resources are still very rare. In this paper, we propose an unsupervised graph-based ranking method that extracts relevant multi-hop knowledge from a background knowledge resource. This knowledge is integrated into a neural argumentative relation classifier via an attention-based gating mechanism. In contrast to prior work we emphasize the selection of relevant multi-hop knowledge, and apply methods to automatically enrich the knowledge resource with missing knowledge. We assess model performance on two datasets, showing considerable improvement over strong baselines.},
  eventtitle = {Computational {{Models}} of {{Argument}}}
}

@online{Paun2020DesigningEfficiencyHow,
  title = {Designing {{With Efficiency}}: {{How Familiarity Can Enhance Experiences}}},
  author = {Paun, Goran},
  date = {2020-10-02},
  url = {https://www.forbes.com/sites/forbesagencycouncil/2020/10/02/designing-with-efficiency-how-familiarity-can-enhance-experiences/},
  urldate = {2022-05-03},
  abstract = {Our society is continually inspired by innovation. As creatives, it can consequently be tempting to shift from established design conventions in pursuit of originality.},
  langid = {english},
  organization = {Forbes}
}

@article{Pedronette2013ImageRerankingRank,
  title = {Image Re-Ranking and Rank Aggregation Based on Similarity of Ranked Lists},
  author = {Pedronette, Daniel Carlos Guimarães and family=S. Torres, given=Ricardo, prefix=da, useprefix=true},
  date = {2013-08-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {46},
  number = {8},
  pages = {2350--2360},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2013.01.004},
  url = {https://www.sciencedirect.com/science/article/pii/S003132031300023X},
  urldate = {2025-07-15},
  abstract = {In Content-based Image Retrieval (CBIR) systems, ranking accurately collection images is of great relevance. Users are interested in the returned images placed at the first positions, which usually are the most relevant ones. Collection images are ranked in increasing order of their distance to the query pattern (e.g., query image) defined by users. Therefore, the effectiveness of these systems is very dependent on the accuracy of the distance function adopted. In this paper, we present a novel context-based approach for redefining distances and later re-ranking images aiming to improve the effectiveness of CBIR systems. In our approach, distances among images are redefined based on the similarity of their ranked lists. Conducted experiments involving shape, color, and texture descriptors demonstrate the effectiveness of our method.}
}

@article{Peldszus2013ArgumentDiagramsArgumentation,
  title = {From {{Argument Diagrams}} to {{Argumentation Mining}} in {{Texts}} - {{A Survey}}.},
  author = {Peldszus, Andreas and Stede, Manfred},
  date = {2013-01-01},
  journaltitle = {IJCINI},
  volume = {7},
  number = {1},
  pages = {1--31},
  doi = {10.4018/jcini.2013010101},
  url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/jcini.2013010101},
  urldate = {2018-09-01}
}

@inproceedings{Peldszus2016AnnotatedCorpusArgumentative,
  title = {An {{Annotated Corpus}} of {{Argumentative Microtexts}}},
  booktitle = {Argumentation and {{Reasoned Action}}: {{Proceedings}} of the 1st {{European Conference}} on {{Argumentation}}},
  author = {Peldszus, Andreas and Stede, Manfred},
  date = {2016},
  volume = {2},
  pages = {801--816},
  publisher = {College Publications},
  location = {Lisbon, Portugal},
  abstract = {We present a freely available corpus of argumentative "microtexts", featuring short and dense authentic arguments, annotated according to a scheme for representing text-level argumentation structure. The corpus consists of 112 German texts plus professional English translations that preserve linearization and argumentative structure. We provide statistics of the variety and the linguistic realization of argumentation structure in the corpus. We hope the data release serves the needs of data-driven approaches to argument mining and qualitative analysis alike.}
}

@inproceedings{Pennington2014GloveGlobalVectors,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014-10},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  url = {http://www.aclweb.org/anthology/D14-1162},
  urldate = {2018-09-04}
}

@inproceedings{Perozzi2014DeepWalkOnlineLearning,
  title = {{{DeepWalk}}: Online Learning of Social Representations},
  shorttitle = {{{DeepWalk}}},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
  date = {2014-08-24},
  series = {{{KDD}} '14},
  pages = {701--710},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2623330.2623732},
  url = {https://dl.acm.org/doi/10.1145/2623330.2623732},
  urldate = {2025-05-15},
  abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
  isbn = {978-1-4503-2956-9}
}

@inproceedings{Persiani2024FantasticArgumentationTools,
  title = {Fantastic {{Argumentation Tools And Where To Find Them}}},
  booktitle = {Proceedings of the {{Fifth International Workshop}} on {{Systems}} and {{Algorithms}} for {{Formal Argumentation}}},
  author = {Persiani, Michele and Guerrero, Esteban and Brännström, Andreas and Kilic, Kaan and Kampik, Timotheus},
  editor = {Borg, AnneMarie and Ellmauthaler, Stefan and Mailly, Jean-Guy and Niskanen, Andreas},
  date = {2024-09},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3757},
  pages = {56--68},
  publisher = {CEUR},
  location = {Hagen, Germany},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3757/#paper4},
  urldate = {2025-04-12},
  eventtitle = {Systems and {{Algorithms}} for {{Formal Argumentation}} 2024},
  langid = {english}
}

@inproceedings{Persing2010ModelingOrganizationStudent,
  title = {Modeling {{Organization}} in {{Student Essays}}},
  booktitle = {Proceedings of the 2010 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Persing, Isaac and Davis, Alan and Ng, Vincent},
  date = {2010-10},
  pages = {229--239},
  publisher = {Association for Computational Linguistics},
  location = {Cambridge, MA},
  url = {https://www.aclweb.org/anthology/D10-1023},
  urldate = {2019-11-18},
  eventtitle = {{{EMNLP}} 2010}
}

@inproceedings{Persing2015ModelingArgumentStrength,
  title = {Modeling {{Argument Strength}} in {{Student Essays}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Persing, Isaac and Ng, Vincent},
  date = {2015-07},
  pages = {543--552},
  publisher = {Association for Computational Linguistics},
  location = {Beijing, China},
  doi = {10.3115/v1/P15-1053},
  url = {https://www.aclweb.org/anthology/P15-1053},
  urldate = {2019-11-18},
  eventtitle = {{{ACL-IJCNLP}} 2015}
}

@inproceedings{Petrov2012UniversalPartofSpeechTagset,
  title = {A {{Universal Part-of-Speech Tagset}}},
  booktitle = {Proceedings of the {{Eighth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'12)},
  author = {Petrov, Slav and Das, Dipanjan and McDonald, Ryan},
  date = {2012-05},
  pages = {2089--2096},
  publisher = {European Language Resources Association (ELRA)},
  location = {Istanbul, Turkey},
  url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf},
  urldate = {2021-02-09},
  abstract = {To facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices, we propose a tagset that consists of twelve universal part-of-speech categories. In addition to the tagset, we develop a mapping from 25 different treebank tagsets to this universal set. As a result, when combined with the original treebank data, this universal tagset and mapping produce a dataset consisting of common parts-of-speech for 22 different languages. We highlight the use of this resource via three experiments, that (1) compare tagging accuracies across languages, (2) present an unsupervised grammar induction approach that does not use gold standard part-of-speech tags, and (3) use the universal tags to transfer dependency parsers between languages, achieving state-of-the-art results.},
  eventtitle = {{{LREC}} 2012}
}

@thesis{Pfister2017SimilaritybasedRetrievalNatural,
  type = {mathesis},
  title = {Similarity-Based {{Retrieval}} of {{Natural Language Argumentation Graphs}}},
  author = {Pfister, Maximiliam},
  date = {2017-06-08},
  institution = {University of Trier},
  abstract = {The retrieval process is usually the first phase in case-based reasoning (CBR) ap- plications and thus represents the basis for subsequent CBR phases such as reuse, revision and retainment. Key tasks to enable the automatic retrieval of cases being similar to a query include the definition of case representations, similarity measures as well as similarity computation and retrieval algorithms suitable for a application domain. In this thesis a concept for the similarity-based retrieval of natural language argumentation graphs is presented, with the overall goal to support users during the deliberation and synthesis of arguments. In particular, graph-based case representa- tions, similarity assessment concepts and retrieval approaches from process-oriented case-based reasoning (POCBR) are transferred to argumentation graphs. A lo- cal similarity measure based on word embeddings is introduced which allows for a similarity assessment independent of the domain of the topic of cases and queries. Furthermore, a two-staged retrieval model is presented which increases both the accuracy and the execution time of the retrieval.},
  pagetotal = {85}
}

@inproceedings{Pfister2025LLaMmleinTransparentCompact,
  title = {{{LLäMmlein}}: {{Transparent}}, {{Compact}} and {{Competitive German-Only Language Models}} from {{Scratch}}},
  shorttitle = {{{LLäMmlein}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pfister, Jan and Wunderle, Julia and Hotho, Andreas},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {2227--2246},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.111/},
  urldate = {2025-07-29},
  abstract = {We transparently create two German-only decoder models, LLäMmlein 120M and 1B, from scratch and publish them, along with the training data, for the (German) NLP research community to use. The model training involved several key steps, including data preprocessing/filtering, the creation of a German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks, also against existing models. Throughout the training process, multiple checkpoints were saved in equal intervals and analyzed using the German SuperGLEBer benchmark to gain insights into the models' learning process.Compared to state-of-the-art models on the SuperGLEBer benchmark, both LLäMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early during training, offering valuable insights into resource allocation for future models.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@book{Pilehvar2021EmbeddingsNaturalLanguage,
  title = {Embeddings in {{Natural Language Processing}}},
  author = {Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  date = {2021},
  series = {Synthesis {{Lectures}} on {{Human Language Technologies}}},
  edition = {1},
  publisher = {Springer International Publishing},
  url = {https://link.springer.com/book/10.1007/978-3-031-02177-0},
  urldate = {2022-08-05},
  abstract = {Embeddings have undoubtedly been one of the most influential research areas in Natural Language Processing (NLP). Encoding information into a low-dimensional vector representation, which is easily integrable in modern machine learning models, has played a central role in the development of NLP. Embedding techniques initially focused on words, but the attention soon started to shift to other forms: from graph structures, such as knowledge bases, to other types of textual content, such as sentences and documents. This book provides a high-level synthesis of the main embedding techniques in NLP, in the broad sense. The book starts by explaining conventional word vector space models and word embeddings (e.g., Word2Vec and GloVe) and then moves to other types of embeddings, such as word sense, sentence and document, and graph embeddings. The book also provides an overview of recent developments in contextualized representations (e.g., ELMo and BERT) and explains their potential in NLP. Throughout the book, the reader can find both essential information for understanding a certain topic from scratch and a broad overview of the most successful techniques developed in the literature.},
  isbn = {978-3-031-02177-0},
  langid = {english},
  pagetotal = {157}
}

@article{Pirro2019BuildingRelatednessExplanations,
  title = {Building Relatedness Explanations from Knowledge Graphs},
  author = {Pirrò, Giuseppe},
  date = {2019-01-01},
  journaltitle = {Semantic Web},
  volume = {10},
  number = {6},
  pages = {963--990},
  publisher = {IOS Press},
  issn = {1570-0844},
  doi = {10.3233/SW-190348},
  url = {https://content.iospress.com/articles/semantic-web/sw190348},
  urldate = {2020-08-03},
  abstract = {Knowledge graphs (KGs) are a key ingredient to complement search results, discover entities and their relations and support several knowledge discovery tasks. We face the problem of building relatedness explanations, that is, graphs that can explain},
  langid = {english}
}

@inproceedings{Plenz2024PAKTPerspectivizedArgumentation,
  title = {{{PAKT}}: {{Perspectivized Argumentation Knowledge Graph}} and~{{Tool}} for~{{Deliberation Analysis}}},
  shorttitle = {{{PAKT}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Plenz, Moritz and Heinisch, Philipp and Frank, Anette and Cimiano, Philipp},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {89--107},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_6},
  abstract = {Deliberative processes play a vital role in shaping opinions, decisions and policies in our society. In contrast to persuasive debates, deliberation aims to foster understanding of conflicting perspectives among interested parties. The exchange of arguments in deliberation serves to elucidate viewpoints, to raise awareness of conflicting interests, and to finally converge on a resolution. To better understand and analyze the underlying processes of deliberation, we propose PAKT, a Perspectivized Argumentation Knowledge Graph and Tool. The graph structures the argumentative space across diverse topics, where arguments i) are divided into premises and conclusions, ii) are annotated for stances, framings and their underlying values and iii) are connected to background knowledge. We show how to construct PAKT and conduct case studies on the obtained multifaceted argumentation graph. Our findings show the analytical potential offered by our framework, highlighting the capability to go beyond individual arguments and to reveal structural patterns in the way participants and stakeholders argue in a debate. The overarching goal of our work is to facilitate constructive discourse and informed decision making as a special form of argumentation. We offer public access to PAKT and its rich capabilities to support analytics, visualization, navigation and efficient search, for diverse forms of argumentation (GitHub: www.github.com/Heidelberg-NLP/PAKTWebsite: www.webtentacle1.techfak.uni-bielefeld.de/accept/).},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Plisson2004RuleBasedApproach,
  title = {A Rule Based Approach to Word Lemmatization},
  booktitle = {Proceedings of {{IS}}},
  author = {Plisson, Joël and Lavrac, Nada and Mladenic, Dunja},
  date = {2004},
  volume = {3},
  pages = {83--86}
}

@inproceedings{Poiaganova2025DebatesDiplomacyArgument,
  title = {From {{Debates}} to {{Diplomacy}}: {{Argument Mining Across Political Registers}}},
  shorttitle = {From {{Debates}} to {{Diplomacy}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Poiaganova, Maria and Stede, Manfred},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {205--216},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.20/},
  urldate = {2025-07-28},
  abstract = {This paper addresses the problem of cross-register generalization in argument mining within political discourse. We examine whether models trained on adversarial, spontaneous U.S. presidential debates can generalize to the more diplomatic and prepared register of UN Security Council (UNSC) speeches. To this end, we conduct a comprehensive evaluation across four core argument mining tasks. Our experiments show that the tasks of detecting and classifying argumentative units transfer well across registers, while identifying and labeling argumentative relations remains notably challenging, likely due to register-specific differences in how argumentative relations are structured and expressed. As part of this work, we introduce ArgUNSC, a new corpus of 144 UNSC speeches manually annotated with claims, premises, and their argumentative links. It provides a resource for future in- and cross-domain studies and novel research directions at the intersection of argument mining and political science.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Pojoni2023ArgumentMiningPodcastsUsing,
  title = {Argument-{{Mining}} from {{Podcasts Using ChatGPT}}},
  booktitle = {Proceedings of the {{Workshops}} at the 31st {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2023)},
  author = {Pojoni, Mircea-Luchian and Dumani, Lorik and Schenkel, Ralf},
  editor = {Malburg, Lukas and Verma, Deepika},
  date = {2023-07-17},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3438},
  pages = {129--144},
  publisher = {CEUR},
  location = {Aberdeen, Scotland},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3438/#paper_10},
  urldate = {2023-07-31},
  abstract = {Podcasts have emerged as a significant platform for the exchange of ideas, opinions, and knowledge on a variety of topics. At the same time, the extraction of arguments (called: argument mining) has received great attention. However, to the best of our knowledge, there exist no work that investigates the extraction of arguments from podcasts. One reason can be that podcasts often involve unpredictable and complex argument structures, and extracting valuable insights from them is challenging. In this work, we present the novel approach of extracting two different types of argumentative structures from podcast after transcribing them, i.e., (1) a simple but often used variant describing arguments as consisting of only a claim and a premise, where the claim describes the standpoint and the premise the reason to support or attack that claim and (2) an extended variant where an argument comprises premises, a main claim, counterarguments, and rebuttals. For this purpose, we utilize two specially designed prompts and OpenAI’s GPT-4 language model. For our test data, we chose three podcasts considering current computational constraints and the need for diversity in topics and discussion styles. Our evaluation shows the high feasibility of extracting arguments from podcasts using ChatGPT. We publish the podcasts’ transcripts as well as the extracted arguments.},
  eventtitle = {{{ICCBR}} 2023 {{Workshop Proceedings}}},
  langid = {english}
}

@inproceedings{Poole1995NovelAlgorithmMatching,
  title = {A Novel Algorithm for Matching Conceptual and Related Graphs},
  booktitle = {Conceptual {{Structures}}: {{Applications}}, {{Implementation}} and {{Theory}}},
  author = {Poole, Jonathan and Campbell, J. A.},
  editor = {Ellis, Gerard and Levinson, Robert and Rich, William and Sowa, John F.},
  date = {1995},
  pages = {293--307},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-60161-9_45},
  abstract = {This paper presents a new similarity metric and algorithm for situations represented as graphs. The metric is based on the concept of shared information, and there is discussion of how this would apply for different forms of similarity—including surface, structural and thematic similarity. An algorithm is presented which will determine the similarity of two conceptual graphs for any given measure of information content, which can, as a result, be used for any similarity measure that is based on the concept of shared information. It therefore allows the very flexible use of domain and application specific factors. While the algorithm is not polynomial time, it is argued that for real examples of a useful size it can give an answer in a reasonable time.},
  isbn = {978-3-540-49539-0},
  langid = {english}
}

@article{Popel2018TrainingTipsTransformer,
  title = {Training {{Tips}} for the {{Transformer Model}}},
  author = {Popel, Martin and Bojar, Ondřej},
  date = {2018-04-01},
  journaltitle = {The Prague Bulletin of Mathematical Linguistics},
  volume = {110},
  number = {1},
  pages = {43--70},
  issn = {1804-0462},
  doi = {10.2478/pralin-2018-0002},
  url = {http://content.sciendo.com/view/journals/pralin/110/1/article-p43.xml},
  urldate = {2022-01-28},
  abstract = {Abstract             This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra “more data and larger models”, we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.}
}

@inproceedings{Popic2016PerformanceEvaluationUsing,
  title = {Performance Evaluation of Using {{Protocol Buffers}} in the {{Internet}} of {{Things}} Communication},
  booktitle = {2016 {{International Conference}} on {{Smart Systems}} and {{Technologies}} ({{SST}})},
  author = {Popić, Srđan and Pezer, Dražen and Mrazovac, Bojan and Teslić, Nikola},
  date = {2016-10},
  pages = {261--265},
  doi = {10.1109/SST.2016.7765670},
  abstract = {Things connected to the internet may not be connected only to servers on the cloud. Its usefulness can be unleashed only if they are interconnected as well. This interconnection is recognized as communication between services of the Internet of Things. Due to the nature of the things on the web, spatial overhead for any data exchanged needs to be kept to a minimum. JSON is recognized as a most efficient way to transfer various data in domain of distributed embedded systems. JSON's binary representation, BSON is even more preferable. This paper explores the possibilities and critically examines and evaluates the effectiveness of using Google's Protocol Buffer as a processing protocol and communication standard in transportation domain of the Internet of Things.},
  eventtitle = {2016 {{International Conference}} on {{Smart Systems}} and {{Technologies}} ({{SST}})}
}

@misc{Porter2001SnowballLanguageStemming,
  title = {Snowball: {{A}} Language for Stemming Algorithms},
  author = {Porter, Michael},
  date = {2001-01-01},
  url = {http://snowball.tartarus.org/texts/introduction.html},
  urldate = {2018-09-01},
  abstract = {Algorithmic stemmers continue to have great utility in IR, despite the promise of out- performance by dictionary-based stemmers. Nevertheless, there are few algorithmic descriptions of stemmers, and even when they exist they are liable to misinterpretation. Here we look at the ideas underlying stemming, and on this website define a language, Snowball, in which stemmers can be exactly defined, and from which fast stemmer programs in ANSI C or Java can be generated. A range of stemmers is presented in parallel algorithmic and~…}
}

@inproceedings{Portinale2024IntegratingKNNRetrieval,
  title = {Integrating {{kNN Retrieval}} with~{{Inference}} on~{{Graphical Models}} in~{{Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Portinale, Luigi},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {1--16},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_1},
  abstract = {In Case-Based Reasoning, when the similarity assumption does not hold, knowledge about the adaptability of solutions has to be exploited, in order to retrieve cases with adaptable solutions. We propose a novel approach to address this issue, where kNN retrieval is integrated with inference on a metric Markov Random Field (MRF). Nodes of the MRF represent cases and edges connect nodes whose solutions are close in the solution space. States of the nodes represent different adaptation levels with respect to the potential query. Metric-based potentials enforce connected nodes to share the same state, since cases having similar solutions should share the same adaptability effort with respect to the query. The goal is to enlarge the set of potentially adaptable cases that are retrieved, by controlling precision and accuracy of retrieval. We experiment on a retrieval architecture where a simple kNN retrieval (on the problem description) is followed by a further retrieval step based on MRF inference, and we discuss the promising results we have obtained in two different setting: using manually-engineered adaptation rules and adopting an automatic learning strategies for such rules.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@incollection{Portoraro2019AutomatedReasoning,
  title = {Automated {{Reasoning}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Portoraro, Frederic},
  editor = {Zalta, Edward N.},
  date = {2019},
  edition = {Spring 2019},
  publisher = {Metaphysics Research Lab, Stanford University},
  url = {https://plato.stanford.edu/archives/spr2019/entries/reasoning-automated/},
  urldate = {2021-01-31},
  abstract = {Reasoning is the ability to make inferences, and automated reasoningis concerned with the building of computing systems that automate thisprocess. Although the overall goal is to mechanize different forms ofreasoning, the term has largely been identified with valid deductivereasoning as practiced in mathematics and formal logic. In thisrespect, automated reasoning is akin to mechanical theorem proving.Building an automated reasoning program means providing an algorithmicdescription to a formal calculus so that it can be implemented on acomputer to prove theorems of the calculus in an efficient manner.Important aspects of this exercise involve defining the class ofproblems the program will be required to solve, deciding what languagewill be used by the program to represent the information given to itas well as new information inferred by the program, specifying themechanism that the program will use to conduct deductive inferences,and figuring out how to perform all these computations efficiently.While basic research work continues in order to provide the necessarytheoretical framework, the field has reached a point where automatedreasoning programs are being used by researchers to attack openquestions in mathematics and logic, provide important applications incomputing science, solve problems in engineering, and find novelapproaches to questions in exact philosophy.}
}

@inproceedings{Potthast2019ArgumentSearchAssessing,
  title = {Argument {{Search}}: {{Assessing Argument Relevance}}},
  shorttitle = {Argument {{Search}}},
  booktitle = {Proceedings of the {{42Nd International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Potthast, Martin and Gienapp, Lukas and Euchner, Florian and Heilenkötter, Nick and Weidmann, Nico and Wachsmuth, Henning and Stein, Benno and Hagen, Matthias},
  date = {2019},
  series = {{{SIGIR}}'19},
  pages = {1117--1120},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/3331184.3331327},
  url = {http://doi.acm.org/10.1145/3331184.3331327},
  urldate = {2019-09-04},
  abstract = {We report on the first user study on assessing argument relevance. Based on a search among more than 300,000 arguments, four standard retrieval models are compared on 40 topics for 20 controversial issues: every issue has one topic with a biased stance and another neutral one. Following TREC, the top results of the different models on a topic were pooled and relevance-judged by one assessor per topic. The assessors also judged the arguments' rhetorical, logical, and dialectical quality, the results of which were cross-referenced with the relevance judgments. Furthermore, the assessors were asked for their personal opinion, and whether it matched the predefined stance of a topic. Among other results, we find that Terrier's implementations of DirichletLM and DPH are on par, significantly outperforming TFIDF and BM25. The judgments of relevance and quality hardly correlate, giving rise to a more diverse set of ranking criteria than relevance alone. We did not measure a significant bias of assessors when their stance is at odds with a topic's stance.},
  isbn = {978-1-4503-6172-9},
  venue = {Paris, France}
}

@inproceedings{Prade2017AnalogicalProportionsAnalogical,
  title = {Analogical {{Proportions}} and {{Analogical Reasoning}} - {{An Introduction}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Prade, Henri and Richard, Gilles},
  editor = {Aha, David W. and Lieber, Jean},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {16--32},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-61030-6_2},
  abstract = {Analogical proportions are statements of the form “a is to b as c is to d”. For more than a decade now, their formalization and use have raised the interest of a number of researchers. In this talk we shall primarily focus on their modeling in logical settings, both in the Boolean and in the multiple-valued cases. This logical view makes clear that analogy is as much a matter of dissimilarity as a matter of similarity. Moreover analogical proportions emerge as being especially remarkable in the framework of logical proportions. The analogical proportion and seven other code independent logical proportions can be shown as being of particular interest. Besides, analogical proportions are at the basis of an inference mechanism which enables us to complete or create a fourth item from three other items. The relation with case-based reasoning and case-based decision is emphasized. Potential applications and current developments are also discussed.},
  isbn = {978-3-319-61030-6},
  langid = {english}
}

@article{Pradeep2024PracticalExplorationConvergence,
  title = {A Practical Exploration of the Convergence of {{Case-Based Reasoning}} and {{Explainable Artificial Intelligence}}},
  author = {Pradeep, Preeja and Caro-Martínez, Marta and Wijekoon, Anjana},
  date = {2024-12-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {255},
  pages = {124733},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2024.124733},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417424016002},
  urldate = {2024-09-16},
  abstract = {As Artificial Intelligence (AI) systems become increasingly complex, ensuring their decisions are transparent and understandable to users has become paramount. This paper explores the integration of Case-Based Reasoning (CBR) with Explainable Artificial Intelligence (XAI) through a real-world example, which presents an innovative CBR-driven XAI platform. This study investigates how CBR, a method that solves new problems based on the solutions of similar past problems, can be harnessed to enhance the explainability of AI systems. Though the literature has few works on the synergy between CBR and XAI, exploring the principles for developing a CBR-driven XAI platform is necessary. This exploration outlines the key features and functionalities, examines the alignment of CBR principles with XAI goals to make AI reasoning more transparent to users, and discusses methodological strategies for integrating CBR into XAI frameworks. Through a case study of our CBR-driven XAI platform, iSee: Intelligent Sharing of Explanation Experience, we demonstrate the practical application of these principles, highlighting the enhancement of system transparency and user trust. The platform elucidates the decision-making processes of AI models and adapts to provide explanations tailored to diverse user needs. Our findings emphasize the importance of interdisciplinary approaches in AI research and the significant role CBR can play in advancing the goals of XAI.}
}

@article{Prakken2010AbstractFrameworkArgumentation,
  title = {An Abstract Framework for Argumentation with Structured Arguments},
  author = {Prakken, Henry},
  date = {2010-06-01},
  journaltitle = {Argument \& Computation},
  volume = {1},
  number = {2},
  pages = {93--124},
  publisher = {Taylor \& Francis},
  issn = {1946-2166},
  doi = {10.1080/19462160903564592},
  url = {https://doi.org/10.1080/19462160903564592},
  urldate = {2021-02-14},
  abstract = {An abstract framework for structured arguments is presented, which instantiates Dung's (‘On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming, and n-Person Games’, Artificial Intelligence, 77, 321–357) abstract argumentation frameworks. Arguments are defined as inference trees formed by applying two kinds of inference rules: strict and defeasible rules. This naturally leads to three ways of attacking an argument: attacking a premise, attacking a conclusion and attacking an inference. To resolve such attacks, preferences may be used, which leads to three corresponding kinds of defeat: undermining, rebutting and undercutting defeats. The nature of the inference rules, the structure of the logical language on which they operate and the origin of the preferences are, apart from some basic assumptions, left unspecified. The resulting framework integrates work of Pollock, Vreeswijk and others on the structure of arguments and the nature of defeat and extends it in several respects. Various rationality postulates are proved to be satisfied by the framework, and several existing approaches are proved to be a special case of the framework, including assumption-based argumentation and DefLog.}
}

@article{Prakken2015FormalizationArgumentationSchemes,
  title = {A Formalization of Argumentation Schemes for Legal Case-Based Reasoning in {{ASPIC}}+},
  author = {Prakken, Henry and Wyner, Adam and Bench-Capon, Trevor and Atkinson, Katie},
  date = {2015-10-01},
  journaltitle = {Journal of Logic and Computation},
  shortjournal = {J Logic Computation},
  volume = {25},
  number = {5},
  pages = {1141--1166},
  publisher = {Oxford Academic},
  issn = {0955-792X},
  doi = {10.1093/logcom/ext010},
  url = {https://academic.oup.com/logcom/article/25/5/1141/937102},
  urldate = {2020-09-02},
  abstract = {Abstract.  In this article we offer a formal account of reasoning with legal cases in terms of argumentation schemes. These schemes, and undercutting attacks as},
  langid = {english}
}

@online{PrincetonUniversity2021Morphy,
  title = {Morphy},
  author = {{Princeton University}},
  date = {2021},
  url = {https://wordnet.princeton.edu/documentation/morphy7wn},
  urldate = {2021-03-08},
  organization = {WordNet Documentation}
}

@online{PrincetonUniversity2021WordNetDatabaseFormat,
  title = {{{WordNet Database Format}}},
  author = {{Princeton University}},
  date = {2021},
  url = {https://wordnet.princeton.edu/documentation/wndb5wn},
  urldate = {2021-02-11}
}

@online{PrincetonUniversity2021WordNetGlossary,
  title = {{{WordNet Glossary}}},
  author = {{Princeton University}},
  date = {2021},
  url = {https://wordnet.princeton.edu/documentation/wngloss7wn},
  urldate = {2021-02-11}
}

@online{PrincetonUniversity2021WordNetStatistics,
  title = {{{WordNet Statistics}}},
  author = {{Princeton University}},
  date = {2021},
  url = {https://wordnet.princeton.edu/documentation/wnstats7wn},
  urldate = {2021-03-09},
  organization = {WordNet Documentation}
}

@inproceedings{Qiao2023ReasoningLanguageModel,
  title = {Reasoning with {{Language Model Prompting}}: {{A Survey}}},
  shorttitle = {Reasoning with {{Language Model Prompting}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {5368--5393},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.294},
  url = {https://aclanthology.org/2023.acl-long.294},
  urldate = {2024-09-16},
  abstract = {Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).},
  eventtitle = {{{ACL}} 2023}
}

@inproceedings{Qu2021RocketQAOptimizedTraining,
  title = {{{RocketQA}}: {{An Optimized Training Approach}} to {{Dense Passage Retrieval}} for {{Open-Domain Question Answering}}},
  shorttitle = {{{RocketQA}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Qu, Yingqi and Ding, Yuchen and Liu, Jing and Liu, Kai and Ren, Ruiyang and Zhao, Wayne Xin and Dong, Daxiang and Wu, Hua and Wang, Haifeng},
  editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  date = {2021-06},
  pages = {5835--5847},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.naacl-main.466},
  url = {https://aclanthology.org/2021.naacl-main.466/},
  urldate = {2025-07-15},
  abstract = {In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.},
  eventtitle = {{{NAACL-HLT}} 2021}
}

@inproceedings{Quensel2025InvestigatingSubjectiveFactors,
  title = {Investigating {{Subjective Factors}} of {{Argument Strength}}: {{Storytelling}}, {{Emotions}}, and {{Hedging}}},
  shorttitle = {Investigating {{Subjective Factors}} of {{Argument Strength}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Quensel, Carlotta and Falk, Neele and Lapesa, Gabriella},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {126--139},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.12/},
  urldate = {2025-07-28},
  abstract = {In assessing argument strength, the notions of what makes a good argument are manifold. With the broader trend towards treating subjectivity as an asset and not a problem in NLP, new dimensions of argument quality are studied. Although studies on individual subjective features like personal stories exist, there is a lack of large-scale analyses of the relation between these features and argument strength. To address this gap, we conduct regression analysis to quantify the impact of subjective factors – emotions, storytelling, and hedging - on two standard datasets annotated for objective argument quality and subjective persuasion. As such, our contribution is twofold: at the level of contributed resources, as there are no datasets annotated with all studied dimensions, this work compares and evaluates automated annotation methods for each subjective feature. At the level of novel insights, our regression analysis uncovers different patterns of impact of subjective features on the two facets of argument strength encoded in the datasets. Our results show that storytelling and hedging have contrasting effects on objective and subjective argument quality, while the influence of emotions depends on their rhetoric utilization rather than the domain.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{R2021TextDocumentSummarization,
  title = {Text {{Document Summarization Using POS}} Tagging for {{Kannada Text Documents}}},
  booktitle = {2021 11th {{International Conference}} on {{Cloud Computing}}, {{Data Science}} \& {{Engineering}} ({{Confluence}})},
  author = {R, Jayashree and Anami, Basavaraj S and K, Poornima B},
  date = {2021-01},
  pages = {423--426},
  doi = {10.1109/Confluence51648.2021.9377106},
  url = {https://ieeexplore.ieee.org/document/9377106},
  urldate = {2023-10-26},
  abstract = {Humongous amount of data available on the world wide web has been a constant issue pertaining to better Information Retrieval (IR) techniques. Text document summarization is there around for the past several decades, but providing a succinct summary has been challenging as ever. This work focuses on extractive summarization techniques using POS tagging, where the goal is to tag individual words with its parts of speech in a document and do the extractive summarization with more grammatical meaning. The Hidden Markov Model (HMM) is used for tagging the dataset. The idea is to use sentence ranking to produce the summary of a given document. This method of summarization uses the key phrase extraction, where the goal is to select individual words or sentences to tag a document to create text document summary.},
  eventtitle = {2021 11th {{International Conference}} on {{Cloud Computing}}, {{Data Science}} \& {{Engineering}} ({{Confluence}})}
}

@online{Radensky2024ScideatorHumanLLMScientific,
  title = {Scideator: {{Human-LLM Scientific Idea Generation Grounded}} in {{Research-Paper Facet Recombination}}},
  shorttitle = {Scideator},
  author = {Radensky, Marissa and Shahid, Simra and Fok, Raymond and Siangliulue, Pao and Hope, Tom and Weld, Daniel S.},
  date = {2024-09-23},
  eprint = {2409.14634},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.14634},
  url = {http://arxiv.org/abs/2409.14634},
  urldate = {2024-10-19},
  abstract = {The scientific ideation process often involves blending salient aspects of existing papers to create new ideas. To see if large language models (LLMs) can assist this process, we contribute Scideator, a novel mixed-initiative tool for scientific ideation. Starting from a user-provided set of papers, Scideator extracts key facets (purposes, mechanisms, and evaluations) from these and relevant papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users to gauge idea novelty by searching the literature for potential overlaps and showing automated novelty assessments and explanations. To support these tasks, Scideator introduces four LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty Checker, and Idea Novelty Iterator. In a within-subjects user study, 19 computer-science researchers identified significantly more interesting ideas using Scideator compared to a strong baseline combining a scientific search engine with LLM interaction.},
  pubstate = {prepublished}
}

@online{Radford2018ImprovingLanguageUnderstanding,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018},
  eprinttype = {OpenAI},
  url = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  abstract = {Natural language understanding comprises a wide range of diverse tasks suchas textual entailment, question answering, semantic similarity assessment, anddocument classification.   Although  large unlabeled  text  corpora are abundant,labeled data for learning these specific tasks is scarce, making it challenging fordiscriminatively trained models to perform adequately. We demonstrate that largegains on these tasks can be realized bygenerative pre-trainingof a language modelon a diverse corpus of unlabeled text, followed bydiscriminative fine-tuningon eachspecific task. In contrast to previous approaches, we make use of task-aware inputtransformations during fine-tuning to achieve effective transfer while requiringminimal changes to the model architecture. We demonstrate the effectiveness ofour approach on a wide range of benchmarks for natural language understanding.Our general task-agnostic model outperforms discriminatively trained models thatuse architectures specifically crafted for each task, significantly improving upon thestate of the art in 9 out of the 12 tasks studied. For instance, we achieve absoluteimprovements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% onquestion answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  pubstate = {prepublished}
}

@online{Radford2019LanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  pubstate = {prepublished}
}

@inproceedings{Rahimi2015IncorporatingCoherenceTopics,
  title = {Incorporating {{Coherence}} of {{Topics}} as a {{Criterion}} in {{Automatic Response-to-Text Assessment}} of the {{Organization}} of {{Writing}}},
  booktitle = {Proceedings of the {{Tenth Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}}},
  author = {Rahimi, Zahra and Litman, Diane and Wang, Elaine and Correnti, Richard},
  date = {2015-06},
  pages = {20--30},
  publisher = {Association for Computational Linguistics},
  location = {Denver, Colorado},
  doi = {10.3115/v1/W15-0603},
  url = {https://www.aclweb.org/anthology/W15-0603},
  urldate = {2019-11-18}
}

@article{Rahwan2007LayingFoundationsWorld,
  title = {Laying the Foundations for a {{World Wide Argument Web}}},
  author = {Rahwan, Iyad and Zablith, Fouad and Reed, Chris},
  date = {2007-07-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {171},
  number = {10--15},
  pages = {897--921},
  doi = {10.1016/j.artint.2007.04.015},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0004370207000768},
  urldate = {2018-09-01}
}

@online{Raj2025ExploringNewApproaches,
  title = {Exploring New {{Approaches}} for {{Information Retrieval}} through {{Natural Language Processing}}},
  author = {Raj, Manak and Mishra, Nidhi},
  date = {2025-05-04},
  eprint = {2505.02199},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.02199},
  url = {http://arxiv.org/abs/2505.02199},
  urldate = {2025-09-03},
  abstract = {This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.},
  pubstate = {prepublished}
}

@article{Ramesh2025ReviewNLPZeroshot,
  title = {A Review on {{NLP}} Zero-Shot and Few-Shot Learning: Methods and Applications},
  shorttitle = {A Review on {{NLP}} Zero-Shot and Few-Shot Learning},
  author = {Ramesh, G. and Sahil, Mahammad and Palan, Shashank A. and Bhandary, Darshan and Ashok, Teli Abhishek and Shreyas, J. and Sowjanya, N.},
  date = {2025-08-21},
  journaltitle = {Discover Applied Sciences},
  shortjournal = {Discov Appl Sci},
  volume = {7},
  number = {9},
  pages = {966},
  issn = {3004-9261},
  doi = {10.1007/s42452-025-07225-5},
  url = {https://doi.org/10.1007/s42452-025-07225-5},
  urldate = {2025-09-09},
  abstract = {Zero-shot and few-shot learning techniques in natural language processing (NLP), this comprehensive review traces their evolution from traditional methods to cutting-edge approaches like transfer learning and pre-trained language models, semantic embedding, attribute-based approaches, generative models for data augmentation in zero-shot learning, and meta-learning, model-agnostic meta-learning, relationship networks, model-agnostic meta-learning (MAML), prototypical networks in few-shot learning. Real-world applications underscore the adaptability and efficacy of these techniques across various NLP tasks in both industry and academia. Acknowledging challenges inherent in zero-shot and few-shot learning, this review identifies limitations and suggests avenues for improvement. It emphasizes theoretical foundations alongside practical considerations such as accuracy and generalization across diverse NLP tasks. By consolidating key insights, this review provides researchers and practitioners with valuable guidance on the current state and future potential of zero-shot and few-shot learning techniques in addressing real-world NLP challenges. Looking ahead, this review aims to stimulate further research, fostering a deeper understanding of the complexities and applicability of zero-shot and few-shot learning techniques in NLP. By offering a roadmap for future exploration, it seeks to contribute to the ongoing advancement and practical implementation of NLP technologies across various domains.},
  langid = {english}
}

@article{Ramos2003UsingTfidfDetermine,
  title = {Using Tf-Idf to Determine Word Relevance in Document Queries},
  author = {Ramos, Juan},
  date = {2003-01-01},
  journaltitle = {cs.rutgers.edu},
  url = {https://www.cs.rutgers.edu/~mlittman/courses/ml03/iCML03/papers/ramos.pdf},
  urldate = {2018-09-01},
  abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency (TF-IDF) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high TF-IDF numbers imply a strong relationship with the document they appear in, suggesting that if that~…}
}

@inproceedings{Raphael2006DerivationalAnalogyChallenges,
  title = {Derivational {{Analogy}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Derivational {{Analogy}}},
  booktitle = {Intelligent {{Computing}} in {{Engineering}} and {{Architecture}}},
  author = {Raphael, B.},
  editor = {Smith, Ian F. C.},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {545--553},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11888598_49},
  abstract = {Transformational analogy is currently more widely employed than derivational analogy in CBR applications, even though the latter has significant advantages over the former. The main reason for the reluctance to use derivational analogy is the complexity of representation. Other factors include issues related to retrieval and difficulties in system validation. Means of addressing these issues are described in this paper. Unique opportunities offered by the approach are illustrated with examples.},
  isbn = {978-3-540-46247-7},
  langid = {english}
}

@thesis{Ravindran2024TextGenerationArgument,
  type = {mathesis},
  title = {Text {{Generation}} from {{Argument Graphs}} with {{User-Generated Content}}},
  author = {Ravindran, Nila},
  date = {2024-05-24},
  institution = {Trier University},
  location = {Trier, Germany},
  abstract = {This thesis addresses the challenge of converting user-generated arguments into coherent textual representations. While traditional sources like newspaper articles or scientific journals have a plain text representation, user-generated content lacks such textual counterparts, posing a challenge for generating and assessing textual representations. For this purpose, our approach delves into unsupervised summarization, exploring both extractive and abstractive methods. Our goal is to create concise yet comprehensive summaries of user-generated arguments. Specifically, we concentrate on arguments sourced from the Kialo platform, which hosts debates structured hierarchically—a specialized form of argument graphs. Our research investigates various methods for conveying the structural information inherent in these argument graphs, leveraging pre-trained language models to generate summaries. Through human evaluation, we found a notable preference for abstractive summaries, with gpt-4 Turbo model exhibiting promising performance. Furthermore, we found that representing argument graphs using structured formats such as JSON yields better results compared to alternative strategies.},
  langid = {english}
}

@article{Recio-Garcia2014TemplateBasedDesignCOLIBRI,
  title = {Template-{{Based Design}} in {{COLIBRI Studio}}},
  author = {Recio-García, Juan A. and González-Calero, Pedro A. and Díaz-Agudo, Belén},
  date = {2014-03-01},
  journaltitle = {Information Systems},
  shortjournal = {Information Systems},
  volume = {40},
  pages = {168--178},
  issn = {0306-4379},
  doi = {10.1016/j.is.2012.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437912001457},
  urldate = {2025-05-13},
  abstract = {Case-Based Reasoning (CBR) and software generation share a common conceptual model of reusing components to obtain new solutions. Based on this model, in this paper we describe a novel development process for generating CBR systems based on the idea of reusing previous system designs. Template-Based Design (TBD) is the most significant activity in this development process and defines several actors (researcher, developer, student, etc.) involved in the development process of CBR systems with different backgrounds, motivations and goals. The TBD activity is supported by several tools integrated into the COLIBRI Studio Development Environment. These tools rely on semantic descriptions of workflows and components by means of an ontology that supports the reasoning regarding the correctness of the application being generated. We claim that the conceptualization of system behavior into templates and its reuse through the TBD serves to reduce the development effort required to build CBR applications. Thus, we present an experimental evaluation of the viability of the approach.}
}

@inproceedings{Recio-Garcia2021CaseBasedApproachSelection,
  title = {A {{Case-Based Approach}} for the {{Selection}} of {{Explanation Algorithms}} in {{Image Classification}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Recio-García, Juan A. and Parejas-Llanovarced, Humberto and Orozco-del-Castillo, Mauricio G. and Brito-Borges, Esteban E.},
  editor = {Sánchez-Ruiz, Antonio A. and Floyd, Michael W.},
  date = {2021},
  pages = {186--200},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-86957-1_13},
  abstract = {Research on eXplainable AI (XAI) is continuously proposing novel approaches for the explanation of image classification models, where we can find both model-dependent and model-independent strategies. However, it is unclear how to choose the best explanation approach for a given image, as these novel XAI approaches are radically different. In this paper, we propose a CBR solution to the problem of choosing the best alternative for the explanation of an image classifier. The case base reflects the human perception of the quality of the explanations generated with different image explanation methods. Then, this experience is reused to select the best explanation approach for a given image.},
  isbn = {978-3-030-86957-1},
  langid = {english}
}

@book{Recio-Garcia2024CaseBasedReasoningResearch,
  title = {Case-{{Based Reasoning Research}} and {{Development}}: 32nd {{International Conference}}, {{ICCBR}} 2024, {{Merida}}, {{Mexico}}, {{July}} 1–4, 2024, {{Proceedings}}},
  shorttitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14775},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2},
  url = {https://link.springer.com/10.1007/978-3-031-63646-2},
  urldate = {2024-06-26},
  isbn = {978-3-031-63645-5 978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Reed1996ArchitectureArgumentativeDialogue,
  title = {An Architecture for Argumentative Dialogue Planning},
  booktitle = {Practical {{Reasoning}}},
  author = {Reed, Chris and Long, Derek and Fox, Maria},
  editor = {Gabbay, Dov M. and Ohlbach, Hans Jürgen},
  date = {1996},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {555--566},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-61313-7_100},
  abstract = {Argument represents an opportunity for a system to convince a possibly sceptical or resistant audience of the veracity of its own beliefs. This ability is a vital component of rich communication, facilitating explanation, instruction, cooperation and conflict resolution. In this paper, a proposal is presented for the architecture of a system capable of constructing arguments. The design of the architecture has made use of the wealth of naturally occurring argument, which, unlike much natural language, is particularly suited to analysis due to its clear aims and structure. The proposed framework is based upon a core hierarchical planner conceptually split into four levels of processing, the highest being responsible for abstract, intentional and pragmatic guidance, and the lowest handling realisation into natural language. The higher levels will have control over not just the logical form of the argument, but also over matters of style and rhetoric, in order to produce as cogent and convincing an argument as possible.},
  isbn = {978-3-540-68454-1},
  langid = {english}
}

@article{Reed2001ApplicationsArgumentationSchemes,
  title = {Applications of {{Argumentation Schemes}}},
  author = {Reed, Chris and Walton, Doug},
  date = {2001-05-17},
  journaltitle = {OSSA Conference Archive},
  url = {https://scholar.uwindsor.ca/ossaarchive/OSSA4/papersandcommentaries/97},
  abstract = {Argumentation schemes capture common, stereotypical patterns of reasoning which are nondeductive and nonmonotonic. As interest in understanding these schemes from a theoretical point of view grows, so too does an awareness within computational work that these schemes might yield powerful techniques in a range of domains. This paper aims to perform two functions. First, to briefly review the literature on argumentation schemes, including the key works by Hastings, Walton and Kienpointner, and to set it in a broader context, adducing concerns from deductivism and presumptive, nonmonotonic reasoning. The second is to consider the various roles argumentation schemes might play in Artificial Intelligence, and in particular to consider (i) how schemes might be characterized as planning operators in domains such as natural language generation, with operationalized schemes representing means of achieving specific (e.g. persuasive) goals; (ii) how schemes might be exploited in AI domains typically characterized by a deductive (if argumentation theoretic) basis for communication, such as communication between intelligent agents; (iii) how schemes might be used in teaching critical thinking, and how they might be characterized in pedagogical software; (iv) more broadly, the representation tasks that can be facilitated using argumentation schemes (drawing particularly upon the wide-ranging discussions held at the Symposium on Argument and Computation held in Scotland in July 2000); and (v) the role that critical questions associated with schemes have in generative, representational and pedagogical aspects of argumentation in AI. The paper concludes with desiderata for a theoretical understanding of argumentation schemes, motivated by the demands of the AI applications discussed, with the aim of stimulating and outlining further foundational research in the area.}
}

@inproceedings{Reed2003ArgumentationSchemesArgumentasProcess,
  title = {Argumentation {{Schemes}} in {{Argument-as-Process}} and {{Argument-as-Product}}},
  booktitle = {Proceedings of the {{Conference Celebrating Informal Logic}}},
  author = {Reed, Chris and Walton, Douglas},
  date = {2003},
  location = {Windsor, ON},
  url = {https://core.ac.uk/outputs/72771208/},
  urldate = {2025-09-08},
  abstract = {Seeing an argument as a static, fixed, product of reasoning has allowed representational models to be developed which can handle and manipulate complex argument structures. Implementing these models using artificial intelligence techniques has shown how these static structures can not only be the result of argument processes, but can also be the foundation for argument processes. An argument-as-product representation can be used as a basis for providing structured information, for eliciting knowledge from experts, and for mediating online discussion, but in each case must be combined with elements of an argument-as-process representation. One example of a particularly close tie between the product- and process-oriented representations lies in argumentation schemes. It has been demonstrated both that such schemes have a crucial role to play in understanding everyday discourse, and also that they are well within the capabilities of current AI technology. However, the ways in which argumentation schemes drive a dialogue onwards, through a combination of critical questioning and relevance maintenance, has remained largely unaddressed. Here, the relationship between the argument-as-process and argument-as-product representations is explored, using as a focus the roles that argumentation schemes play in the two approaches.},
  eventtitle = {Conference {{Celebrating Informal Logic}}}
}

@article{Reed2004AraucariaSoftwareArgument,
  title = {Araucaria: Software for Argument Analysis, Diagramming and Representation},
  shorttitle = {Araucaria},
  author = {Reed, Chris and Rowe, Glenn},
  date = {2004-12},
  journaltitle = {International Journal on Artificial Intelligence Tools},
  shortjournal = {Int. J. Artif. Intell. Tools},
  volume = {13},
  number = {04},
  pages = {961--979},
  publisher = {World Scientific Publishing Co.},
  issn = {0218-2130},
  doi = {10.1142/S0218213004001922},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218213004001922},
  urldate = {2022-04-21},
  abstract = {Argumentation theory involves the analysis of naturally occurring argument, and one key tool employed to this end both in the academic community and in teaching critical thinking skills to undergraduates is argument diagramming. By identifying the structure of an argument in terms of its constituents and the relationships between them, it becomes easier to critically evaluate each part of an argument in turn. The task of analysis and diagramming, however, is labor intensive and often idiosyncratic, which can make academic exchange difficult. The Araucaria system provides an interface which supports the diagramming process, and then saves the result using AML, an open standard, designed in XML, for describing argument structure. Araucaria aims to be of use not only in pedagogical situations, but also in support of research activity. As a result, it has been designed from the outset to handle more advanced argumentation theoretic concepts such as schemes, which capture stereotypical patterns of reasoning. The software is also designed to be compatible with a number of applications under development, including dialogic interaction and online corpus provision. Together, these features, combined with its platform independence and ease of use, have the potential to make Araucaria a valuable resource for the academic community.}
}

@book{Reed2004ArgumentationMachines,
  title = {Argumentation {{Machines}}},
  editor = {Reed, Chris and Norman, Timothy J.},
  editora = {family=Eemeren, given=Frans H., prefix=van, useprefix=true and Jacobs, Scott and Krabbe, Erik C. W. and Woods, John},
  editoratype = {redactor},
  date = {2004},
  series = {Argumentation {{Library}}},
  volume = {9},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-017-0431-1},
  url = {http://link.springer.com/10.1007/978-94-017-0431-1},
  urldate = {2019-09-14},
  isbn = {978-94-017-0431-1}
}

@incollection{Reed2004RoadmapResearchArgument,
  title = {A {{Roadmap}} of {{Research}} in {{Argument}} and {{Computation}}},
  booktitle = {Argumentation {{Machines}}: {{New Frontiers}} in {{Argument}} and {{Computation}}},
  author = {Reed, Chris and Norman, Timothy J.},
  editor = {Reed, Chris and Norman, Timothy J.},
  date = {2004},
  series = {Argumentation {{Library}}},
  pages = {1--13},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-017-0431-1_1},
  url = {https://doi.org/10.1007/978-94-017-0431-1_1},
  urldate = {2023-11-22},
  abstract = {The aim of this chapter is to lay out a roadmap of artificial intelligence and argumentation research, summarising the key points in the development of the interdisciplinary field. By showing where such collaboration has been successful, it becomes easier to see the points at which renewed interaction between the fields might yield substantial gains. These points of potential growth are then briefly discussed to motivate and position the work described in the subsequent chapters.},
  isbn = {978-94-017-0431-1},
  langid = {english}
}

@inproceedings{Reed2006PreliminaryResultsArgument,
  title = {Preliminary Results from an Argument Corpus},
  booktitle = {In {{Eloína Miyares Bermúdez}} \& {{Leonel Ruiz Miyares}} ({{Eds}}), {{Linguistics}} in the Twenty-First Century},
  author = {Reed, Chris},
  date = {2006},
  pages = {185--196},
  publisher = {Scholars Press},
  abstract = {Abstract. As reported in (Katzav et al., 2003), the University of Dundee has been developing a small corpus of examples of argumentation from a variety of domains (newspaper editorials, advertising, parliamentary records,}
}

@article{Reed2007ArgumentDiagrammingLogic,
  title = {Argument Diagramming in Logic, Law and Artificial Intelligence},
  author = {Reed, Chris and Walton, Douglas and Macagno, Fabrizio},
  date = {2007-03},
  journaltitle = {The Knowledge Engineering Review},
  volume = {22},
  number = {1},
  pages = {87--109},
  issn = {1469-8005, 0269-8889},
  doi = {10.1017/S0269888907001051},
  url = {https://www.cambridge.org/core/journals/knowledge-engineering-review/article/argument-diagramming-in-logic-law-and-artificial-intelligence/41CDB75673A7175B6A99F76250921665},
  urldate = {2025-09-04},
  abstract = {In this paper, we present a survey of the development of the technique of argument diagramming covering not only the fields in which it originated — informal logic, argumentation theory, evidence law and legal reasoning — but also more recent work in applying and developing it in computer science and artificial intelligence (AI). Beginning with a simple example of an everyday argument, we present an analysis of it visualized as an argument diagram constructed using a software tool. In the context of a brief history of the development of diagramming, it is then shown how argument diagrams have been used to analyse and work with argumentation in law, philosophy and AI.},
  langid = {english}
}

@inproceedings{Reed2008AIFDialogueArgument,
  title = {{{AIF}}+: Dialogue in the Argument Interchange Format},
  shorttitle = {{{AIF}}+},
  booktitle = {Computational {{Models}} of {{Argument}}: {{Proceedings}} of {{COMMA}} 2008},
  author = {Reed, Chris and Wells, Simon and Devereux, Joseph and Rowe, Glenn},
  editor = {Besnard, Philippe and Doutre, Sylvie and Hunter, Anthony},
  date = {2008},
  series = {Frontiers in Artificial Intelligence and Applications},
  pages = {311--323},
  publisher = {IOS Press},
  location = {Amsterdam},
  url = {http://www.scopus.com/inward/record.url?scp=84875930895&partnerID=8YFLogxK},
  urldate = {2022-04-22},
  abstract = {This paper extends the Argument Interchange Format to enable it to represent dialogic argumentation. One of the challenges is to tie together the rules expressed in dialogue protocols with the inferential relations between premises and conclusions. The extensions are founded upon two important analogies which minimise the extra ontological machinery required. First, locutions in a dialogue are analogous to AIF I-nodes which capture propositional data. Second, steps between locutions are analogous to AIF S-nodes which capture inferential movement. This paper shows how these two analogies combine to allow both dialogue protocols and dialogue histories to be represented alongside monologic arguments in a single coherent system.}
}

@inproceedings{Reed2012HowDialoguesCreate,
  title = {How Dialogues Create Arguments},
  booktitle = {Proceedings of the 7th {{Conference}} of the {{International Society}} for the {{Study}} of {{Argumentation}} ({{ISSA}} 2010)},
  author = {Reed, C and Budzynska, Katarzyna},
  editor = {family=Eemeren, given=F H, prefix=van, useprefix=true},
  date = {2012},
  location = {Amsterdam},
  url = {https://www.semanticscholar.org/paper/136f6a955776de82866c1218b730482b5097a1b0},
  langid = {english}
}

@misc{Reed2016IATAnnotationGuidelines,
  title = {{{IAT}} Annotation Guidelines for {{US2016}}},
  author = {Reed, Chris and Budzynska, Katarzyna and Visser, Jacky},
  date = {2016-09},
  url = {IAT annotation guidelines for US2016},
  urldate = {2024-09-16},
  langid = {english}
}

@article{Reed2017ArgumentWebOnline,
  title = {The {{Argument Web}}: An {{Online Ecosystem}} of {{Tools}}, {{Systems}} and {{Services}} for {{Argumentation}}},
  author = {Reed, Chris and Budzynska, Katarzyna and Duthie, Rory and Janier, Mathilde and Konat, Barbara and Lawrence, John and Pease, Alison and Snaith, Mark},
  date = {2017-01-01},
  journaltitle = {Philosophy \& Technology},
  shortjournal = {Philos. Technol.},
  volume = {30},
  number = {2},
  pages = {137--160},
  doi = {10.1007/s13347-017-0260-8},
  url = {https://link.springer.com/article/10.1007/s13347-017-0260-8},
  urldate = {2018-09-01},
  abstract = {The Argument Web is maturing as both a platform built upon a synthesis of many contemporary theories of argumentation in philosophy and also as an ecosystem in which various applications and...}
}

@inproceedings{Reimers2019ClassificationClusteringArguments,
  title = {Classification and {{Clustering}} of {{Arguments}} with {{Contextualized Word Embeddings}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Reimers, Nils and Schiller, Benjamin and Beck, Tilman and Daxenberger, Johannes and Stab, Christian and Gurevych, Iryna},
  date = {2019-07},
  pages = {567--578},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/P19-1054},
  url = {https://www.aclweb.org/anthology/P19-1054},
  urldate = {2020-09-02},
  abstract = {We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.},
  eventtitle = {{{ACL}} 2019}
}

@inproceedings{Reimers2019SentenceBERTSentenceEmbeddings,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2019-11},
  pages = {3982--3992},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-1410},
  url = {https://www.aclweb.org/anthology/D19-1410},
  urldate = {2020-05-15},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textbackslash textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019}
}

@unpublished{Reimers2020MakingMonolingualSentence,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2020-04-21},
  eprint = {2004.09813},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.09813},
  urldate = {2020-05-15},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 10 languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.}
}

@article{Reiter1980LogicDefaultReasoning,
  title = {A Logic for Default Reasoning},
  author = {Reiter, R.},
  date = {1980-04-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  series = {Special {{Issue}} on {{Non-Monotonic Logic}}},
  volume = {13},
  number = {1},
  pages = {81--132},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(80)90014-4},
  url = {https://www.sciencedirect.com/science/article/pii/0004370280900144},
  urldate = {2025-05-13},
  abstract = {The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults. In this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.}
}

@book{Reiter2000BuildingNaturalLanguage,
  title = {Building {{Natural Language Generation Systems}}},
  author = {Reiter, Ehud and Dale, Robert},
  date = {2000},
  series = {Studies in {{Natural Language Processing}}},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9780511519857},
  url = {https://www.cambridge.org/core/books/building-natural-language-generation-systems/0AE70C709A9BFBDC80B349B2D22A78CD},
  urldate = {2025-09-17},
  abstract = {This book explains how to build Natural Language Generation (NLG) systems - computer software systems which use techniques from artificial intelligence and computational linguistics to automatically generate understandable texts in English or other human languages, either in isolation or as part of multimedia documents, Web pages, and speech output systems. Typically starting from some non-linguistic representation of information as input, NLG systems use knowledge about language and the application domain to automatically produce documents, reports, explanations, help messages, and other kinds of texts. The book covers the algorithms and representations needed to perform the core tasks of document planning, microplanning, and surface realization, using a case study to show how these components fit together. It also discusses engineering issues such as system architecture, requirements analysis, and the integration of text generation into multimedia and speech output systems.},
  isbn = {978-0-521-62036-9}
}

@inproceedings{Ren2024SurveyLargeLanguage,
  title = {A {{Survey}} of {{Large Language Models}} for {{Graphs}}},
  booktitle = {Proceedings of the 30th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla, Nitesh and Huang, Chao},
  date = {2024-08-24},
  series = {{{KDD}} '24},
  pages = {6616--6626},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3637528.3671460},
  url = {https://dl.acm.org/doi/10.1145/3637528.3671460},
  urldate = {2024-09-16},
  abstract = {Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers.},
  isbn = {979-8-4007-0490-1}
}

@online{Rescala2024CanLanguageModels,
  title = {Can {{Language Models Recognize Convincing Arguments}}?},
  author = {Rescala, Paula and Ribeiro, Manoel Horta and Hu, Tiancheng and West, Robert},
  date = {2024-03-31},
  eprint = {2404.00750},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.00750},
  url = {http://arxiv.org/abs/2404.00750},
  urldate = {2024-09-23},
  abstract = {The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus \& Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with this paper contribute to the crucial ongoing effort of continuously evaluating and monitoring the rapidly evolving capabilities and potential impact of LLMs.},
  pubstate = {prepublished}
}

@unpublished{Resnik1995UsingInformationContent,
  title = {Using {{Information Content}} to {{Evaluate Semantic Similarity}} in a {{Taxonomy}}},
  author = {Resnik, Philip},
  date = {1995-11-29},
  eprint = {cmp-lg/9511007},
  eprinttype = {arXiv},
  abstract = {This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).}
}

@unpublished{Ribeiro2020InvestigatingPretrainedLanguage,
  title = {Investigating {{Pretrained Language Models}} for {{Graph-to-Text Generation}}},
  author = {Ribeiro, Leonardo F. R. and Schmitt, Martin and Schütze, Hinrich and Gurevych, Iryna},
  date = {2020-07-16},
  eprint = {2007.08426},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2007.08426},
  urldate = {2020-10-16},
  abstract = {Graph-to-text generation, a subtask of data-to-text generation, aims to generate fluent texts from graph-based data. Many graph-to-text models have shown strong performance in this task employing specialized graph encoders. However, recent approaches employ large pretrained language models (PLMs) achieving state-of-the-art results in data-to-text generation. In this paper, we aim to investigate the impact of large PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. Our analysis shows that PLMs such as BART and T5 achieve state-of-the-art results in graph-to-text benchmarks without explicitly encoding the graph structure. We also demonstrate that task-adaptive pretraining strategies are beneficial to the target task, improving even further the state of the art in two benchmarks for graph-to-text generation. In a final analysis, we investigate possible reasons for the PLMs' success on graph-to-text tasks. We find evidence that their knowledge about the world gives them a big advantage, especially when generating texts from KGs.}
}

@inproceedings{Ribeiro2022FactGraphEvaluatingFactuality,
  title = {{{FactGraph}}: {{Evaluating Factuality}} in {{Summarization}} with {{Semantic Graph Representations}}},
  shorttitle = {{{FactGraph}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ribeiro, Leonardo F. R. and Liu, Mengwen and Gurevych, Iryna and Dreyer, Markus and Bansal, Mohit},
  editor = {Carpuat, Marine and family=Marneffe, given=Marie-Catherine, prefix=de, useprefix=true and Meza Ruiz, Ivan Vladimir},
  date = {2022-07},
  pages = {3238--3253},
  publisher = {Association for Computational Linguistics},
  location = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.236},
  url = {https://aclanthology.org/2022.naacl-main.236},
  urldate = {2024-10-31},
  abstract = {Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15\%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.},
  eventtitle = {{{NAACL-HLT}} 2022}
}

@inproceedings{Richter2007FoundationsSimilarityUtility,
  title = {Foundations of {{Similarity}} and {{Utility}}},
  booktitle = {Proceedings of the {{Twentieth International Florida Artificial Intelligence Research Society Conference}}},
  author = {Richter, Michael M.},
  editor = {Wilson, David and Sutcliffe, Geoff},
  date = {2007},
  volume = {20},
  pages = {30--37},
  publisher = {AAAI Press},
  location = {Key West, FL, USA},
  url = {http://www.aaai.org/Library/FLAIRS/2007/flairs07-008.php},
  urldate = {2025-07-07},
  eventtitle = {{{FLAIRS}}}
}

@book{Richter2013CaseBasedReasoningTextbook,
  title = {Case-{{Based Reasoning}}: {{A Textbook}}},
  shorttitle = {Case-{{Based Reasoning}}},
  author = {Richter, Michael M. and Weber, Rosina O.},
  date = {2013},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40167-1},
  url = {https://link.springer.com/10.1007/978-3-642-40167-1},
  urldate = {2025-05-13},
  isbn = {978-3-642-40166-4 978-3-642-40167-1},
  langid = {english}
}

@article{Riesen2009ApproximateGraphEdit,
  title = {Approximate Graph Edit Distance Computation by Means of Bipartite Graph Matching},
  author = {Riesen, Kaspar and Bunke, Horst},
  date = {2009-06-04},
  journaltitle = {Image and Vision Computing},
  shortjournal = {Image and Vision Computing},
  series = {7th {{IAPR-TC15 Workshop}} on {{Graph-based Representations}} ({{GbR}} 2007)},
  volume = {27},
  number = {7},
  pages = {950--959},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2008.04.004},
  url = {https://www.sciencedirect.com/science/article/pii/S026288560800084X},
  urldate = {2025-05-30},
  abstract = {In recent years, the use of graph based object representation has gained popularity. Simultaneously, graph edit distance emerged as a powerful and flexible graph matching paradigm that can be used to address different tasks in pattern recognition, machine learning, and data mining. The key advantages of graph edit distance are its high degree of flexibility, which makes it applicable to any type of graph, and the fact that one can integrate domain specific knowledge about object similarity by means of specific edit cost functions. Its computational complexity, however, is exponential in the number of nodes of the involved graphs. Consequently, exact graph edit distance is feasible for graphs of rather small size only. In the present paper we introduce a novel algorithm which allows us to approximately, or suboptimally, compute edit distance in a substantially faster way. The proposed algorithm considers only local, rather than global, edge structure during the optimization process. In experiments on different datasets we demonstrate a substantial speed-up of our proposed method over two reference systems. Moreover, it is emprically verified that the accuracy of the suboptimal distance remains sufficiently accurate for various pattern recognition applications.},
  langid = {english}
}

@inproceedings{Riesen2014CombiningBipartiteGraph,
  title = {Combining {{Bipartite Graph Matching}} and {{Beam Search}} for {{Graph Edit Distance Approximation}}},
  booktitle = {Artificial {{Neural Networks}} in {{Pattern Recognition}}},
  author = {Riesen, Kaspar and Fischer, Andreas and Bunke, Horst},
  editor = {El Gayar, Neamat and Schwenker, Friedhelm and Suen, Cheng},
  date = {2014},
  pages = {117--128},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11656-3_11},
  abstract = {Graph edit distance (GED) is a powerful and flexible graph dissimilarity model. Yet, exact computation of GED is an instance of a quadratic assignment problem and can thus be solved in exponential time complexity only. A previously introduced approximation framework reduces the computation of GED to an instance of a linear sum assignment problem. Major benefit of this reduction is that an optimal assignment of nodes (including local structures) can be computed in polynomial time. Given this assignment an approximate value of GED can be immediately derived. Yet, the primary optimization process of this approximation framework is able to consider local edge structures only, and thus, the observed speed up is at the expense of approximative, rather than exact, distance values. In order to improve the overall approximation quality, the present paper combines the original approximation framework with a fast tree search procedure. More precisely, we regard the assignment from the original approximation as a starting point for a subsequent beam search. In an experimental evaluation on three real world data sets a substantial gain of assignment accuracy can be observed while the run time remains remarkable low.},
  isbn = {978-3-319-11656-3},
  langid = {english}
}

@inproceedings{Riesen2014ComputingUpperLower,
  title = {Computing {{Upper}} and {{Lower Bounds}} of {{Graph Edit Distance}} in {{Cubic Time}}},
  booktitle = {Artificial {{Neural Networks}} in {{Pattern Recognition}}},
  author = {Riesen, Kaspar and Fischer, Andreas and Bunke, Horst},
  editor = {El Gayar, Neamat and Schwenker, Friedhelm and Suen, Cheng},
  date = {2014},
  pages = {129--140},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11656-3_12},
  abstract = {Exact computation of graph edit distance (GED) can be solved in exponential time complexity only. A previously introduced approximation framework reduces the computation of GED to an instance of a linear sum assignment problem. Major benefit of this reduction is that an optimal assignment of nodes (including local structures) can be computed in polynomial time. Given this assignment an approximate value of GED can be immediately derived. Yet, since this approach considers local – rather than the global – structural properties of the graphs only, the GED derived from the optimal assignment is suboptimal. The contribution of the present paper is twofold. First, we give a formal proof that this approximation builds an upper bound of the true graph edit distance. Second, we show how the existing approximation framework can be reformulated such that a lower bound of the edit distance can be additionally derived. Both bounds are simultaneously computed in cubic time.},
  isbn = {978-3-319-11656-3},
  langid = {english}
}

@inproceedings{Riesen2015GreedyGraphEdit,
  title = {Greedy {{Graph Edit Distance}}},
  booktitle = {Machine {{Learning}} and {{Data Mining}} in {{Pattern Recognition}}},
  author = {Riesen, Kaspar and Ferrer, Miquel and Dornberger, Rolf and Bunke, Horst},
  editor = {Perner, Petra},
  date = {2015},
  pages = {3--16},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-21024-7_1},
  abstract = {In pattern recognition and data mining applications, where the underlying data is characterized by complex structural relationships, graphs are often used as a formalism for object representation. Yet, the high representational power and flexibility of graphs is accompanied by a significant increase of the complexity of many algorithms. For instance, exact computation of pairwise graph dissimilarity, i.e.~distance, can be accomplished in exponential time complexity only. A previously introduced approximation framework reduces the problem of graph comparison to an instance of a linear sum assignment problem which allows graph dissimilarity computation in cubic time. The present paper introduces an extension of this approximation framework that runs in quadratic time. We empirically confirm the scalability of our extension with respect to the run time, and moreover show that the quadratic approximation leads to graph dissimilarities which are sufficiently accurate for graph based pattern classification.},
  isbn = {978-3-319-21024-7},
  langid = {english}
}

@incollection{Rietzke2025HybridesWissensbasiertesReasoning,
  title = {Hybrides wissensbasiertes Reasoning Für~wissensintensive Prozesse am Beispiel von Notrufabfragen},
  booktitle = {Hybride KI mit Machine Learning und Knowledge Graphs: Innovative Lösungen aus der Praxis},
  author = {Rietzke, Eric and Maletzki, Carsten and Grumbach, Lisa and Bergmann, Ralph},
  editor = {Hinkelmann, Knut and Hoppe, Thomas and Humm, Bernhard G.},
  date = {2025},
  pages = {297--318},
  publisher = {Springer Fachmedien},
  location = {Wiesbaden},
  doi = {10.1007/978-3-658-44781-6_13},
  url = {https://doi.org/10.1007/978-3-658-44781-6_13},
  urldate = {2025-03-05},
  abstract = {Die Bewältigung von Naturkatastrophen, Pandemien und Großschadensereignissen erfordert vielfältige Maßnahmen, die zu einer Aktivierung von Feuerwehr, Rettungsdienst und anderen Behörden und Organisationen mit Sicherheitsaufgaben (BOS) führen. Die BOS reagieren hierbei oftmals auf Notrufe, die Bürgerinnen und Bürger bspw. unter der europaweiten Notrufnummer 112 absetzen. Entgegengenommen werden diese Notrufe durch Disponenten, die initial durch die Notsituation begleiten und eine angemessene Alarmierung von Einsatzkräften einleiten.},
  isbn = {978-3-658-44781-6},
  langid = {ngerman}
}

@book{Rijsbergen1979InformationRetrieval,
  title = {Information {{Retrieval}}},
  author = {Rijsbergen, C. J. Van},
  date = {1979},
  edition = {2},
  publisher = {Butterworth-Heinemann},
  location = {USA},
  url = {http://www.dcs.gla.ac.uk/Keith/Preface.html},
  isbn = {978-0-408-70929-3},
  pagetotal = {208}
}

@article{Rissland2005CasebasedReasoningLaw,
  title = {Case-Based Reasoning and Law},
  author = {Rissland, Edwina L. and Ashley, Kevin D. and Branting, L. Karl},
  date = {2005-09},
  journaltitle = {The Knowledge Engineering Review},
  volume = {20},
  number = {3},
  pages = {293--298},
  publisher = {Cambridge University Press},
  issn = {1469-8005, 0269-8889},
  doi = {10.1017/S0269888906000701},
  url = {https://www.cambridge.org/core/journals/knowledge-engineering-review/article/casebased-reasoning-and-law/C28717CB05116C845697BB33F926B784},
  urldate = {2020-09-02},
  abstract = {A primary research stream that contributed to the birth of case-based reasoning (CBR) was Artificial Intelligence and Law. Since law is largely about cases, it is a particularly interesting domain for CBR researchers. This article surveys some of the historically significant systems and developments in this field.},
  langid = {english}
}

@inproceedings{Ristoski2016RDF2VecRDFGraph,
  title = {{{RDF2Vec}}: {{RDF Graph Embeddings}} for {{Data Mining}}},
  shorttitle = {{{RDF2Vec}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2016},
  author = {Ristoski, Petar and Paulheim, Heiko},
  editor = {Groth, Paul and Simperl, Elena and Gray, Alasdair and Sabou, Marta and Krötzsch, Markus and Lecue, Freddy and Flöck, Fabian and Gil, Yolanda},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {498--514},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-46523-4_30},
  abstract = {Linked Open Data has been recognized as a valuable source for background information in data mining. However, most data mining tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. Our evaluation shows that such vector representations outperform existing techniques for the propositionalization of RDF graphs on a variety of different predictive machine learning tasks, and that feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.},
  isbn = {978-3-319-46523-4},
  langid = {english}
}

@article{Robertson1994OkapiTREC3,
  title = {Okapi at {{TREC-3}}},
  author = {Robertson, Stephen E.},
  date = {1994},
  pages = {109--126},
  url = {http://trec.nist.gov/pubs/trec3/papers/city.ps.gz}
}

@inproceedings{Rocha2023AssessingGoodBad,
  title = {Assessing {{Good}}, {{Bad}} and {{Ugly Arguments Generated}} by {{ChatGPT}}: A {{New Dataset}}, Its {{Methodology}} and {{Associated Tasks}}},
  shorttitle = {Assessing {{Good}}, {{Bad}} and {{Ugly Arguments Generated}} by {{ChatGPT}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Rocha, Victor Hugo Nascimento and Silveira, Igor Cataneo and Pirozelli, Paulo and Mauá, Denis Deratani and Cozman, Fabio Gagliardi},
  editor = {Moniz, Nuno and Vale, Zita and Cascalho, José and Silva, Catarina and Sebastião, Raquel},
  date = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {428--440},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-49008-8_34},
  abstract = {The recent success of Large Language Models (LLMs) has sparked concerns about their potential to spread misinformation. As a result, there is a pressing need for tools to identify “fake arguments” generated by such models. To create these tools, examples of texts generated by LLMs are needed. This paper introduces a methodology to obtain good, bad and ugly arguments from argumentative essays produced by ChatGPT, OpenAI’s LLM. We then describe a novel dataset containing a set of diverse arguments, ArGPT. We assess the effectiveness of our dataset and establish baselines for several argumentation-related tasks. Finally, we show that the artificially generated data relates well to human argumentation and thus is useful as a tool to train and test systems for the defined tasks.},
  isbn = {978-3-031-49008-8},
  langid = {english}
}

@article{Rocha2024CrossgenreArgumentMining,
  title = {Cross-Genre Argument Mining: {{Can}} Language Models Automatically Fill in Missing Discourse Markers?},
  shorttitle = {Cross-Genre Argument Mining},
  author = {Rocha, Gil and Lopes Cardoso, Henrique and Belouadi, Jonas and Eger, Steffen},
  date = {2024-01-01},
  journaltitle = {Argument \& Computation},
  volume = {Preprint},
  pages = {1--41},
  publisher = {IOS Press},
  issn = {1946-2166},
  doi = {10.3233/AAC-230008},
  url = {https://content.iospress.com/articles/argument-and-computation/aac230008},
  urldate = {2024-06-21},
  abstract = {Available corpora for Argument Mining differ along several axes, and one of the key differences is the presence (or absence) of discourse markers to signal argumentative content. Exploring effective ways to use discourse markers has received wide att},
  issue = {Preprint},
  langid = {english}
}

@inproceedings{Romberg2022YourPerspectiveAlso,
  title = {Is {{Your Perspective Also My Perspective}}? {{Enriching Prediction}} with {{Subjectivity}}},
  shorttitle = {Is {{Your Perspective Also My Perspective}}?},
  booktitle = {Proceedings of the 9th {{Workshop}} on {{Argument Mining}}},
  author = {Romberg, Julia},
  editor = {Lapesa, Gabriella and Schneider, Jodi and Jo, Yohan and Saha, Sougata},
  date = {2022-10},
  pages = {115--125},
  publisher = {International Conference on Computational Linguistics},
  location = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.argmining-1.11},
  urldate = {2024-04-09},
  abstract = {Although argumentation can be highly subjective, the common practice with supervised machine learning is to construct and learn from an aggregated ground truth formed from individual judgments by majority voting, averaging, or adjudication. This approach leads to a neglect of individual, but potentially important perspectives and in many cases cannot do justice to the subjective character of the tasks. One solution to this shortcoming are multi-perspective approaches, which have received very little attention in the field of argument mining so far. In this work we present PerspectifyMe, a method to incorporate perspectivism by enriching a task with subjectivity information from the data annotation process. We exemplify our approach with the use case of classifying argument concreteness, and provide first promising results for the recently published CIMT PartEval Argument Concreteness Corpus.},
  eventtitle = {{{ArgMining}} 2022}
}

@incollection{Rose2010AutomaticKeywordExtraction,
  title = {Automatic {{Keyword Extraction}} from {{Individual Documents}}},
  booktitle = {Text {{Mining}}},
  author = {Rose, Stuart and Engel, Dave and Cramer, Nick and Cowley, Wendy},
  editor = {Berry, Michael W. and Kogan, Jacob},
  date = {2010-03-04},
  pages = {1--20},
  publisher = {John Wiley \& Sons, Ltd},
  location = {Chichester, UK},
  doi = {10.1002/9780470689646.ch1},
  url = {http://doi.wiley.com/10.1002/9780470689646.ch1},
  urldate = {2021-02-07},
  isbn = {978-0-470-68964-6},
  langid = {english}
}

@article{Rosenblatt1958PerceptronProbabilisticModel,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  author = {Rosenblatt, F},
  date = {1958-11},
  journaltitle = {Psychological review},
  volume = {65},
  number = {6},
  eprint = {13602029},
  eprinttype = {pubmed},
  pages = {386--408},
  issn = {0033-295X},
  doi = {10.1037/h0042519},
  url = {http://dx.doi.org/10.1037/h0042519},
  langid = {english}
}

@inproceedings{Roth-Berghofer2005MappingGoalsKinds,
  title = {Mapping {{Goals}} and {{Kinds}} of {{Explanations}} to the {{Knowledge Containers}} of {{Case-Based Reasoning Systems}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Roth-Berghofer, Thomas R. and Cassens, Jörg},
  editor = {Muñoz-Ávila, Héctor and Ricci, Francesco},
  date = {2005},
  pages = {451--464},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11536406_35},
  abstract = {Research on explanation in Case-Based Reasoning (CBR) is a topic that gains momentum. In this context, fundamental issues on what are and to which end do we use explanations have to be reconsidered. This article presents a prelimenary outline of the combination of two recently proposed classifications of explanations based on the type of the explanation itself and user goals which should be fulfilled. Further on, the contribution of the different knowledge containers for modeling the necessary knowledge is examined.},
  isbn = {978-3-540-31855-2},
  langid = {english}
}

@thesis{Roth2003CasebasedReasoningLaw,
  title = {Case-Based Reasoning in the Law: A Formal Theory of Reasoning by Case Comparison},
  shorttitle = {Case-Based Reasoning in the Law},
  author = {Roth, A. C.},
  date = {2003-01-01},
  institution = {Universiteit Maastricht},
  url = {https://cris.maastrichtuniversity.nl/portal/en/publications/casebased-reasoning-in-the-law--a-formal-theory-of-reasoning-by-case-comparison(432b58f7-61ec-4b35-b9d4-d8b8e1740dcf).html},
  urldate = {2019-10-28},
  langid = {english}
}

@inproceedings{Roush2020DebateSumLargescaleArgument,
  title = {{{DebateSum}}: {{A}} Large-Scale Argument Mining and Summarization Dataset},
  shorttitle = {{{DebateSum}}},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{Argument Mining}}},
  author = {Roush, Allen and Balaji, Arvind},
  date = {2020-12},
  pages = {1--7},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  url = {https://aclanthology.org/2020.argmining-1.1},
  urldate = {2022-01-13},
  abstract = {Prior work in Argument Mining frequently alludes to its potential applications in automatic debating systems. Despite this focus, almost no datasets or models exist which apply natural language processing techniques to problems found within competitive formal debate. To remedy this, we present the DebateSum dataset. DebateSum consists of 187,386 unique pieces of evidence with corresponding argument and extractive summaries. DebateSum was made using data compiled by competitors within the National Speech and Debate Association over a 7year period. We train several transformer summarization models to benchmark summarization performance on DebateSum. We also introduce a set of fasttext word-vectors trained on DebateSum called debate2vec. Finally, we present a search engine for this dataset which is utilized extensively by members of the National Speech and Debate Association today. The DebateSum search engine is available to the public here: http://www.debate.cards},
  eventtitle = {{{ArgMining-COLING}} 2020}
}

@inproceedings{Roush2024OpenDebateEvidenceMassiveScaleArgument,
  title = {{{OpenDebateEvidence}}: {{A Massive-Scale Argument Mining}} and {{Summarization Dataset}}},
  shorttitle = {{{OpenDebateEvidence}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Roush, Allen and Shabazz, Yusuf and Balaji, Arvind and Zhang, Peter and Mezza, Stefano and Zhang, Markus and Basu, Sanjay and Vishwanath, Sriram and Fatemi, Mehdi and Ziv, Ravid S.},
  date = {2024-12-16},
  volume = {37},
  pages = {34270--34293},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/3c630d28df1cff44314d5798f82e02ec-Abstract-Datasets_and_Benchmarks_Track.html},
  urldate = {2025-09-03},
  eventtitle = {{{NeurIPS}} 2024},
  langid = {english}
}

@inproceedings{Roussakis2022ExtendingArgQLSpecification,
  title = {Extending the {{ArgQL Specification}}},
  booktitle = {Proceedings of the 16th {{International Rule Challenge}} and 6th {{Doctoral Consortium}} @ {{RuleML}}+{{RR}} 2022},
  author = {Roussakis, Yannis and Flouris, Giorgos and Zografistou, Dimitra and Ymeralli, Elisjana},
  editor = {Arndt, Dörthe and Soylu, Ahmet and Vanthienen, Jan and Kharlamov, Evgeny and Steen, Alexander},
  date = {2022-09-19},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3229},
  publisher = {CEUR},
  location = {Berlin, Germany (virtual due to Covid-},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3229/#paper21},
  urldate = {2025-03-21},
  eventtitle = {International {{Rule Challenge}} and {{Doctoral Consortium}}},
  langid = {english}
}

@article{Rousseeuw1987SilhouettesGraphicalAid,
  title = {Silhouettes: {{A}} Graphical Aid to the Interpretation and Validation of Cluster Analysis},
  shorttitle = {Silhouettes},
  author = {Rousseeuw, Peter J.},
  date = {1987-11-01},
  journaltitle = {Journal of Computational and Applied Mathematics},
  shortjournal = {Journal of Computational and Applied Mathematics},
  volume = {20},
  pages = {53--65},
  issn = {0377-0427},
  doi = {10.1016/0377-0427(87)90125-7},
  url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
  urldate = {2022-08-02},
  abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.},
  langid = {english}
}

@inproceedings{Ruckdeschel2024ArgumentMiningAttack,
  title = {Argument {{Mining}} of~{{Attack}} and~{{Support Patterns}} in~{{Dialogical Conversations}} with~{{Sequential Pattern Mining}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Ruckdeschel, Mattes and Baumann, Ringo and Wiedemann, Gregor},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {39--56},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_3},
  abstract = {Argument mining usually operates on short, decontextualized argumentative units such as main and subordinate clauses, or full sentences as proxies for arguments. Argumentation in digital media environments, however, is embedded in larger contexts. Especially on social media platforms, argumentation unfolds in dialog threads or tree structures where users interact with each other. To reveal patterns of such interactions, we transform 2.5 million tweets from 38k German Twitter conversations concerning nuclear energy from 2017, 2019, and 2021 into an abstract representation encoding their stance, and aspects. We then apply Sequential Pattern Mining, a common method for finding patterns in large databases, and explore its capabilities to investigate typical argumentation schemes in user debates. The approach reveals distinct patterns of support and attack relations between pro and contra arguments about nuclear energy in conversational threads when comparing different time slices of our corpus. For example, we are seeing an increasing relevance of the climate aspect in attacks on anti-nuclear arguments. However, the pro arguments are increasingly being countered by cost aspects. Analyzing this diachronic change of patterns allows us to describe the discursive processes of argumentation on a macro level that drive the slow but steady transformation of a society’s social and political convictions.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@unpublished{Ruckle2018ConcatenatedPowerMean,
  title = {Concatenated {{Power Mean Word Embeddings}} as {{Universal Cross-Lingual Sentence Representations}}},
  author = {Rücklé, Andreas and Eger, Steffen and Peyrard, Maxime and Gurevych, Iryna},
  date = {2018-09-12},
  eprint = {1803.01400},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.01400},
  urldate = {2020-10-20},
  abstract = {Average word embeddings are a common baseline for more sophisticated sentence embedding techniques. However, they typically fall short of the performances of more complex models such as InferSent. Here, we generalize the concept of average word embeddings to power mean word embeddings. We show that the concatenation of different types of power mean word embeddings considerably closes the gap to state-of-the-art methods monolingually and substantially outperforms these more complex techniques cross-lingually. In addition, our proposed method outperforms different recently proposed baselines such as SIF and Sent2Vec by a solid margin, thus constituting a much harder-to-beat monolingual baseline. Our data and code are publicly available.}
}

@article{Ruiz-Dolz2021VivesDebateNewAnnotated,
  title = {{{VivesDebate}}: {{A New Annotated Multilingual Corpus}} of {{Argumentation}} in a {{Debate Tournament}}},
  shorttitle = {{{VivesDebate}}},
  author = {Ruiz-Dolz, Ramon and Nofre, Montserrat and Taulé, Mariona and Heras, Stella and García-Fornes, Ana},
  date = {2021-01},
  journaltitle = {Applied Sciences},
  volume = {11},
  number = {15},
  pages = {7160},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app11157160},
  url = {https://www.mdpi.com/2076-3417/11/15/7160},
  urldate = {2023-07-26},
  abstract = {The application of the latest Natural Language Processing breakthroughs in computational argumentation has shown promising results, which have raised the interest in this area of research. However, the available corpora with argumentative annotations are often limited to a very specific purpose or are not of adequate size to take advantage of state-of-the-art deep learning techniques (e.g., deep neural networks). In this paper, we present VivesDebate, a large, richly annotated and versatile professional debate corpus for computational argumentation research. The corpus has been created from 29 transcripts of a debate tournament in Catalan and has been machine-translated into Spanish and English. The annotation contains argumentative propositions, argumentative relations, debate interactions and professional evaluations of the arguments and argumentation. The presented corpus can be useful for research on a heterogeneous set of computational argumentation underlying tasks such as Argument Mining, Argument Analysis, Argument Evaluation or Argument Generation, among others. All this makes VivesDebate a valuable resource for computational argumentation research within the context of massive corpora aimed at Natural Language Processing tasks.},
  issue = {15},
  langid = {english}
}

@article{Ruiz-Dolz2024NLASmultiMultilingualCorpus,
  title = {{{NLAS-multi}}: {{A}} Multilingual Corpus of Automatically Generated {{Natural Language Argumentation Schemes}}},
  shorttitle = {{{NLAS-multi}}},
  author = {Ruiz-Dolz, Ramon and Taverner, Joaquin and Lawrence, John and Reed, Chris},
  date = {2024-12-01},
  journaltitle = {Data in Brief},
  shortjournal = {Data in Brief},
  volume = {57},
  pages = {111087},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2024.111087},
  url = {https://www.sciencedirect.com/science/article/pii/S2352340924010497},
  urldate = {2025-07-09},
  abstract = {Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated. To address these limitations, in this paper we present the following two contributions: an effective methodology for the automatic generation of natural language arguments in different topics and languages, and the largest publicly available corpus of Natural Language Argumentation Schemes available to date.}
}

@inproceedings{Ruiz-Dolz2025LookingUnseenEffective,
  title = {Looking at the {{Unseen}}: {{Effective Sampling}} of {{Non-Related Propositions}} for {{Argument Mining}}},
  shorttitle = {Looking at the {{Unseen}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}},
  author = {Ruiz-Dolz, Ramon and Gemechu, Debela and Kikteva, Zlata and Reed, Chris},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  date = {2025-01},
  pages = {2131--2143},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, UAE},
  url = {https://aclanthology.org/2025.coling-main.145/},
  urldate = {2025-07-28},
  abstract = {Traditionally, argument mining research has approached the task of automatic identification of argument structures by using existing definitions of what constitutes an argument, while leaving the equally important matter of what does not qualify as an argument unaddressed. With the ability to distinguish between what is and what is not a natural language argument being at the core of argument mining as a field, it is interesting that no previous work has explored approaches to effectively select non-related propositions (i.e., propositions that are not connected through an argumentative relation, such as support or attack) that improve the data for learning argument mining tasks better. In this paper, we address the question of how to effectively sample non-related propositions from six different argument mining corpora belonging to different domains and encompassing both monologue and dialogue forms of argumentation. To that end, in addition to considering undersampling baselines from previous work, we propose three new sampling strategies relying on context (i.e., short/long) and the semantic similarity between propositions. Our results indicate that using more informed sampling strategies improves the performance, not only when evaluating models on their respective test splits, but also in the case of cross-domain evaluation.},
  eventtitle = {{{COLING}} 2025}
}

@inproceedings{Ruiz-Dolz2025MiningComplexPatterns,
  title = {Mining {{Complex Patterns}} of {{Argumentative Reasoning}} in {{Natural Language Dialogue}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ruiz-Dolz, Ramon and Kikteva, Zlata and Lawrence, John},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {7421--7435},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.368/},
  urldate = {2025-07-28},
  abstract = {Argumentation scheme mining is the task of automatically identifying reasoning mechanisms behind argument inferences. These mechanisms provide insights into underlying argument structures and guide the assessment of natural language arguments. Research on argumentation scheme mining, however, has always been limited by the scarcity of large enough publicly available corpora containing scheme annotations. In this paper, we present the first state-of-the-art results for mining argumentation schemes in natural language dialogue. For this purpose, we create QT-Schemes, a new corpus of 441 arguments annotated with 24 argumentation schemes. Using this corpus, we leverage the capabilities of LLMs and Transformer-based models, pre-training them on a large corpus containing textbook-like argumentation schemes and validating their applicability in real-world scenarios.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@article{Russakovsky2015ImageNetLargeScale,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2015-12-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {115},
  number = {3},
  pages = {211--252},
  issn = {1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  url = {https://doi.org/10.1007/s11263-015-0816-y},
  urldate = {2025-07-15},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5~years of the challenge, and propose future directions and improvements.},
  langid = {english}
}

@inproceedings{Russell2025PeopleWhoFrequently,
  title = {People Who Frequently Use {{ChatGPT}} for Writing Tasks Are Accurate and Robust Detectors of {{AI-generated}} Text},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Russell, Jenna and Karpinska, Marzena and Iyyer, Mohit},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {5342--5373},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.267/},
  urldate = {2025-07-29},
  abstract = {In this paper, we study how well humans can detect text generated by commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such “expert” annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization. Qualitative analysis of the experts' free-form explanations shows that while they rely heavily on specific lexical clues (`AI vocabulary'), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors. We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Saadat-Yazdi2024RecognisingEntailmentFormalising,
  title = {Beyond {{Recognising Entailment}}: {{Formalising Natural Language Inference}} from an {{Argumentative Perspective}}},
  shorttitle = {Beyond {{Recognising Entailment}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Saadat-Yazdi, Ameer and Kökciyan, Nadin},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  date = {2024-08},
  pages = {9620--9636},
  publisher = {Association for Computational Linguistics},
  location = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.520},
  url = {https://aclanthology.org/2024.acl-long.520/},
  urldate = {2025-05-28},
  abstract = {In argumentation theory, argument schemes are a characterisation of stereotypical patterns of inference. There has been little work done to develop computational approaches to identify these schemes in natural language. Moreover, advancements in recognizing textual entailment lack a standardized definition of inference, which makes it challenging to compare methods trained on different datasets and rely on the generalisability of their results. In this work, we propose a rigorous approach to align entailment recognition with argumentation theory. Wagemans' Periodic Table of Arguments (PTA), a taxonomy of argument schemes, provides the appropriate framework to unify these two fields. To operationalise the theoretical model, we introduce a tool to assist humans in annotating arguments according to the PTA. Beyond providing insights into non-expert annotator training, we present Kialo-PTA24, the first multi-topic dataset for the PTA. Finally, we benchmark the performance of pre-trained language models on various aspects of argument analysis. Our experiments show that the task of argument canonicalisation poses a significant challenge for state-of-the-art models, suggesting an inability to represent argumentative reasoning and a direction for future investigation.},
  eventtitle = {{{ACL}} 2024}
}

@article{Sadiq2022ArgumentationFrameworksBrief,
  title = {Argumentation {{Frameworks}} – {{A Brief Review}}},
  author = {Sadiq, Ahmed T. and Abdulah, Hasanen S. and Kareem, Adnan Taher},
  date = {2022-02-16},
  journaltitle = {International Journal of Online and Biomedical Engineering (iJOE)},
  volume = {18},
  number = {02},
  pages = {55--70},
  issn = {2626-8493},
  doi = {10.3991/ijoe.v18i02.28023},
  url = {https://online-journals.org/index.php/i-joe/article/view/28023},
  urldate = {2025-09-07},
  abstract = {The main aim of this article is to provide a short review of the most important argumentation frameworks (AFs) systems being used. This paper presents the overall thought of unique argumentation, featuring the work way of these theoretical systems in the argumentation interaction and surveys the first Dung structures. It introduces how these systems give acceptable arguments by focused on the argumentation frameworks structures and how to deal with the arguments and the basic rules to give the result. Finally, it surveys the idea of theoretical rationalistic structures, quite possibly the broadest frameworks for dynamic argumentation, and gives a short description of several argumentation frameworks that are more famous.},
  langid = {english}
}

@thesis{Sahitaj2022InvestigatingExplainableArtificial,
  type = {mathesis},
  title = {Investigating {{Explainable Artificial Intelligence}} and {{Interaction}} for a {{Novel Argument Graph Mining System}}},
  author = {Sahitaj, Premtim},
  date = {2022-08-22},
  institution = {Trier University},
  location = {Trier, Germany},
  abstract = {Humans search for arguments to discuss ideas, influence others, or recommend adequate actions with the goal of persuasion. Current Web search engines primarily operate on the textual level and only retrieve individual facts which must be tediously assembled into a structured argument by hand. Anticipated argumentation machines (AM) execute queries on the additional structured level and retrieve argument structures assembled into graphs [8, 89]. Argument mining or the annotation of argument graph structures is a necessary preceding step towards the realization of AMs. Manual annotation is expensive, while automated annotation is error-prone. Current automated argument mining systems do not perform well [56, 103], do not allow user intervention, and lack transparency. The complexity of mining natural language arguments and the scarcity of information about the end-to-end mining process, necessitate an interactive hybrid approach that is faster than manual curation and more accurate than automated curation [122]. An interactive argument mining system requires informed user decisions which can only be based on explanations about the argument mining process and the intermediate results. With this thesis we target the conceptualization of an interactive and explainable argument mining system for the purpose of improving automated annotation results. We identify three aspects to this problem setting. First, we formulate the argument mining process tasks to integrate novel machine learning solution approaches while considering recent progress in the field of argument mining. Our learning-based approaches for the tasks of argument extraction, relation classification, and major claim detection suggest a significant improvement in the field of argument mining, which demonstrates the importance of our selected strategies. Then, we describe an interactive argument mining system that encapsulates the tasks of the argument mining process for the integration of users as a final control authority. Relevant explanation characteristics are captured and utilized in an target audience interview to elicit trends about explanations requirements from the participants. Based on these user insights we conceptualize task-specific and model-agnostic explanations which are primarily build upon the categories of explanation by visualization, local change, example, simplification, and feature relevance. Finally, the target audience feedback is collected and evaluated against our proposed explanation concept where we identify encouraging support for the majority of our ideas.},
  langid = {english},
  pagetotal = {151}
}

@inproceedings{Sahitaj2025HybridAnnotationPropaganda,
  title = {Hybrid {{Annotation}} for {{Propaganda Detection}}: {{Integrating LLM Pre-Annotations}} with {{Human Intelligence}}},
  shorttitle = {Hybrid {{Annotation}} for {{Propaganda Detection}}},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{NLP}} for {{Positive Impact}} ({{NLP4PI}})},
  author = {Sahitaj, Ariana and Sahitaj, Premtim and Solopova, Veronika and Li, Jiaao and Möller, Sebastian and Schmitt, Vera},
  editor = {Atwell, Katherine and Biester, Laura and Borah, Angana and Dementieva, Daryna and Ignat, Oana and Kotonya, Neema and Liu, Ziyi and Wan, Ruyuan and Wilson, Steven and Zhao, Jieyu},
  date = {2025-07},
  pages = {215--228},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.nlp4pi-1.18/},
  urldate = {2025-07-28},
  abstract = {Propaganda detection on social media remains challenging due to task complexity and limited high-quality labeled data. This paper introduces a novel framework that combines human expertise with Large Language Model (LLM) assistance to improve both annotation consistency and scalability. We propose a hierarchical taxonomy that organizes 14 fine-grained propaganda techniques (CITATION) into three broader categories, conduct a human annotation study on the HQP dataset (CITATION) that reveals low inter-annotator agreement for fine-grained labels, and implement an LLM-assisted pre-annotation pipeline that extracts propagandistic spans, generates concise explanations, and assigns local labels as well as a global label. A secondary human verification study shows significant improvements in both agreement and time-efficiency. Building on this, we fine-tune smaller language models (SLMs) to perform structured annotation. Instead of fine-tuning on human annotations, we train on high-quality LLM-generated data, allowing a large model to produce these annotations and a smaller model to learn to generate them via knowledge distillation. Our work contributes towards the development of scalable and robust propaganda detection systems, supporting the idea of transparent and accountable media ecosystems in line with SDG 16. The code is publicly available at our GitHub repository.},
  eventtitle = {Fourth {{Workshop}} on {{NLP}} for {{Positive Impact}} ({{NLP4PI}})},
  isbn = {978-1-959429-19-7}
}

@article{Salton1975VectorSpaceModel,
  title = {A Vector Space Model for Automatic Indexing},
  author = {Salton, G. and Wong, A. and Yang, C. S.},
  date = {1975-11-01},
  journaltitle = {Commun. ACM},
  volume = {18},
  number = {11},
  pages = {613--620},
  issn = {0001-0782},
  doi = {10.1145/361219.361220},
  url = {https://dl.acm.org/doi/10.1145/361219.361220},
  urldate = {2025-09-10},
  abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.}
}

@article{Salton1988TermweightingApproachesAutomatic,
  title = {Term-Weighting Approaches in Automatic Text Retrieval},
  author = {Salton, Gerard and Buckley, Christopher},
  date = {1988-01-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {24},
  number = {5},
  pages = {513--523},
  issn = {0306-4573},
  doi = {10.1016/0306-4573(88)90021-0},
  url = {http://www.sciencedirect.com/science/article/pii/0306457388900210},
  urldate = {2018-09-01},
  abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.}
}

@inproceedings{Samoladas2008SQOOSSQualityModel,
  title = {The {{SQO-OSS Quality Model}}: {{Measurement Based Open Source Software Evaluation}}},
  shorttitle = {The {{SQO-OSS Quality Model}}},
  booktitle = {Open {{Source Development}}, {{Communities}} and {{Quality}}},
  author = {Samoladas, Ioannis and Gousios, Georgios and Spinellis, Diomidis and Stamelos, Ioannis},
  editor = {Russo, Barbara and Damiani, Ernesto and Hissam, Scott and Lundell, Björn and Succi, Giancarlo},
  date = {2008},
  series = {{{IFIP}} – {{The International Federation}} for {{Information Processing}}},
  pages = {237--248},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-0-387-09684-1_19},
  abstract = {Software quality evaluation has always been an important part of software business. The quality evaluation process is usually based on hierarchical quality models that measure various aspects of software quality and deduce a characterization of the product quality being evaluated. The particular nature of open source software has rendered existing models inappropriate for detailed quality evaluations. In this paper, we present a hierarchical quality model that evaluates source code and community processes, based on automatic calculation of metric values and their correlation to a set of predefined quality profiles.1},
  isbn = {978-0-387-09684-1},
  langid = {english}
}

@inproceedings{Santacruz2017NodeMatchingComputation,
  title = {Node {{Matching Computation Between Two Large Graphs}} in {{Linear Computational Cost}}},
  booktitle = {Graph-{{Based Representations}} in {{Pattern Recognition}}},
  author = {Santacruz, Pep and Algabli, Shaima and Serratosa, Francesc},
  editor = {Foggia, Pasquale and Liu, Cheng-Lin and Vento, Mario},
  date = {2017},
  pages = {143--153},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-58961-9_13},
  abstract = {Error-tolerant graph matching has been demonstrated to be an NP-problem, for this reason, several sub-optimal algorithms have been presented with the aim of making the runtime acceptable in some applications. Some well-known sub-optimal algorithms have 6th, cubic or quadratic cost with respect to the order of the graphs. When applications deal with large graphs (social nets), these costs are not acceptable. For this reason, we present an error-tolerant graph-matching algorithm that it is linear with respect to the order of the graphs. Our method needs an initial seed, which is composed of one or several node-to-node mappings. The algorithm has been applied to analyse the friendship variability of social nets.},
  isbn = {978-3-319-58961-9},
  langid = {english}
}

@inproceedings{Saracevic1995EvaluationEvaluationInformation,
  title = {Evaluation of Evaluation in Information Retrieval},
  booktitle = {Proceedings of the 18th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Saracevic, Tefko},
  date = {1995-07-01},
  series = {{{SIGIR}} '95},
  pages = {138--146},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/215206.215351},
  url = {https://doi.org/10.1145/215206.215351},
  urldate = {2021-03-13},
  isbn = {978-0-89791-714-8}
}

@article{Sarker2021NeurosymbolicArtificialIntelligence,
  title = {Neuro-Symbolic Artificial Intelligence: {{Current}} Trends},
  shorttitle = {Neuro-Symbolic Artificial Intelligence},
  author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
  date = {2021-01-01},
  journaltitle = {AI Communications},
  volume = {34},
  number = {3},
  pages = {197--209},
  publisher = {SAGE Publications},
  issn = {0921-7126},
  doi = {10.3233/AIC-210084},
  url = {https://journals.sagepub.com/action/showAbstract},
  urldate = {2025-07-07},
  abstract = {Neuro-Symbolic Artificial Intelligence – the combination of symbolic methods with methods that are based on artificial neural networks – has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.},
  langid = {english}
}

@article{Sauer2020UsabilityUserExperience,
  title = {Usability, User Experience and Accessibility: Towards an Integrative Model},
  shorttitle = {Usability, User Experience and Accessibility},
  author = {Sauer, Juergen and Sonderegger, Andreas and Schmutz, Sven},
  date = {2020-10-02},
  journaltitle = {Ergonomics},
  volume = {63},
  number = {10},
  eprint = {32450782},
  eprinttype = {pubmed},
  pages = {1207--1220},
  publisher = {Taylor \& Francis},
  issn = {0014-0139},
  doi = {10.1080/00140139.2020.1774080},
  url = {https://doi.org/10.1080/00140139.2020.1774080},
  urldate = {2022-05-03},
  abstract = {Within the field of ergonomics, the concepts of usability, user experience and accessibility have played an increasingly important role. The present paper examined the meaning of these concepts and their relationship to each other, which included an analysis of the definitions, methods, and typical outcome measures employed. Despite some concerns in the literature about the utility of usability, user experience and accessibility as umbrella terms, we provide arguments for their continued use. The article proposes how the three concepts and their different perspectives can be integrated. We propose the term ‘interaction experience’ (IX) as a higher-level concept. Due to the multi-facetted nature of umbrella concepts, we suggest using spider charts as a means to report the results of evaluating artefacts with regard to usability, user experience and accessibility. Practitioner Summary: A better integration of the concepts of usability, user experience and accessibility is expected to provide some benefits to practitioners. We propose employing spider charts for reporting the outcome of artefact evaluations regarding the three concepts. This may help practitioners interpret the characteristics of a device at a glance. Abbreviations: IX: interaction experience; UX: user experience; ISO: International Standard Organisation}
}

@inproceedings{Schaefer2020AnnotationDetectionArguments,
  title = {Annotation and {{Detection}} of {{Arguments}} in {{Tweets}}},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{Argument Mining}}},
  author = {Schaefer, Robin and Stede, Manfred},
  date = {2020-12},
  pages = {53--58},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  url = {https://aclanthology.org/2020.argmining-1.6},
  urldate = {2022-01-13},
  abstract = {Notwithstanding the increasing role Twitter plays in modern political and social discourse, resources built for conducting argument mining on tweets remain limited. In this paper, we present a new corpus of German tweets annotated for argument components. To the best of our knowledge, this is the first corpus containing not only annotated full tweets but also argumentative spans within tweets. We further report first promising results using supervised classification (F1: 0.82) and sequence labeling (F1: 0.72) approaches.},
  eventtitle = {{{ArgMining-COLING}} 2020}
}

@article{Schaefer2021ArgumentMiningTwitter,
  title = {Argument {{Mining}} on {{Twitter}}: {{A}} Survey},
  shorttitle = {Argument {{Mining}} on {{Twitter}}},
  author = {Schaefer, Robin and Stede, Manfred},
  date = {2021-02-01},
  journaltitle = {it - Information Technology},
  volume = {63},
  number = {1},
  pages = {45--58},
  publisher = {De Gruyter Oldenbourg},
  issn = {2196-7032},
  doi = {10.1515/itit-2020-0053},
  url = {https://www.degruyter.com/document/doi/10.1515/itit-2020-0053/html},
  urldate = {2021-10-07},
  abstract = {In the last decade, the field of argument mining has grown notably. However, only relatively few studies have investigated argumentation in social media and specifically on Twitter. Here, we provide the, to our knowledge, first critical in-depth survey of the state of the art in tweet-based argument mining. We discuss approaches to modelling the structure of arguments in the context of tweet corpus annotation, and we review current progress in the task of detecting argument components and their relations in tweets. We also survey the intersection of argument mining and stance detection, before we conclude with an outlook.},
  langid = {english}
}

@inproceedings{Schaefer2025IntegratingLLMsArgument,
  title = {On {{Integrating LLMs Into}} an {{Argument Annotation Workflow}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Schaefer, Robin},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {87--99},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.8/},
  urldate = {2025-07-28},
  abstract = {Given the recent success of LLMs across different NLP tasks, their usability for data annotation has become a promising area of research. In this work, we investigate to what extent LLMs can be used as annotators for argument components and their semantic types in German tweets through a series of experiments combining different models and prompt configurations. Each prompt is constructed from modular components, such as class definitions or contextual information. Our results suggest that LLMs can indeed perform argument annotation, particularly of semantic argument types, if provided with precise class definitions. However, a fine-tuned BERT baseline remains a strong contender, often matching or exceeding LLM performance. These findings highlight the importance of considering not only model performance, but also ecological and financial costs when defining an annotation workflow.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@article{Scheuer2010ComputersupportedArgumentationReview,
  title = {Computer-Supported Argumentation: {{A}} Review of the State of the Art},
  shorttitle = {Computer-Supported Argumentation},
  author = {Scheuer, Oliver and Loll, Frank and Pinkwart, Niels and McLaren, Bruce M.},
  date = {2010-03-01},
  journaltitle = {International Journal of Computer-Supported Collaborative Learning},
  shortjournal = {Computer Supported Learning},
  volume = {5},
  number = {1},
  pages = {43--102},
  issn = {1556-1615},
  doi = {10.1007/s11412-009-9080-x},
  url = {https://doi.org/10.1007/s11412-009-9080-x},
  urldate = {2022-04-21},
  abstract = {Argumentation is an important skill to learn. It is valuable not only in many professional contexts, such as the law, science, politics, and business, but also in everyday life. However, not many people are good arguers. In response to this, researchers and practitioners over the past 15–20~years have developed software tools both to support and teach argumentation. Some of these tools are used in individual fashion, to present students with the “rules” of argumentation in a particular domain and give them an opportunity to practice, while other tools are used in collaborative fashion, to facilitate communication and argumentation between multiple, and perhaps distant, participants. In this paper, we review the extensive literature on argumentation systems, both individual and collaborative, and both supportive and educational, with an eye toward particular aspects of the past work. More specifically, we review the types of argument representations that have been used, the various types of interaction design and ontologies that have been employed, and the system architecture issues that have been addressed. In addition, we discuss intelligent and automated features that have been imbued in past systems, such as automatically analyzing the quality of arguments and providing intelligent feedback to support and/or tutor argumentation. We also discuss a variety of empirical studies that have been done with argumentation systems, including, among other aspects, studies that have evaluated the effect of argument diagrams (e.g., textual versus graphical), different representations, and adaptive feedback on learning argumentation. Finally, we conclude by summarizing the “lessons learned” from this large and impressive body of work, particularly focusing on lessons for the CSCL research community and its ongoing efforts to develop computer-mediated collaborative argumentation systems.},
  langid = {english}
}

@inproceedings{Schuler2023SemisupervisedSimilarityLearning,
  title = {Semi-Supervised {{Similarity Learning}} in~{{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Artificial {{Intelligence XL}}},
  author = {Schuler, Nicolas and Hoffmann, Maximilian and Beise, Hans-Peter and Bergmann, Ralph},
  editor = {Bramer, Max and Stahl, Frederic},
  date = {2023},
  pages = {159--173},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-47994-6_12},
  abstract = {Supervised learning is typically challenging with insufficient amounts of labeled training data and high costs for label acquisition, creating a demand for unsupervised learning methods. In the research area of Process-Oriented Case-Based Reasoning (POCBR), this demand is created by training data that is manually-modeled and computationally-expensive labeling methods. In this paper, we propose a semi-supervised transfer learning method for learning similarities between pairs of semantic graphs in POCBR with Graph Neural Networks (GNNs). The method aims to replace the fully supervised learning procedure from previous work with an unsupervised and a supervised training phase. In the first phase, the GNNs are pretrained with a triplet learning procedure that utilizes graph augmentation and random selection to enable unsupervised training. This phase is followed by a supervised one where the pretrained model is trained on the original labeled training data. The experimental evaluation examines the quality of the semi-supervised models compared to the supervised models from previous work for three semantic graph domains with different properties. The results indicate the potential of the proposed approach for improving retrieval quality.},
  isbn = {978-3-031-47994-6},
  langid = {english}
}

@online{Schulhoff2025PromptReportSystematic,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompt Engineering Techniques}}},
  shorttitle = {The {{Prompt Report}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Costa, Hevander Da and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  date = {2025-02-26},
  eprint = {2406.06608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.06608},
  url = {http://arxiv.org/abs/2406.06608},
  urldate = {2025-09-09},
  abstract = {Generative Artificial Intelligence (GenAI) systems are increasingly being deployed across diverse industries and research domains. Developers and end-users interact with these systems through the use of prompting and prompt engineering. Although prompt engineering is a widely adopted and extensively researched area, it suffers from conflicting terminology and a fragmented ontological understanding of what constitutes an effective prompt due to its relatively recent emergence. We establish a structured understanding of prompt engineering by assembling a taxonomy of prompting techniques and analyzing their applications. We present a detailed vocabulary of 33 vocabulary terms, a taxonomy of 58 LLM prompting techniques, and 40 techniques for other modalities. Additionally, we provide best practices and guidelines for prompt engineering, including advice for prompting state-of-the-art (SOTA) LLMs such as ChatGPT. We further present a meta-analysis of the entire literature on natural language prefix-prompting. As a culmination of these efforts, this paper presents the most comprehensive survey on prompt engineering to date.},
  pubstate = {prepublished}
}

@online{Schuller2022ComposingComplexHybrid,
  title = {Composing {{Complex}} and {{Hybrid AI Solutions}}},
  author = {Schüller, Peter and Costeira, João Paolo and Crowley, James and Grosinger, Jasmin and Ingrand, Félix and Köckemann, Uwe and Saffiotti, Alessandro and Welss, Martin},
  date = {2022-02-25},
  eprint = {2202.12566},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.12566},
  url = {http://arxiv.org/abs/2202.12566},
  urldate = {2023-11-20},
  abstract = {Progress in several areas of computer science has been enabled by comfortable and efficient means of experimentation, clear interfaces, and interchangable components, for example using OpenCV for computer vision or ROS for robotics. We describe an extension of the Acumos system towards enabling the above features for general AI applications. Originally, Acumos was created for telecommunication purposes, mainly for creating linear pipelines of machine learning components. Our extensions include support for more generic components with gRPC/Protobuf interfaces, automatic orchestration of graphically assembled solutions including control loops, sub-component topologies, and event-based communication,and provisions for assembling solutions which contain user interfaces and shared storage areas. We provide examples of deployable solutions and their interfaces. The framework is deployed at http://aiexp.ai4europe.eu/ and its source code is managed as an open source Eclipse project.},
  pubstate = {prepublished}
}

@thesis{Schultheis2022ErklaerungAehnlichkeitenIm,
  type = {mathesis},
  title = {Erklärung von Ähnlichkeiten im Prozessorientierten Fallbasierten Schließen durch Visualisierungen},
  author = {Schultheis, Alexander},
  date = {2022-09-16},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {ngerman},
  pagetotal = {103}
}

@inproceedings{Schultheis2023ExplanationSimilaritiesProcessOriented,
  title = {Explanation of~{{Similarities}} in~{{Process-Oriented Case-Based Reasoning}} by~{{Visualization}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Schultheis, Alexander and Hoffmann, Maximilian and Malburg, Lukas and Bergmann, Ralph},
  editor = {Massie, Stewart and Chakraborti, Sutanu},
  date = {2023},
  pages = {53--68},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-40177-0_4},
  abstract = {Modeling similarity measures in Case-Based Reasoning is a knowledge-intensive, demanding, and error-prone task even for domain experts. Visualizations offer support for users, but are currently only available for certain subdomains and case representations. Currently, there are only visualizations that can be used for local attributes or specific case representations. However, there is no possibility to visualize similarities between complete processes accordingly so far, although complex domains may be present. Therefore, an extension of existing approaches or the design of new suitable concepts for this application domain is necessary. The contribution of this work is to enable a more profound understanding of similarity for knowledge engineers who create a similarity model and support them in this task by using visualization methods in Process-Oriented Case-Based Reasoning (POCBR). For this purpose, we present related approaches and evaluate them against derived requirements for visualizations in POCBR. On this basis, suitable visualizations are further developed as well as new approaches designed. Three such visualizations are created: (1) a graph mapping approach, (2) a merge graph, and (3)~a visualization based on heatmaps. An evaluation of these approaches has been performed based on the requirements in which the domain experts determine the graph-mapping visualization as best-suited for engineering of similarity models.},
  isbn = {978-3-031-40177-0},
  langid = {english}
}

@inproceedings{Schultheis2023OverviewComparisonCaseBased,
  title = {An {{Overview}} and~{{Comparison}} of~{{Case-Based Reasoning Frameworks}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Schultheis, Alexander and Zeyen, Christian and Bergmann, Ralph},
  editor = {Massie, Stewart and Chakraborti, Sutanu},
  date = {2023},
  pages = {327--343},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-40177-0_21},
  abstract = {Case-Based Reasoning (CBR) is a methodology with many applications in industrial and scientific domains. Over the past decades, various frameworks have been developed to facilitate the development of CBR applications. For practitioners and researchers, it is challenging to overview the landscape of existing frameworks with their specific scope and features. This makes it difficult to choose the most suitable framework for specific requirements. To address this issue, this work provides an overview and comparison of CBR frameworks, focusing on five recent, open-source CBR frameworks: CloodCBR, eXiT*CBR, jColibri, myCBR, and ProCAKE. They are compared by supported CBR types, knowledge containers, CBR phases, interfaces, and special features.},
  isbn = {978-3-031-40177-0},
  langid = {english}
}

@article{Schultheis2025EASYEnergyEfficientAnalysis,
  title = {{{EASY}}: {{Energy-Efficient Analysis}} and {{Control Processes}} in the {{Dynamic Edge-Cloud Continuum}} for {{Industrial Manufacturing}}},
  shorttitle = {{{EASY}}},
  author = {Schultheis, Alexander and Alt, Benjamin and Bast, Sebastian and Guldner, Achim and Jilg, David and Katic, Darko and Mundorf, Johannes and Schlagenhauf, Tobias and Weber, Sebastian and Bergmann, Ralph and Bergweiler, Simon and Creutz, Lars and Dartmann, Guido and Malburg, Lukas and Naumann, Stefan and Rezapour, Mahdi and Ruskowski, Martin},
  date = {2025-06-01},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {39},
  number = {2},
  pages = {161--166},
  issn = {1610-1987},
  doi = {10.1007/s13218-024-00868-3},
  url = {https://doi.org/10.1007/s13218-024-00868-3},
  urldate = {2025-08-26},
  abstract = {According to the guiding principles of Industry~4.0, edge computing enables the data-sovereign and near-real-time processing of data directly at the point of origin. Using these edge devices in manufacturing organization will drive the use of industrial analysis, control, and Artificial Intelligence (AI) applications close to production. The goal of the EASY project is to make the added value of edge computing available by providing an easily usable Edge-Cloud Continuum with a runtime environment and services for the execution of AI-based Analysis and Control processes. Within this continuum, a dynamic, distributed, and optimized execution of services is automated across the entire spectrum from centralized cloud to decentralized edge instances to increase productivity and resource efficiency.},
  langid = {english}
}

@article{Schulz2011TreevisnetTreeVisualization,
  title = {Treevis.Net: {{A Tree Visualization Reference}}},
  shorttitle = {Treevis.Net},
  author = {Schulz, Hans-Jorg},
  date = {2011-11},
  journaltitle = {IEEE Computer Graphics and Applications},
  volume = {31},
  number = {6},
  pages = {11--15},
  issn = {1558-1756},
  doi = {10.1109/MCG.2011.103},
  url = {https://ieeexplore.ieee.org/document/6056510},
  urldate = {2025-02-13},
  abstract = {Tree visualization is one of the best-studied areas of information visualization; researchers have developed more than 200 visualization and layout techniques for trees. The treevis.net project aims to provide a hand-curated bibliographical reference to this ever-growing wealth of techniques. It offers a visual overview that users can filter to a desired subset along the design criteria of dimensionality, edge representation, and node alignment. Details, including links to the original publications, can be brought up on demand. Treevis.net has become a community effort, with researchers sending in preprints of their tree visualization techniques to be published or pointing out additional information.},
  eventtitle = {{{IEEE Computer Graphics}} and {{Applications}}}
}

@article{Scott1955ReliabilityContentAnalysis,
  title = {Reliability of {{Content Analysis}}:{{The Case}} of {{Nominal Scale Coding}}},
  shorttitle = {Reliability of {{Content Analysis}}},
  author = {Scott, William A.},
  date = {1955-01-01},
  journaltitle = {Public Opinion Quarterly},
  shortjournal = {Public Opinion Quarterly},
  volume = {19},
  number = {3},
  pages = {321--325},
  issn = {0033-362X},
  doi = {10.1086/266577},
  url = {https://doi.org/10.1086/266577},
  urldate = {2021-03-07}
}

@article{Seiger2022IntegratingProcessManagement,
  title = {Integrating Process Management and Event Processing in Smart Factories: {{A}} Systems Architecture and Use Cases},
  shorttitle = {Integrating Process Management and Event Processing in Smart Factories},
  author = {Seiger, Ronny and Malburg, Lukas and Weber, Barbara and Bergmann, Ralph},
  date = {2022-04-01},
  journaltitle = {Journal of Manufacturing Systems},
  shortjournal = {Journal of Manufacturing Systems},
  volume = {63},
  pages = {575--592},
  issn = {0278-6125},
  doi = {10.1016/j.jmsy.2022.05.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0278612522000814},
  urldate = {2024-03-20},
  abstract = {The developments of new concepts for an increased digitization of manufacturing industries in the context of Industry 4.0 have brought about novel system architectures and frameworks for smart production systems. These range from generic frameworks for Industry 4.0 to domain-specific architectures for Industrial Internet of Things (IIoT). While most of the approaches include a service-based architecture for selective integration with enterprise systems, a close two-way integration of the production control systems and IIoT sensors and actuators with Process-Aware Information Systems (PAIS) on the management level for automation and mining of production processes is rarely discussed. This fusion of Business Process Management (BPM) with IIoT can be mutually beneficial for both research areas, but is still in its infancy. We propose a systems architecture for IIoT that shows how to integrate the low-level hardware components–sensors and actuators–of a smart factory with BPM systems. We discuss the software components and their interactions to address challenges of device encapsulation, integration of sensor events, and interaction with existing BPM systems. This integration is demonstrated within several use cases regarding process modeling, automation and mining for a smart factory model, showing benefits of using BPM technologies to analyze, control, and adapt discrete production processes in IIoT.}
}

@article{Sein2011ActionDesignResearch,
  title = {Action {{Design Research}}},
  author = {Sein, Maung K. and Henfridsson, Ola and Purao, Sandeep and Rossi, Matti and Lindgren, Rikard},
  date = {2011},
  journaltitle = {MIS Quarterly},
  volume = {35},
  number = {1},
  eprint = {23043488},
  eprinttype = {jstor},
  pages = {37--56},
  publisher = {Management Information Systems Research Center, University of Minnesota},
  issn = {0276-7783},
  doi = {10.2307/23043488},
  url = {https://www.jstor.org/stable/23043488},
  urldate = {2025-09-16},
  abstract = {Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers' intent. We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of ADR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.}
}

@inproceedings{Sen2024CounterfactualBasedSyntheticCase,
  title = {Counterfactual-{{Based Synthetic Case Generation}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Sen, Anik and Mainali, Mallika and Rauch, Christopher B. and Addison, Ursula and Floyd, Michael W. and Goel, Prateek and Karneeb, Justin and Kulhanek, Ray and Larue, Othalia and Ménager, David and Molineaux, Matthew and family=Turner, given=JT, given-i=JT and Weber, Rosina O.},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {388--403},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_25},
  abstract = {Case augmentation is often desirable when applying case-based reasoning to real-world problems. Initially explored for explainability, counterfactuals were recently recommended as a strategy to augment data. In this work, we implement an existing approach for generating counterfactuals, propose one variant of the original approach, and propose a third approach based on the literature on algorithmic recourse. We apply these three approaches to two datasets in military medical triage. To assess generalization, we also examine one of our approaches on three publicly available datasets. We compare the approaches based on the number of counterfactuals they produce, their resulting accuracy, overlapping counterfactuals, and domain knowledge. Experimental results are encouraging for the proposed approaches and bring up opportunities for future research.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@unpublished{Shazeer2016SwivelImprovingEmbeddings,
  title = {Swivel: {{Improving Embeddings}} by {{Noticing What}}'s {{Missing}}},
  shorttitle = {Swivel},
  author = {Shazeer, Noam and Doherty, Ryan and Evans, Colin and Waterson, Chris},
  date = {2016-02-05},
  eprint = {1602.02215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {We present Submatrix-wise Vector Embedding Learner (Swivel), a method for generating low-dimensional feature embeddings from a feature co-occurrence matrix. Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. While this requires computation proportional to the size of the entire matrix, we make use of vectorized multiplication to process thousands of rows and columns at once to compute millions of predicted values. Furthermore, we partition the matrix into shards in order to parallelize the computation across many nodes. This approach results in more accurate embeddings than can be achieved with methods that consider only observed co-occurrences, and can scale to much larger corpora than can be handled with sampling methods.}
}

@unpublished{Shevtsov2020AnalysisTwitterYouTube,
  title = {Analysis of {{Twitter}} and {{YouTube}} during {{USelections}} 2020},
  author = {Shevtsov, Alexander and Oikonomidou, Maria and Antonakaki, Despoina and Pratikakis, Polyvios and Ioannidis, Sotiris},
  date = {2020-11-10},
  eprint = {2010.08183},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.08183},
  urldate = {2022-02-06},
  abstract = {The presidential elections in the United States on 3 November 2020 have caused extensive discussions on social media. A part of the content on US elections is organic, coming from users discussing their opinions of the candidates, political positions, or relevant content presented on television. Another significant part of the content generated originates from organized campaigns, both official and by astroturfing. In this study, we obtain approximately 17.5M tweets containing 3M users, based on prevalent hashtags related to US election 2020, as well as the related YouTube links, contained in the Twitter dataset, likes, dislikes and comments of the videos and conduct volume, sentiment and graph analysis on the communities formed. Particularly, we study the daily traffic per prevalent hashtags, plot the retweet graph from July to September 2020, show how its main connected component becomes denser in the period closer to the elections and highlight the two main entities ('Biden' and 'Trump'). Additionally, we gather the related YouTube links contained in the previous dataset and perform sentiment analysis. The results on sentiment analysis on the Twitter corpus and the YouTube metadata gathered, show the positive and negative sentiment for the two entities throughout this period. The results of sentiment analysis indicate that 45.7\% express positive sentiment towards Trump in Twitter and 33.8\% positive sentiment towards Biden, while 14.55\% of users express positive sentiment in YouTube metadata gathered towards Trump and 8.7\% positive sentiment towards Biden. Our analysis fill the gap between the connection of offline events and their consequences in social media by monitoring important events in real world and measuring public volume and sentiment before and after the event in social media.}
}

@book{Simari2009ArgumentationArtificialIntelligence,
  title = {Argumentation in {{Artificial Intelligence}}},
  editor = {Simari, Guillermo and Rahwan, Iyad},
  date = {2009},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-0-387-98197-0},
  url = {https://link.springer.com/10.1007/978-0-387-98197-0},
  urldate = {2025-09-07},
  isbn = {978-0-387-98196-3 978-0-387-98197-0},
  langid = {english}
}

@article{Simpson2018FindingConvincingArguments,
  title = {Finding {{Convincing Arguments Using Scalable Bayesian Preference Learning}}},
  author = {Simpson, Edwin and Gurevych, Iryna},
  date = {2018},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {6},
  pages = {357--371},
  doi = {10.1162/tacl_a_00026},
  url = {https://www.aclweb.org/anthology/Q18-1026},
  urldate = {2019-11-21},
  abstract = {We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard ratings or rankings. In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification. Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments. One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning. We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data. We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains. While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.}
}

@inproceedings{Singh2002OpenMindCommon,
  title = {Open {{Mind Common Sense}}: {{Knowledge Acquisition}} from the {{General Public}}},
  shorttitle = {Open {{Mind Common Sense}}},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}} 2002: {{CoopIS}}, {{DOA}}, and {{ODBASE}}},
  author = {Singh, Push and Lin, Thomas and Mueller, Erik T. and Lim, Grace and Perkins, Travell and Li Zhu, Wan},
  editor = {Meersman, Robert and Tari, Zahir},
  date = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1223--1237},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-36124-3_77},
  abstract = {Open Mind Common Sense is a knowledge acquisition system designed to acquire commonsense knowledge from the general public over the web. We describe and evaluate our first fielded system, which enabled the construction of a 450,000 assertion commonsense knowledge base. We then discuss how our second-generation system addresses weaknesses discovered in the first. The new system acquires facts, descriptions, and stories by allowing participants to construct and fill in natural language templates. It employs word-sense disambiguation and methods of clarifying entered knowledge, analogical inference to provide feedback, and allows participants to validate knowledge and in turn each other.},
  isbn = {978-3-540-36124-4},
  langid = {english}
}

@inproceedings{Singh2002PublicAcquisitionCommonsense,
  title = {The Public Acquisition of Commonsense Knowledge},
  booktitle = {Proceedings of {{AAAI Spring Symposium}}: {{Acquiring}} (and {{Using}}) {{Linguistic}} (and {{World}}) {{Knowledge}} for {{Information Access}}},
  author = {Singh, Push},
  date = {2002},
  abstract = {The Open Mind Common Sense project is an attempt to construct a database of commonsense knowledge through the collaboration of a distributed community of thousands of non-expert netizens. We give an overview of the project, describe our knowledge acquisition and representation strategy of using natural language rather than formal logic, and demonstrate this strategy with a search engine application that employs simple commonsense reasoning to reformulate problem queries into more effective solution queries.}
}

@incollection{Sizov2014AcquisitionReuseReasoning,
  title = {Acquisition and {{Reuse}} of {{Reasoning Knowledge}} from {{Textual Cases}} for {{Automated Analysis}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Sizov, Gleb and Öztürk, Pinar and Štyrák, Jozef},
  editor = {Lamontagne, Luc and Plaza, Enric},
  date = {2014},
  volume = {8765},
  pages = {465--479},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11209-1_33},
  url = {http://link.springer.com/10.1007/978-3-319-11209-1_33},
  urldate = {2019-08-20},
  abstract = {Analysis is essential for solving complex problems such as diagnosing a patient, investigating an accident or predicting the outcome of a legal case. It is a non-trivial process even for human experts. To assist experts in this process we propose a CBR-based approach for automated problem analysis. In this approach a new problem is analysed by reusing reasoning knowledge from the analysis of a similar problem. To avoid the laborious process of manual case acquisition, the reasoning knowledge is extracted automatically from text and captured in a graph-based representation, which we dubbed Text Reasoning Graph (TRG), that consists of causal, entailment and paraphrase relations. The reuse procedure involves adaptation of a similar past analysis to a new problem by finding paths in TRG that connect the evidence in the new problem to conclusions of the past analysis. The objective is to generate the best explanation of how the new evidence connects to the conclusion. For evaluation, we built a system for analysing aircraft accidents based on the collection of aviation investigation reports. The evaluation results show that our reuse method increases the precision of the retrieved conclusions.},
  isbn = {978-3-319-11208-4 978-3-319-11209-1},
  langid = {english}
}

@inproceedings{Sizov2016CompositionalAdaptationExplanations,
  title = {Compositional {{Adaptation}} of {{Explanations}} in {{Textual Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Sizov, Gleb and Öztürk, Pinar and Marsi, Erwin},
  editor = {Goel, Ashok and Díaz-Agudo, M Belén and Roth-Berghofer, Thomas},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {387--401},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-47096-2_26},
  abstract = {When problem solving systems are deployed in real life, it is usually not enough to provide only a solution without any explanation. Users need an explanation in order to trust the system’s decisions. At the same time, explanations may also function internally in the system’s own reasoning process. One way to come up with an explanation for a new problem is to adapt an explanation from a similar problem encountered earlier, which is the idea behind the case-based explanation approach introduced by [29]. The original approach relies on manual construction of cases with explanations, which is difficult to scale up. In earlier work, therefore, we developed a system for automatic acquisition of cases with explanations from textual reports, including retrieval and adaptation of such cases [32, 33]. In this paper, we improve the adaptation method by combining explanations from more than one case, which we call compositional adaptation. The method is evaluated on an incident analysis task where the goal is to identify the root causes of a transportation incident, explaining it in terms of the information contained in the incident description. The evaluation results show that the proposed approach increases both the recall and the precision of the system.},
  isbn = {978-3-319-47096-2},
  langid = {english}
}

@inproceedings{Skeppstedt2018MoreLessControlled,
  title = {More or Less Controlled Elicitation of Argumentative Text: {{Enlarging}} a Microtext Corpus via Crowdsourcing},
  shorttitle = {More or Less Controlled Elicitation of Argumentative Text},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Argument Mining}}},
  author = {Skeppstedt, Maria and Peldszus, Andreas and Stede, Manfred},
  editor = {Slonim, Noam and Aharonov, Ranit},
  date = {2018-11},
  pages = {155--163},
  publisher = {Association for Computational Linguistics},
  location = {Brussels, Belgium},
  doi = {10.18653/v1/W18-5218},
  url = {https://aclanthology.org/W18-5218/},
  urldate = {2025-07-14},
  abstract = {We present an extension of an annotated corpus of short argumentative texts that had originally been built in a controlled text production experiment. Our extension more than doubles the size of the corpus by means of crowdsourcing. We report on the setup of this experiment and on the consequences that crowdsourcing had for assembling the data, and in particular for annotation. We labeled the argumentative structure by marking claims, premises, and relations between them, following the scheme used in the original corpus, but had to make a few modifications in response to interesting phenomena in the data. Finally, we report on an experiment with the automatic prediction of this argumentation structure: We first replicated the approach of an earlier study on the original corpus, and compare the performance to various settings involving the extension.},
  eventtitle = {{{ArgMining}} 2018}
}

@inproceedings{Slonim2018ProjectDebater,
  title = {Project {{Debater}}},
  booktitle = {Proceedings of {{Computational Models}} of {{Argument}}},
  author = {Slonim, Noam},
  date = {2018},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {305},
  pages = {4},
  publisher = {IOS Press},
  location = {Warsaw, Poland},
  doi = {10.3233/978-1-61499-906-5-4},
  abstract = {Project Debater is the first AI system that was shown to debate humans in a meaningful manner in a full live debate. Developing this system started in 2012, as the next AI Grand Challenge pursued by IBM Research, following the demonstration of Deep Blue in Chess in 1997, and Watson in Jeopardy! In 2011. The Project Debater system was demonstrated for the first time in San Francisco in June 2018, in two full live debates vs. expert human debaters, and correspondingly received massive media attention. This talk will present the challenges in developing this system, its current capabilities and present limitations, as well as how we envision its future.}
}

@article{Slonim2021AutonomousDebatingSystem,
  title = {An Autonomous Debating System},
  author = {Slonim, Noam and Bilu, Yonatan and Alzate, Carlos and Bar-Haim, Roy and Bogin, Ben and Bonin, Francesca and Choshen, Leshem and Cohen-Karlik, Edo and Dankin, Lena and Edelstein, Lilach and Ein-Dor, Liat and Friedman-Melamed, Roni and Gavron, Assaf and Gera, Ariel and Gleize, Martin and Gretz, Shai and Gutfreund, Dan and Halfon, Alon and Hershcovich, Daniel and Hoory, Ron and Hou, Yufang and Hummel, Shay and Jacovi, Michal and Jochim, Charles and Kantor, Yoav and Katz, Yoav and Konopnicki, David and Kons, Zvi and Kotlerman, Lili and Krieger, Dalia and Lahav, Dan and Lavee, Tamar and Levy, Ran and Liberman, Naftali and Mass, Yosi and Menczel, Amir and Mirkin, Shachar and Moshkowich, Guy and Ofek-Koifman, Shila and Orbach, Matan and Rabinovich, Ella and Rinott, Ruty and Shechtman, Slava and Sheinwald, Dafna and Shnarch, Eyal and Shnayderman, Ilya and Soffer, Aya and Spector, Artem and Sznajder, Benjamin and Toledo, Assaf and Toledo-Ronen, Orith and Venezian, Elad and Aharonov, Ranit},
  date = {2021-03},
  journaltitle = {Nature},
  volume = {591},
  number = {7850},
  pages = {379--384},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03215-w},
  url = {https://www.nature.com/articles/s41586-021-03215-w},
  urldate = {2023-11-24},
  abstract = {Artificial intelligence (AI) is defined as the ability of machines to perform tasks that are usually associated with intelligent beings. Argument and debate are fundamental capabilities of human intelligence, essential for a wide range of human activities, and common to all human societies. The development of computational argumentation technologies is therefore an important emerging discipline in AI research1. Here we present Project Debater, an autonomous debating system that can engage in a competitive debate with humans. We provide a complete description of the system’s architecture, a thorough and systematic evaluation of its operation across a wide range of debate topics, and a detailed account of the system’s performance in its public debut against three expert human debaters. We also highlight the fundamental differences between debating with humans as opposed to challenging humans in game competitions, the latter being the focus of classical ‘grand challenges’ pursued by the AI research community over the past few decades. We suggest that such challenges lie in the ‘comfort zone’ of AI, whereas debating with humans lies in a different territory, in which humans still prevail, and for which novel paradigms are required to make substantial progress.},
  issue = {7850},
  langid = {english}
}

@article{Smith1985DesignDivideConquer,
  title = {The Design of Divide and Conquer Algorithms},
  author = {Smith, Douglas R.},
  date = {1985-01-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  volume = {5},
  pages = {37--58},
  issn = {0167-6423},
  doi = {10.1016/0167-6423(85)90003-6},
  url = {https://www.sciencedirect.com/science/article/pii/0167642385900036},
  urldate = {2022-08-02},
  abstract = {The structure common to a class of divide and conquer algorithms is represented by a program scheme. A theorem is presented which relates the functionality of a divide and conquer algorithm to its structure and the functionalities of its subalgorithms. Several strategies for designing divide and conquer algorithms arise from this theorem and they are used to formally derive algorithms for sorting a list of numbers, forming the cartesian product of two sets, and finding the convex hull of a set of planar points.},
  langid = {english}
}

@inproceedings{Sobhani2015ArgumentationMiningStance,
  title = {From {{Argumentation Mining}} to {{Stance Classification}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Argumentation Mining}}},
  author = {Sobhani, Parinaz and Inkpen, Diana and Matwin, Stan},
  date = {2015-06},
  pages = {67--77},
  publisher = {Association for Computational Linguistics},
  location = {Denver, CO},
  doi = {10.3115/v1/W15-0509},
  url = {https://www.aclweb.org/anthology/W15-0509},
  urldate = {2020-10-21}
}

@inproceedings{Sokolova2006AccuracyFScoreROC,
  title = {Beyond {{Accuracy}}, {{F-Score}} and {{ROC}}: {{A Family}} of {{Discriminant Measures}} for {{Performance Evaluation}}},
  shorttitle = {Beyond {{Accuracy}}, {{F-Score}} and {{ROC}}},
  booktitle = {{{AI}} 2006: {{Advances}} in {{Artificial Intelligence}}},
  author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
  date = {2006-12-04},
  pages = {1015--1021},
  publisher = {Springer, Berlin, Heidelberg},
  doi = {10.1007/11941439_114},
  url = {https://link.springer.com/chapter/10.1007/11941439_114},
  urldate = {2021-03-13},
  abstract = {Different evaluation measures assess different characteristics of machine learning algorithms. The empirical evaluation of algorithms and classifiers is a matter of on-going debate among researchers....},
  eventtitle = {Australasian {{Joint Conference}} on {{Artificial Intelligence}}},
  langid = {english}
}

@inproceedings{Soleimani2020BERTEvidenceRetrieval,
  title = {{{BERT}} for {{Evidence Retrieval}} and {{Claim Verification}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Soleimani, Amir and Monz, Christof and Worring, Marcel},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {359--366},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-45442-5_45},
  abstract = {We investigate BERT in an evidence retrieval and claim verification pipeline for the task of evidence-based claim verification. To this end, we propose to use two BERT models, one for retrieving evidence sentences supporting or rejecting claims, and another for verifying claims based on the retrieved evidence sentences. To train the BERT retrieval system, we use pointwise and pairwise loss functions and examine the effect of hard negative mining. Our system achieves a new state of the art recall of 87.1 for retrieving evidence sentences out of the FEVER dataset 50K Wikipedia pages, and scores second in the leaderboard with the FEVER score of 69.7.},
  isbn = {978-3-030-45442-5},
  langid = {english}
}

@inproceedings{Soliman2026CaissaAINeuroSymbolic,
  title = {Caïssa {{AI}}: {{A Neuro-Symbolic Chess Agent}} for~{{Explainable Move Suggestion}} and~{{Grounded Commentary}}},
  shorttitle = {Caïssa {{AI}}},
  booktitle = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}},
  author = {Soliman, Mazen and Ehab, Nourhan},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15956},
  pages = {148--160},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6_11},
  abstract = {Despite the impressive generative capabilities of large language models (LLMs), their lack of grounded reasoning and susceptibility to hallucinations limit their reliability in structured domains such as chess. We present Caïssa AI, a neuro-symbolic chess agent that augments LLM-generated move commentary with symbolic reasoning, knowledge graph integration, and verification modules. Caïssa AI combines a fine-tuned chess-specific LLM with a Prolog-based rule engine encoding chess tactics and rules, along with a dynamically constructed Neo4j knowledge graph representing the current board state. This hybrid architecture enables the system to generate not only accurate move suggestions but also coherent, strategically grounded commentary. A LangGraph-based verification module cross-checks LLM outputs against symbolic logic to ensure consistency and correctness, effectively mitigating hallucinations. By aligning data-driven generation with formal domain knowledge, Caïssa AI enhances both trustworthiness and explainability. Our results demonstrate that this tight neuro-symbolic integration produces verifiable, high-quality commentary and serves as a generalizable blueprint for AI systems requiring real-time, interpretable decision support.},
  eventtitle = {German {{Conference}} on {{Artificial Intelligence}}},
  isbn = {978-3-032-02813-6},
  langid = {english}
}

@inproceedings{Song2020StructuralInformationPreserving,
  title = {Structural {{Information Preserving}} for {{Graph-to-Text Generation}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Song, Linfeng and Wang, Ante and Su, Jinsong and Zhang, Yue and Xu, Kun and Ge, Yubin and Yu, Dong},
  date = {2020-07},
  pages = {7987--7998},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.712},
  url = {https://www.aclweb.org/anthology/2020.acl-main.712},
  urldate = {2020-10-16},
  abstract = {The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our model via multi-task training. Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.},
  eventtitle = {{{ACL}} 2020}
}

@inproceedings{Sourati2023CaseBasedReasoningLanguage,
  title = {Case-{{Based Reasoning}} with {{Language Models}} for {{Classification}} of {{Logical Fallacies}}},
  booktitle = {Proceedings of the {{Thirty-Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Sourati, Zhivar and Ilievski, Filip and Sandlin, Hông-Ân and Mermoud, Alain},
  date = {2023-08},
  pages = {5188--5196},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  location = {Macau, SAR China},
  doi = {10.24963/ijcai.2023/576},
  url = {https://www.ijcai.org/proceedings/2023/576},
  urldate = {2024-09-16},
  abstract = {The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.},
  eventtitle = {Thirty-{{Second International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI-23}}\vphantom\{\}},
  isbn = {978-1-956792-03-4},
  langid = {english}
}

@inproceedings{Spagnola2011EdgeDependentPathway,
  title = {Edge Dependent Pathway Scoring for Calculating Semantic Similarity in {{ConceptNet}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Computational Semantics}} ({{IWCS}} 2011)},
  author = {Spagnola, Steve and Lagoze, Carl},
  date = {2011},
  url = {https://www.aclweb.org/anthology/W11-0148},
  urldate = {2020-05-02},
  abstract = {Most techniques that calculate the relatedness between two concepts use a semantic network, such as Wikipedia, WordNet, or ConceptNet, to find the shortest intermediate pathway between two nodes. These techniques assume that a low number of edges on the shortest pathway indicates conceptual similarity. Although this technique has proven valid in conforming to psychological data, we test the usefulness of additional pathway variables in ConceptNet, such as edge type and user-rated score. Our results show strong evidence for the application of additional pathway variables in calculating semantic similarity.}
}

@inproceedings{Spanoudakis2020ArgumentationAll,
  title = {Argumentation for All},
  booktitle = {Proceedings of the 35th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Spanoudakis, Nikolaos and Kostis, Konstantinos and Mania, Katerina},
  date = {2020-03-30},
  pages = {980--982},
  publisher = {ACM},
  location = {Brno Czech Republic},
  doi = {10.1145/3341105.3374122},
  url = {https://dl.acm.org/doi/10.1145/3341105.3374122},
  urldate = {2023-10-25},
  abstract = {This paper proposes the use of a web-based system for the development of applications of argumentation. It focuses on bringing  the capability to develop decision policies based on argumentation  to people that have little or no knowledge of logic programming  or of an argumentation framework. To achieve this, it presents an  implementation of the table formalism that has recently been put  forward by previous work. Our system was evaluated using the  think aloud protocol from the early stages of development.},
  eventtitle = {{{SAC}} '20: {{The}} 35th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  isbn = {978-1-4503-6866-7},
  langid = {english}
}

@inproceedings{Speer2017ConceptNetOpenMultilingual,
  title = {{{ConceptNet}} 5.5: An Open Multilingual Graph of General Knowledge},
  shorttitle = {{{ConceptNet}} 5.5},
  booktitle = {Proceedings of the {{Thirty-First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  date = {2017-02-04},
  series = {{{AAAI}}'17},
  pages = {4444--4451},
  publisher = {AAAI Press},
  location = {San Francisco, California, USA},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.}
}

@online{Sperrle2019VIANAVisualInteractive,
  title = {{{VIANA}}: {{Visual Interactive Annotation}} of {{Argumentation}}},
  shorttitle = {{{VIANA}}},
  author = {Sperrle, Fabian and Sevastjanova, Rita and Kehlbeck, Rebecca and El-Assady, Mennatallah},
  date = {2019-07-29},
  eprint = {1907.12413},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.12413},
  url = {http://arxiv.org/abs/1907.12413},
  urldate = {2023-10-18},
  abstract = {Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.},
  pubstate = {prepublished}
}

@misc{SPICE2023APIsSpecificationDeployment,
  title = {{{APIs Specification}} and {{Deployment}}},
  author = {{SPICE}},
  date = {2023-04-28},
  url = {https://spice-h2020.eu/document/deliverable/D6.8.pdf},
  urldate = {2025-02-05}
}

@incollection{Sriram1997AnalogicalCaseBasedReasoning,
  title = {Analogical and {{Case-Based Reasoning}}},
  booktitle = {Intelligent {{Systems}} for {{Engineering}}: {{A Knowledge-based Approach}}},
  author = {Sriram, Ram D.},
  editor = {Sriram, Ram D.},
  date = {1997},
  pages = {285--334},
  publisher = {Springer},
  location = {London},
  doi = {10.1007/978-1-4471-0631-9_6},
  url = {https://doi.org/10.1007/978-1-4471-0631-9_6},
  urldate = {2021-02-13},
  abstract = {Analogical Reasoning (AR) involves the use of past experiences to solve problems that are similar to problems solved before. This kind of reasoning is pervasive in engineering disciplines, particularly design. As there are no established, widely accepted, theories and methods for engineering design, practitioners often rely on prior design cases to exploit past successes and to avoid repeating the same mistakes. Research in analogical problem solving concentrates on the process of similarity recognition, mapping of past cases to current situation, and modification of past cases to suit a given task. An important aspect of this research is the development of representations that accurately capture prior experiences, conditions, and explanations. The experiences (cases) are represented and stored in knowledge-bases called case memories. As case memories grow in size, issues relating to indexing and retrieval become important. This aspect of analogical reasoning is called case- based reasoning (CBR). The primary emphasis of case-based reasoning is on the organization, hierarchy indexing and retrieval of case memory, while the main emphasis of analogical reasoning is on the process of modifying, adapting and verifying past derivations (cases) [5]. However, our treatment of case-based reasoning will also include elements of analogical reasoning.},
  isbn = {978-1-4471-0631-9},
  langid = {english}
}

@inproceedings{Stab2014AnnotatingArgumentComponents,
  title = {Annotating {{Argument Components}} and {{Relations}} in {{Persuasive Essays}}},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Stab, Christian and Gurevych, Iryna},
  date = {2014-08},
  pages = {1501--1510},
  publisher = {{Dublin City University and Association for Computational Linguistics}},
  location = {Dublin, Ireland},
  url = {https://aclanthology.org/C14-1142},
  urldate = {2023-10-20},
  eventtitle = {{{COLING}} 2014}
}

@inproceedings{Stab2014IdentifyingArgumentativeDiscourse,
  title = {Identifying {{Argumentative Discourse Structures}} in {{Persuasive Essays}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Stab, Christian and Gurevych, Iryna},
  date = {2014-10},
  pages = {46--56},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1006},
  url = {https://www.aclweb.org/anthology/D14-1006},
  urldate = {2020-06-11},
  eventtitle = {{{EMNLP}} 2014}
}

@article{Stab2017ParsingArgumentationStructures,
  title = {Parsing {{Argumentation Structures}} in {{Persuasive Essays}}},
  author = {Stab, Christian and Gurevych, Iryna},
  date = {2017-09},
  journaltitle = {Computational Linguistics},
  volume = {43},
  number = {3},
  pages = {619--659},
  doi = {10.1162/COLI_a_00295},
  url = {https://www.aclweb.org/anthology/J17-3005},
  urldate = {2020-09-14},
  abstract = {In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using Integer Linear Programming. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement.}
}

@inproceedings{Stab2018ArgumenTextSearchingArguments,
  title = {{{ArgumenText}}: {{Searching}} for {{Arguments}} in {{Heterogeneous Sources}}},
  shorttitle = {{{ArgumenText}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Demonstrations}}},
  author = {Stab, Christian and Daxenberger, Johannes and Stahlhut, Chris and Miller, Tristan and Schiller, Benjamin and Tauchmann, Christopher and Eger, Steffen and Gurevych, Iryna},
  date = {2018-06},
  pages = {21--25},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-5005},
  url = {https://www.aclweb.org/anthology/N18-5005},
  urldate = {2020-09-02},
  abstract = {Argument mining is a core technology for enabling argument search in large corpora. However, most current approaches fall short when applied to heterogeneous texts. In this paper, we present an argument retrieval system capable of retrieving sentential arguments for any given controversial topic. By analyzing the highest-ranked results extracted from Web sources, we found that our system covers 89\% of arguments found in expert-curated lists of arguments from an online debate portal, and also identifies additional valid arguments.}
}

@inproceedings{Stab2018CrosstopicArgumentMining,
  title = {Cross-Topic {{Argument Mining}} from {{Heterogeneous Sources}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Stab, Christian and Miller, Tristan and Schiller, Benjamin and Rai, Pranav and Gurevych, Iryna},
  editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
  date = {2018-10},
  pages = {3664--3674},
  publisher = {Association for Computational Linguistics},
  location = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1402},
  url = {https://aclanthology.org/D18-1402/},
  urldate = {2025-09-19},
  abstract = {Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. We show that integrating topic information into bidirectional long short-term memory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in two- and three-label cross-topic settings. We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.},
  eventtitle = {{{EMNLP}} 2018}
}

@thesis{Stahl2004LearningKnowledgeintensiveSimilarity,
  type = {phdthesis},
  title = {Learning of Knowledge-Intensive Similarity Measures in Case-Based Reasoning},
  author = {Stahl, Armin},
  date = {2004},
  institution = {University of Kaiserslautern},
  location = {Kaiserslautern},
  url = {https://d-nb.info/972459111},
  isbn = {9783898258869},
  langid = {english},
  pagetotal = {237}
}

@inproceedings{Stahl2005LearningSimilarityMeasures,
  title = {Learning {{Similarity Measures}}: {{A Formal View Based}} on a {{Generalized CBR Model}}},
  shorttitle = {Learning {{Similarity Measures}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Stahl, Armin},
  editor = {Muñoz-Ávila, Héctor and Ricci, Francesco},
  date = {2005},
  pages = {507--521},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11536406_39},
  abstract = {Although similarity measures play a crucial role in CBR applications, clear methodologies for defining them have not been developed yet. One approach to simplify the definition of similarity measures involves the use of machine learning techniques. In this paper we investigate important aspects of these approaches in order to support a more goal-directed choice and application of existing approaches and to initiate the development of new techniques. This investigation is based on a novel formal generalization of the classic CBR cycle, which allows a more suitable analysis of the requirements, goals, assumptions and restrictions that are relevant for learning similarity measures.},
  isbn = {978-3-540-31855-2},
  langid = {english}
}

@online{Stechly2024SelfVerificationLimitationsLarge,
  title = {On the {{Self-Verification Limitations}} of {{Large Language Models}} on {{Reasoning}} and {{Planning Tasks}}},
  author = {Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
  date = {2024-02-12},
  eprint = {2402.08115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.08115},
  url = {http://arxiv.org/abs/2402.08115},
  urldate = {2024-03-28},
  abstract = {There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system. In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.},
  pubstate = {prepublished}
}

@online{Steck2024CosineSimilarityEmbeddingsReally,
  title = {Is {{Cosine-Similarity}} of {{Embeddings Really About Similarity}}?},
  author = {Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
  date = {2024-03-08},
  eprint = {2403.05440},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1145/3589335.3651526},
  url = {http://arxiv.org/abs/2403.05440},
  urldate = {2024-03-28},
  abstract = {Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.},
  pubstate = {prepublished}
}

@inproceedings{Stenetorp2012BratWebbasedTool,
  title = {Brat: A {{Web-based Tool}} for {{NLP-Assisted Text Annotation}}},
  shorttitle = {Brat},
  booktitle = {Proceedings of the {{Demonstrations}} at the 13th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Stenetorp, Pontus and Pyysalo, Sampo and Topić, Goran and Ohta, Tomoko and Ananiadou, Sophia and Tsujii, Jun'ichi},
  date = {2012-04},
  pages = {102--107},
  publisher = {Association for Computational Linguistics},
  location = {Avignon, France},
  url = {https://www.aclweb.org/anthology/E12-2021},
  urldate = {2020-10-20}
}

@unpublished{Storks2020RecentAdvancesNatural,
  title = {Recent {{Advances}} in {{Natural Language Inference}}: {{A Survey}} of {{Benchmarks}}, {{Resources}}, and {{Approaches}}},
  shorttitle = {Recent {{Advances}} in {{Natural Language Inference}}},
  author = {Storks, Shane and Gao, Qiaozi and Chai, Joyce Y.},
  date = {2020-02-26},
  eprint = {1904.01172},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.01172},
  urldate = {2020-09-02},
  abstract = {In the NLP community, recent years have seen a surge of research activities that address machines' ability to perform deep language understanding which goes beyond what is explicitly stated in text, rather relying on reasoning and knowledge of the world. Many benchmark tasks and datasets have been created to support the development and evaluation of such natural language inference ability. As these benchmarks become instrumental and a driving force for the NLP research community, this paper aims to provide an overview of recent benchmarks, relevant knowledge resources, and state-of-the-art learning and inference approaches in order to support a better understanding of this growing field.}
}

@thesis{Stricker2022VergleichGraphMatchingAlgorithmen,
  type = {mathesis},
  title = {Vergleich von Graph-Matching Algorithmen im Rahmen des Case-Based Retrieval von Argumenten},
  author = {Stricker, Nikita},
  date = {2022-12-09},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {ngerman}
}

@incollection{Stump2005WordFormationInflectionalMorphology,
  title = {Word-{{Formation}} and {{Inflectional Morphology}}},
  booktitle = {Handbook of {{Word-Formation}}},
  author = {Stump, Gregory T.},
  editor = {Štekauer, Pavol and Lieber, Rochelle},
  date = {2005},
  series = {Studies in {{Natural Language}} and {{Linguistic Theory}}},
  pages = {49--71},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/1-4020-3596-9_3},
  url = {https://doi.org/10.1007/1-4020-3596-9_3},
  urldate = {2021-03-08},
  isbn = {978-1-4020-3596-8},
  langid = {english}
}

@inproceedings{Sui2025CanKnowledgeGraphs,
  title = {Can {{Knowledge Graphs Make Large Language Models More Trustworthy}}? {{An Empirical Study Over Open-ended Question Answering}}},
  shorttitle = {Can {{Knowledge Graphs Make Large Language Models More Trustworthy}}?},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sui, Yuan and He, Yufei and Ding, Zifeng and Hooi, Bryan},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {12685--12701},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-long.622/},
  urldate = {2025-07-29},
  abstract = {Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs' potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{Sukprapa2021TextSummarizationUsing,
  title = {Text {{Summarization}} Using {{Formal Argumentation}}},
  booktitle = {2021 16th {{International Joint Symposium}} on {{Artificial Intelligence}} and {{Natural Language Processing}} ({{iSAI-NLP}})},
  author = {Sukprapa, Isada and Hung, Nguyen Duy and Supnithi, Thepchai},
  date = {2021-12},
  pages = {1--6},
  doi = {10.1109/iSAI-NLP54397.2021.9678183},
  url = {https://ieeexplore.ieee.org/document/9678183},
  urldate = {2023-10-26},
  abstract = {Current approaches to text summarization are not genuinely interested in how competent readers perform the task often by re-constructing the arguments in the text then arriving at the summary from conclusions of acceptable arguments. This paper aims to mimic this natural path using formal argumentation techniques. Assuming the availability Argumentative Discourse Unit (ADU) graph of the given text, we build structured argumentation frameworks called S-ASPIC+ and ABA representing the text. Then we use ABA proof procedures to re-construct arguments in the text and evaluate their acceptabilities. Finally, we aggregate the conclusions of acceptable arguments. We demonstrate our approach using a dataset of argumentative micro-texts and report the results, describing comparisons to other methods.},
  eventtitle = {2021 16th {{International Joint Symposium}} on {{Artificial Intelligence}} and {{Natural Language Processing}} ({{iSAI-NLP}})}
}

@inproceedings{Sulea2017RecognizingTextualEntailment,
  title = {Recognizing {{Textual Entailment}} in {{Twitter Using Word Embeddings}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Evaluating Vector Space Representations}} for {{NLP}}},
  author = {Şulea, Octavia-Maria},
  date = {2017-09},
  pages = {31--35},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/W17-5306},
  url = {https://aclanthology.org/W17-5306},
  urldate = {2022-01-03},
  abstract = {In this paper, we investigate the application of machine learning techniques and word embeddings to the task of Recognizing Textual Entailment (RTE) in Social Media. We look at a manually labeled dataset consisting of user generated short texts posted on Twitter (tweets) and related to four recent media events (the Charlie Hebdo shooting, the Ottawa shooting, the Sydney Siege, and the German Wings crash) and test to what extent neural techniques and embeddings are able to distinguish between tweets that entail or contradict each other or that claim unrelated things. We obtain comparable results to the state of the art in a train-test setting, but we show that, due to the noisy aspect of the data, results plummet in an evaluation strategy crafted to better simulate a real-life train-test scenario.}
}

@inproceedings{Summers-Stay2017DeductiveAnalogicalReasoning,
  title = {Deductive and {{Analogical Reasoning}} on a {{Semantically Embedded Knowledge Graph}}},
  booktitle = {Artificial {{General Intelligence}}},
  author = {Summers-Stay, Douglas},
  editor = {Everitt, Tom and Goertzel, Ben and Potapov, Alexey},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {112--122},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-63703-7_11},
  abstract = {Representing knowledge as high-dimensional vectors in a continuous semantic vector space can help overcome the brittleness and incompleteness of traditional knowledge bases. We present a method for performing deductive reasoning directly in such a vector space, combining analogy, association, and deduction in a straightforward way at each step in a chain of reasoning, drawing on knowledge from diverse sources and ontologies.},
  isbn = {978-3-319-63703-7},
  langid = {english}
}

@inproceedings{Summers-Stay2020PropositionalDeductiveInference,
  title = {Propositional {{Deductive Inference}} by {{Semantic Vectors}}},
  booktitle = {Intelligent {{Systems}} and {{Applications}}},
  author = {Summers-Stay, Douglas},
  editor = {Bi, Yaxin and Bhatia, Rahul and Kapoor, Supriya},
  date = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {810--820},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-29516-5_61},
  abstract = {Representing symbols by high-dimensional vectors makes it easier to perform analogical and associational reasoning, but performing multi-step deductive reasoning typically requires a discrete knowledge base. In this paper, we show a method by which deductive inference can be performed directly on high-dimensional semantic vectors, and characterize some limitations and advantages of this approach. We provide a method for taking a set of semantic vectors representing propositions and encoding a knowledge base telling how those propositions are logically related.},
  isbn = {978-3-030-29516-5},
  langid = {english}
}

@article{Swets1963InformationRetrievalSystems,
  title = {Information {{Retrieval Systems}}},
  author = {Swets, John A.},
  date = {1963},
  journaltitle = {Science},
  volume = {141},
  number = {3577},
  eprint = {1710636},
  eprinttype = {jstor},
  pages = {245--250},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  url = {https://www.jstor.org/stable/1710636},
  urldate = {2021-03-13}
}

@unpublished{Syed2021GeneratingInformativeConclusions,
  title = {Generating {{Informative Conclusions}} for {{Argumentative Texts}}},
  author = {Syed, Shahbaz and Al-Khatib, Khalid and Alshomary, Milad and Wachsmuth, Henning and Potthast, Martin},
  date = {2021-06-02},
  eprint = {2106.01064},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.01064},
  urldate = {2021-06-09},
  abstract = {The purpose of an argumentative text is to support a certain conclusion. Yet, they are often omitted, expecting readers to infer them rather. While appropriate when reading an individual text, this rhetorical device limits accessibility when browsing many texts (e.g., on a search engine or on social media). In these scenarios, an explicit conclusion makes for a good candidate summary of an argumentative text. This is especially true if the conclusion is informative, emphasizing specific concepts from the text. With this paper we introduce the task of generating informative conclusions: First, Webis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of argumentative texts and their conclusions. Second, two paradigms for conclusion generation are investigated; one extractive, the other abstractive in nature. The latter exploits argumentative knowledge that augment the data via control codes and finetuning the BART model on several subsets of the corpus. Third, insights are provided into the suitability of our corpus for the task, the differences between the two generation paradigms, the trade-off between informativeness and conciseness, and the impact of encoding argumentative knowledge. The corpus, code, and the trained models are publicly available.}
}

@unpublished{Tai2015ImprovedSemanticRepresentations,
  title = {Improved {{Semantic Representations From Tree-Structured Long Short-Term Memory Networks}}},
  author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  date = {2015-02-28},
  eprint = {1503.00075},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).}
}

@inproceedings{Tang2020DependencyGraphEnhanced,
  title = {Dependency {{Graph Enhanced Dual-transformer Structure}} for {{Aspect-based Sentiment Classification}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tang, Hao and Ji, Donghong and Li, Chenliang and Zhou, Qiji},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  date = {2020-07},
  pages = {6578--6588},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.588},
  url = {https://aclanthology.org/2020.acl-main.588/},
  urldate = {2025-07-15},
  abstract = {Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.},
  eventtitle = {{{ACL}} 2020}
}

@online{Tay2022ScalingLawsVs,
  title = {Scaling {{Laws}} vs {{Model Architectures}}: {{How}} Does {{Inductive Bias Influence Scaling}}?},
  shorttitle = {Scaling {{Laws}} vs {{Model Architectures}}},
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung Won and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh Q. and Yogatama, Dani and Metzler, Donald},
  date = {2022-07-21},
  eprint = {2207.10551},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.10551},
  url = {http://arxiv.org/abs/2207.10551},
  urldate = {2024-04-25},
  abstract = {There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.},
  pubstate = {prepublished}
}

@inproceedings{Tchechmedjiev2019ClaimsKGKnowledgeGraph,
  title = {{{ClaimsKG}}: {{A Knowledge Graph}} of {{Fact-Checked Claims}}},
  shorttitle = {{{ClaimsKG}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2019},
  author = {Tchechmedjiev, Andon and Fafalios, Pavlos and Boland, Katarina and Gasquet, Malo and Zloch, Matthäus and Zapilko, Benjamin and Dietze, Stefan and Todorov, Konstantin},
  editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
  date = {2019},
  pages = {309--324},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-30796-7_20},
  abstract = {Various research areas at the intersection of computer and social sciences require a ground truth of contextualized claims labelled with their truth values in order to facilitate supervision, validation or reproducibility of approaches dealing, for example, with fact-checking or analysis of societal debates. So far, no reasonably large, up-to-date and queryable corpus of structured information about claims and related metadata is publicly available. In an attempt to fill this gap, we introduce ClaimsKG, a knowledge graph of fact-checked claims, which facilitates structured queries about their truth values, authors, dates, journalistic reviews and other kinds of metadata. ClaimsKG is generated through a semi-automated pipeline, which harvests data from popular fact-checking websites on a regular basis, annotates claims with related entities from DBpedia, and lifts the data to RDF using an RDF/S model that makes use of established vocabularies. In order to harmonise data originating from diverse fact-checking sites, we introduce normalised ratings as well as a simple claims coreference resolution strategy. The current knowledge graph, extensible to new information, consists of 28,383 claims published since 1996, amounting to 6,606,032 triples.},
  isbn = {978-3-030-30796-7},
  langid = {english}
}

@inproceedings{TeixeiraDeLima2025KnowYourRAG,
  title = {Know {{Your RAG}}: {{Dataset Taxonomy}} and {{Generation Strategies}} for {{Evaluating RAG Systems}}},
  shorttitle = {Know {{Your RAG}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}: {{Industry Track}}},
  author = {Teixeira de Lima, Rafael and Gupta, Shubham and Berrospi Ramis, Cesar and Mishra, Lokesh and Dolfi, Michele and Staar, Peter and Vagenas, Panagiotis},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven and Darwish, Kareem and Agarwal, Apoorv},
  date = {2025-01},
  pages = {39--57},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, UAE},
  url = {https://aclanthology.org/2025.coling-industry.4/},
  urldate = {2025-09-16},
  abstract = {Retrieval Augmented Generation (RAG) systems are a widespread application of Large Language Models (LLMs) in the industry. While many tools exist empowering developers to build their own systems, measuring their performance locally, with datasets reflective of the system's use cases, is a technological challenge. Solutions to this problem range from non-specific and cheap (most public datasets) to specific and costly (generating data from local documents). In this paper, we show that using public question and answer (Q\&A) datasets to assess retrieval performance can lead to non-optimal systems design, and that common tools for RAG dataset generation can lead to unbalanced data. We propose solutions to these issues based on the characterization of RAG datasets through labels and through label-targeted data generation. Finally, we show that fine-tuned small LLMs can efficiently generate Q\&A datasets. We believe that these observations are invaluable to the know-your-data step of RAG systems development.},
  eventtitle = {{{COLING}} 2025}
}

@inproceedings{Tel2025UtilizingStructureProcess,
  title = {Utilizing the~{{Structure}} of~{{Process Models}} for~{{Guided Generation}} of~{{Explanatory Texts}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Tel, Tolga and Minor, Mirjam},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15662},
  pages = {157--171},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_11},
  abstract = {This paper explores a hybrid approach combining the strengths of traditional algorithms and Large-Language Model (LLM) powered Retrieval Augmented Generation (RAG) systems. While LLMs excel at generating human-like text and processing unstructured data, they can be unreliable for structured data extraction. To address the challenge of explaining complex business process models, we first use algorithms to extract key information. Then, a case-based RAG system generates textual descriptions. We investigate two methods: a) generating individual element descriptions and ordering them, and b) enhancing coherence through a harmonization step to produce a more unified explanation. A significant contribution of our work lies in the achievement of improved scalability. Unlike existing methods that often suffer from performance degradation when handling larger process models, our hybrid approach demonstrates a remarkable degree of consistency, maintaining accuracy even as the model complexity increases. Furthermore, we have successfully minimized the required context window for the LLM. By relying on the data extracted by the algorithms, we significantly reduce the amount of information the LLM needs to process simultaneously. This reduction in context window size not only improves efficiency but also mitigates the risk of information loss or “hallucinations” that can occur when LLMs are tasked with processing excessively large inputs. Therefore, our approach provides a trustworthy and robust solution for generating accessible explanations of complex business processes.},
  eventtitle = {{{ICCBR}} 2025},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@online{Thakur2025SupportEvaluationTREC,
  title = {Support {{Evaluation}} for the {{TREC}} 2024 {{RAG Track}}: {{Comparing Human}} versus {{LLM Judges}}},
  shorttitle = {Support {{Evaluation}} for the {{TREC}} 2024 {{RAG Track}}},
  author = {Thakur, Nandan and Pradeep, Ronak and Upadhyay, Shivani and Campos, Daniel and Craswell, Nick and Lin, Jimmy},
  date = {2025-04-21},
  eprint = {2504.15205},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.15205},
  url = {http://arxiv.org/abs/2504.15205},
  urldate = {2025-04-23},
  abstract = {Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing "ground truth", thereby reducing system hallucinations. A crucial factor in RAG evaluation is "support", whether the information in the cited documents supports the answer. To this end, we conducted a large-scale comparative study of 45 participant submissions on 36 topics to the TREC 2024 RAG Track, comparing an automatic LLM judge (GPT-4o) against human judges for support assessment. We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions. Our results indicate that for 56\% of the manual from-scratch assessments, human and GPT-4o predictions match perfectly (on a three-level scale), increasing to 72\% in the manual with post-editing condition. Furthermore, by carefully analyzing the disagreements in an unbiased study, we found that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment. To conclude, we provide a qualitative analysis of human and GPT-4o errors to help guide future iterations of support assessment.},
  pubstate = {prepublished}
}

@inproceedings{Thorburn2022OptimizingLanguageModels,
  title = {Optimizing {{Language Models}} for {{Argumentative Reasoning}}},
  author = {Thorburn, Luke and Kruger, Ariel},
  date = {2022},
  url = {https://www.semanticscholar.org/paper/Optimizing-Language-Models-for-Argumentative-Thorburn-Kruger/ae3a6bbe22ea136280e2927807775b3ac8356440},
  urldate = {2023-04-07},
  abstract = {Large transformer-based causal language models are capable of strong performance on many natural language processing tasks. Here, we systematically evaluate the performance of the 2.7 billion parameter GPT Neo pre-trained language model on 6 argumentative reasoning tasks under 5 different optimization strategies, including prompt programming, soft prompts, and parameter tuning. We report both intrinsic evaluation metrics (perplexity), and extrinsic measures of the coherence of model outputs, as judged by an expert human rater. With a few exceptions, the rate at which models produced coherent responses ranged from 15-50\%. In contrast, human performance (users of the Kialo argument mapping platform) ranged from 65-82\% coherent, depending on the task. These results suggest that larger, suitably optimized language models may be capable of supporting authors and auditors of natural language argument maps in human-in-the-loop settings. We share our finetuned models and code.},
  eventtitle = {{{ArgML}}@{{COMMA}}}
}

@inproceedings{Thorne2018FEVERLargescaleDataset,
  title = {{{FEVER}}: A {{Large-scale Dataset}} for {{Fact Extraction}} and {{VERification}}},
  shorttitle = {{{FEVER}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  date = {2018-06},
  pages = {809--819},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1074},
  url = {https://aclanthology.org/N18-1074/},
  urldate = {2025-07-15},
  abstract = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87\%, while if we ignore the evidence we achieve 50.91\%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.},
  eventtitle = {{{NAACL-HLT}} 2018}
}

@inproceedings{Tian2021UnderstandingSelfsupervisedLearning,
  title = {Understanding Self-Supervised Learning Dynamics without Contrastive Pairs},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  date = {2021-07-01},
  pages = {10268--10278},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/tian21a.html},
  urldate = {2025-07-15},
  abstract = {While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \textbackslash emph\{non-contrastive\} SSL (e.g., BYOL and SimSiam) show remarkable performance \{\textbackslash it without\} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \textbackslash ourmethod\{\}, that \textbackslash emph\{directly\} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by 2.52.52.5\% in 300-epoch training (and 555\% in 60-epoch). \textbackslash ourmethod\{\} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released\textbackslash footnote\{\textbackslash url\{https://github.com/facebookresearch/luckmatters/tree/master/ssl\}\}.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{Ting2011ConfusionMatrix,
  title = {Confusion {{Matrix}}},
  author = {Ting, Kai Ming},
  date = {2011},
  journaltitle = {Encyclopedia of Machine Learning},
  pages = {209--209},
  publisher = {Springer, Boston, MA},
  doi = {10.1007/978-0-387-30164-8_157},
  url = {https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_157},
  urldate = {2021-03-14},
  abstract = {A confusion matrix summarizes the classification performance of a classifier with respect to some test data. It is a two-dimensional matrix, indexed in one dimension by the true class of an object...},
  langid = {english}
}

@online{todo,
  title = {Reference Currently Missing},
  author = {{AAAI}},
  date = {1970-01-01},
  pubstate = {prepublished}
}

@book{Toulmin2003UsesArgument,
  title = {The {{Uses}} of {{Argument}}},
  author = {Toulmin, Stephen E.},
  date = {2003},
  edition = {2},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9780511840005},
  url = {https://www.cambridge.org/core/books/uses-of-argument/26CF801BC12004587B66778297D5567C},
  urldate = {2025-09-04},
  abstract = {A central theme throughout the impressive series of philosophical books and articles Stephen Toulmin has published since 1948 is the way in which assertions and opinions concerning all sorts of topics, brought up in everyday life or in academic research, can be rationally justified. Is there one universal system of norms, by which all sorts of arguments in all sorts of fields must be judged, or must each sort of argument be judged according to its own norms? In The Uses of Argument (1958) Toulmin sets out his views on these questions for the first time. In spite of initial criticisms from logicians and fellow philosophers, The Uses of Argument has been an enduring source of inspiration and discussion to students of argumentation from all kinds of disciplinary background for more than forty years.},
  isbn = {978-0-521-82748-5}
}

@inproceedings{Toutanova2003FeatureRichPartofSpeechTagging,
  title = {Feature-{{Rich Part-of-Speech Tagging}} with a {{Cyclic Dependency Network}}},
  booktitle = {Proceedings of the 2003 {{Human Language Technology Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D. and Singer, Yoram},
  date = {2003},
  pages = {252--259},
  url = {https://www.aclweb.org/anthology/N03-1033},
  urldate = {2021-02-09},
  eventtitle = {{{HLT-NAACL}} 2003}
}

@online{Touvron2023LlamaOpenFoundation,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date = {2023-07-19},
  eprint = {2307.09288},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.09288},
  url = {http://arxiv.org/abs/2307.09288},
  urldate = {2023-11-20},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  pubstate = {prepublished}
}

@inproceedings{Trautmann2020FineGrainedArgumentUnit,
  title = {Fine-{{Grained Argument Unit Recognition}} and {{Classification}}},
  booktitle = {Proceedings of the {{Thirty-Fourth AAAI Conference}} on {{Artificial Intelligence}} ({{AAAI}} 2020)},
  author = {Trautmann, Dietrich and Daxenberger, Johannes and Stab, Christian and Schütze, Hinrich and Gurevych, Iryna},
  date = {2020-02},
  eprint = {1904.09688},
  eprinttype = {arXiv},
  location = {New York, NY, USA},
  url = {https://aaai.org/Papers/AAAI/2020GB/AAAI-TrautmannD.7498.pdf},
  urldate = {2020-05-23},
  abstract = {Prior work has commonly defined argument retrieval from heterogeneous document collections as a sentence-level classification task. Consequently, argument retrieval suffers both from low recall and from sentence segmentation errors making it difficult for humans and machines to consume the arguments. In this work, we argue that the task should be performed on a more fine-grained level of sequence labeling. For this, we define the task as Argument Unit Recognition and Classification (AURC). We present a dataset of arguments from heterogeneous sources annotated as spans of tokens within a sentence, as well as with a corresponding stance. We show that and how such difficult argument annotations can be effectively collected through crowdsourcing with high interannotator agreement. The new benchmark, AURC-8, contains up to 15\% more arguments per topic as compared to annotations on the sentence level. We identify a number of methods targeted at AURC sequence labeling, achieving close to human performance on known domains. Further analysis also reveals that, contrary to previous approaches, our methods are more robust against sentence segmentation errors. We publicly release our code and the AURC-8 dataset.}
}

@article{Trautmann2020RelationalFineGrainedArgument,
  title = {Relational and {{Fine-Grained Argument Mining}}},
  author = {Trautmann, Dietrich and Fromm, Michael and Tresp, Volker and Seidl, Thomas and Schütze, Hinrich},
  date = {2020-07-01},
  journaltitle = {Datenbank-Spektrum},
  shortjournal = {Datenbank Spektrum},
  volume = {20},
  number = {2},
  pages = {99--105},
  issn = {1610-1995},
  doi = {10.1007/s13222-020-00341-z},
  url = {https://doi.org/10.1007/s13222-020-00341-z},
  urldate = {2020-09-12},
  abstract = {In our project ReMLAV, funded within the DFG Priority Program RATIO (http://www.spp-ratio.de/), we focus on relational and fine-grained argument mining. In this article, we first introduce the problems we address and then summarize related work. The main part of the article describes our research on argument mining, both coarse-grained and fine-grained methods, and on same-side stance classification, a~relational approach to the problem of stance classification. We conclude with an outlook.},
  langid = {english}
}

@article{Treinish2022RustworkxHighPerformanceGraph,
  title = {Rustworkx: {{A High-Performance Graph Library}} for {{Python}}},
  shorttitle = {Rustworkx},
  author = {Treinish, Matthew and Carvalho, Ivan and Tsilimigkounakis, Georgios and Sá, Nahum},
  date = {2022-11-01},
  journaltitle = {Journal of Open Source Software},
  volume = {7},
  number = {79},
  pages = {3968},
  issn = {2475-9066},
  doi = {10.21105/joss.03968},
  url = {https://joss.theoj.org/papers/10.21105/joss.03968},
  urldate = {2025-03-12},
  abstract = {Treinish et al., (2022). rustworkx: A High-Performance Graph Library for Python. Journal of Open Source Software, 7(79), 3968, https://doi.org/10.21105/joss.03968},
  langid = {english}
}

@unpublished{Trinh2019SimpleMethodCommonsense,
  title = {A {{Simple Method}} for {{Commonsense Reasoning}}},
  author = {Trinh, Trieu H. and Le, Quoc V.},
  date = {2019-09-26},
  eprint = {1806.02847},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.02847},
  urldate = {2020-06-07},
  abstract = {Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.}
}

@article{Turing1950ComputingMachineryIntelligence,
  title = {Computing {{Machinery}} and {{Intelligence}}},
  author = {Turing, A. M.},
  date = {1950-10-01},
  journaltitle = {Mind},
  shortjournal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  url = {https://doi.org/10.1093/mind/LIX.236.433},
  urldate = {2021-02-06}
}

@inproceedings{Turpin2006UserPerformancePrecision,
  title = {User Performance versus Precision Measures for Simple Search Tasks},
  author = {Turpin, Andrew and Scholer, Falk},
  date = {2006-01-01},
  pages = {11--18},
  doi = {10.1145/1148170.1148176},
  eventtitle = {{{SIGIR}} 2006: {{Proceedings}} of the 29th {{Annual International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}, {{Seattle}}, {{Washington}}, {{USA}}, {{August}} 6-11, 2006}
}

@article{Twardy2004ArgumentMapsImprove,
  title = {Argument {{Maps Improve Critical Thinking}}},
  author = {Twardy, Charles},
  date = {2004-05-01},
  journaltitle = {Teaching Philosophy},
  volume = {27},
  number = {2},
  pages = {95--116},
  doi = {10.5840/teachphil200427213},
  url = {https://www.pdcnet.org/pdc/bvdb.nsf/purchase?openform&fp=teachphil&id=teachphil_2004_0027_0002_0095_0116},
  urldate = {2022-04-21},
  abstract = {This paper describes the Reason! method of argument mapping (along with the associated Reason!Able software) and measures its effect on the California Critical Thinking Skills Test. The result of the author’s study is that students who use the Reason! method, rather than other methods of teaching critical thinking skills, perform better on the California test. What accounts for the effectiveness of Reason! method is its use of argument maps, a method of representing arguments using a two-dimensional diagram involving boxes and arrows. In addition to describing the method, and presenting empirical data that supports the Reason! approach, the author provides an assessment of the various strengths and weaknesses of the  method and details its use at the University of Melbourne.},
  langid = {english}
}

@article{Ullmann1976AlgorithmSubgraphIsomorphism,
  title = {An {{Algorithm}} for {{Subgraph Isomorphism}}},
  author = {Ullmann, J. R.},
  date = {1976-01-01},
  journaltitle = {J. ACM},
  volume = {23},
  number = {1},
  pages = {31--42},
  issn = {0004-5411},
  doi = {10.1145/321921.321925},
  url = {https://dl.acm.org/doi/10.1145/321921.321925},
  urldate = {2025-06-11},
  abstract = {Subgraph isomorphism can be determined by means of a brute-force tree-search enumeration procedure. In this paper a new algorithm is introduced that attains efficiency by inferentially eliminating successor nodes in the tree search. To assess the time actually taken by the new algorithm, subgraph isomorphism, clique detection, graph isomorphism, and directed graph isomorphism experiments have been carried out with random and with various nonrandom graphs.A parallel asynchronous logic-in-memory implementation of a vital part of the algorithm is also described, although this hardware has not actually been built. The hardware implementation would allow very rapid determination of isomorphism.}
}

@inproceedings{Upravitelev2025ExploringSemanticFiltering,
  title = {Exploring {{Semantic Filtering Heuristics For Efficient Claim Verification}}},
  booktitle = {Proceedings of the {{Eighth Fact Extraction}} and {{VERification Workshop}} ({{FEVER}})},
  author = {Upravitelev, Max and Sahitaj, Premtim and Hilbert, Arthur and Solopova, Veronika and Yang, Jing and Feldhus, Nils and Anikina, Tatiana and Ostermann, Simon and Schmitt, Vera},
  editor = {Akhtar, Mubashara and Aly, Rami and Christodoulopoulos, Christos and Cocarascu, Oana and Guo, Zhijiang and Mittal, Arpit and Schlichtkrull, Michael and Thorne, James and Vlachos, Andreas},
  date = {2025-07},
  pages = {229--237},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.fever-1.17/},
  urldate = {2025-07-28},
  abstract = {Given the limited computational and financial resources of news agencies, real-life usage of fact-checking systems requires fast response times. For this reason, our submission to the FEVER-8 claim verification shared task focuses on optimizing the efficiency of such pipelines built around subtasks such as evidence retrieval and veracity prediction. We propose the Semantic Filtering for Efficient Fact Checking (SFEFC) strategy, which is inspired by the FEVER-8 baseline and designed with the goal of reducing the number of LLM calls and other computationally expensive subroutines. Furthermore, we explore the reuse of cosine similarities initially calculated within a dense retrieval step to retrieve the top 10 most relevant evidence sentence sets. We use these sets for semantic filtering methods based on similarity scores and create filters for particularly hard classification labels “Not Enough Information” and “Conflicting Evidence/Cherrypicking” by identifying thresholds for potentially relevant information and the semantic variance within these sets. Compared to the parallelized FEVER-8 baseline, which takes 33.88 seconds on average to process a claim according to the FEVER-8 shared task leaderboard, our non-parallelized system remains competitive in regard to AVeriTeC retrieval scores while reducing the runtime to 7.01 seconds, achieving the fastest average runtime per claim.},
  eventtitle = {Fact {{Extraction}} and {{VERification Workshop}} ({{FEVER}})},
  isbn = {978-1-959429-53-1}
}

@inproceedings{Urchs2025Taz2024fullAnalysingGerman,
  title = {Taz2024full: {{Analysing German Newspapers}} for {{Gender Bias}} and {{Discrimination}} across {{Decades}}},
  shorttitle = {Taz2024full},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2025},
  author = {Urchs, Stefanie and Thurner, Veronika and Aßenmacher, Matthias and Heumann, Christian and Thiemichen, Stephanie},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {10661--10671},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.findings-acl.555/},
  urldate = {2025-07-29},
  abstract = {Open-access corpora are essential for advancing natural language processing (NLP) and computational social science (CSS). However,large-scale resources for German remain limited, restricting research on linguistic trends and societal issues such as gender bias. Wepresent taz2024full, the largest publicly available corpus of German newspaper articles to date, comprising over 1.8 million texts fromtaz, spanning 1980 to 2024.As a demonstration of the corpus's utility for bias and discrimination research, we analyse gender representation across four decades ofreporting. We find a consistent overrepresentation of men, but also a gradual shift toward more balanced coverage in recent years. Usinga scalable, structured analysis pipeline, we provide a foundation for studying actor mentions, sentiment, and linguistic framing in Germanjournalistic texts.The corpus supports a wide range of applications, from diachronic language analysis to critical media studies, and is freely available tofoster inclusive and reproducible research in German-language NLP.},
  eventtitle = {Findings 2025},
  isbn = {979-8-89176-256-5}
}

@book{VanEemeren1996FundamentalsArgumentationTheory,
  title = {Fundamentals of {{Argumentation Theory}}},
  shorttitle = {Fundamentals of {{Argumentation Theory}}},
  author = {family=Eemeren, given=Frans H., prefix=van, useprefix=true and Grootendorst, Rob and Johnson, Ralph H. and Plantin, Christian and Willard, Charles A.},
  date = {1996-03-03},
  edition = {1 edition},
  publisher = {Routledge},
  location = {Mahwah, N.J},
  abstract = {Argumentation theory is a distinctly multidisciplinary field of inquiry. It draws its data, assumptions, and methods from disciplines as disparate as formal logic and discourse analysis, linguistics and forensic science, philosophy and psychology, political science and education, sociology and law, and rhetoric and artificial intelligence. This presents the growing group of interested scholars and students with a problem of access, since it is even for those active in the field not common to have acquired a familiarity with relevant aspects of each discipline that enters into this multidisciplinary matrix. This book offers its readers a unique comprehensive survey of the various theoretical contributions which have been made to the study of argumentation. It discusses the historical works that provide the background to the field and all major approaches and trends in contemporary research.   Argument has been the subject of systematic inquiry for twenty-five hundred years. It has been graced with theories, such as formal logic or the legal theory of evidence, that have acquired a more or less settled provenance with regard to specific issues. But there has been nothing to date that qualifies as a unified general theory of argumentation, in all its richness and complexity. This being so, the argumentation theorist must have access to materials and methods that lie beyond his or her "home" subject. It is precisely on this account that this volume is offered to all the constituent research communities and their students. Apart from the historical sections, each chapter provides an economical introduction to the problems and methods that characterize a given part of the contemporary research program. Because the chapters are self-contained, they can be consulted in the order of a reader's interests or research requirements. But there is value in reading the work in its entirety. Jointly authored by the very people whose research has done much to define the current state of argumentation theory and to point the way toward more general and unified future treatments, this book is an impressively authoritative contribution to the field.},
  isbn = {978-0-8058-1862-8},
  langid = {english},
  pagetotal = {440}
}

@book{VanEemeren2014HandbookArgumentationTheory,
  title = {Handbook of {{Argumentation Theory}}},
  author = {Van Eemeren, Frans H. and Garssen, Bart and Krabbe, Erik C. W. and Snoeck Henkemans, A. Francisca and Verheij, Bart and Wagemans, Jean H. M.},
  date = {2014},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-90-481-9473-5},
  url = {https://link.springer.com/10.1007/978-90-481-9473-5},
  urldate = {2023-07-26},
  isbn = {978-90-481-9472-8 978-90-481-9473-5},
  langid = {english}
}

@article{VanEemeren2017ArgumentationTheoryFormal,
  title = {Argumentation Theory in Formal and Computational Perspective},
  author = {family=Eemeren, given=Frans H., prefix=van, useprefix=true and Verheij, Bart},
  date = {2017},
  journaltitle = {IFCoLog Journal of Logics and Their Applications},
  volume = {4},
  number = {8},
  pages = {2099--2181}
}

@book{VanEemeren2018ArgumentationTheoryPragmaDialectical,
  title = {Argumentation {{Theory}}: {{A Pragma-Dialectical Perspective}}},
  shorttitle = {Argumentation {{Theory}}},
  author = {Van Eemeren, Frans H.},
  date = {2018},
  series = {Argumentation {{Library}}},
  volume = {33},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-95381-6},
  url = {http://link.springer.com/10.1007/978-3-319-95381-6},
  urldate = {2025-02-15},
  isbn = {978-3-319-95380-9 978-3-319-95381-6}
}

@article{VanGelder2007RationaleRationale,
  title = {The Rationale for {{Rationale}}™},
  author = {family=Gelder, given=Tim, prefix=van, useprefix=true},
  date = {2007-03-01},
  journaltitle = {Law, Probability and Risk},
  shortjournal = {Law, Probability and Risk},
  volume = {6},
  number = {1--4},
  pages = {23--42},
  issn = {1470-8396},
  doi = {10.1093/lpr/mgm032},
  url = {https://doi.org/10.1093/lpr/mgm032},
  urldate = {2022-04-21},
  abstract = {Complex reasoning and argumentation are central to legal practice. Software-supported argument mapping may be able to help lawyers reason and argue more effectively. This article describes Rationale™, a generic argument mapping software package, and reviews some evidence that using it can help improve reasoning, i.e. make people smarter. It then explores three different explanations for this potential benefit: usability, complementation and semi-formality. First, argument mapping software can be more usable for reasoning activities than traditional methods because it can inherit the wisdom gained through decades of research and experience into usability, can exploit a wider range of representational resources, and is designed specifically to support reasoning activities. Second, such software works by complementing the strengths and weaknesses of our natural or inbuilt cognitive capacities. Third, it helps shift reasoning and argumentation into a semi-formal mode, a kind of ‘sweet spot’ between the laxness of everyday reasoning and the straightjacket of formal logic.}
}

@inproceedings{Vaswani2017AttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://dl.acm.org/doi/10.5555/3295222.3295349},
  urldate = {2024-04-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.}
}

@thesis{VelardeJara2024KnowledgeBasedFactVerification,
  type = {mathesis},
  title = {Knowledge-{{Based Fact Verification}}},
  author = {Velarde Jara, Juan Rodrigo},
  date = {2024-06-07},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {english},
  pagetotal = {74}
}

@online{Verga2024ReplacingJudgesJuries,
  title = {Replacing {{Judges}} with {{Juries}}: {{Evaluating LLM Generations}} with a {{Panel}} of {{Diverse Models}}},
  shorttitle = {Replacing {{Judges}} with {{Juries}}},
  author = {Verga, Pat and Hofstatter, Sebastian and Althammer, Sophia and Su, Yixuan and Piktus, Aleksandra and Arkhangorodsky, Arkady and Xu, Minjie and White, Naomi and Lewis, Patrick},
  date = {2024-04-29},
  eprint = {2404.18796},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.18796},
  url = {http://arxiv.org/abs/2404.18796},
  urldate = {2024-05-01},
  abstract = {As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.},
  pubstate = {prepublished}
}

@unpublished{Vilnis2014WordRepresentationsGaussian,
  title = {Word {{Representations}} via {{Gaussian Embedding}}},
  author = {Vilnis, Luke and McCallum, Andrew},
  date = {2014-12-20},
  eprint = {1412.6623},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.}
}

@article{Visser2020Argumentation2016US,
  title = {Argumentation in the 2016 {{US}} Presidential Elections: Annotated Corpora of Television Debates and Social Media Reaction},
  shorttitle = {Argumentation in the 2016 {{US}} Presidential Elections},
  author = {Visser, Jacky and Konat, Barbara and Duthie, Rory and Koszowy, Marcin and Budzynska, Katarzyna and Reed, Chris},
  date = {2020-03-01},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  volume = {54},
  number = {1},
  pages = {123--154},
  issn = {1574-0218},
  doi = {10.1007/s10579-019-09446-8},
  url = {https://doi.org/10.1007/s10579-019-09446-8},
  urldate = {2025-07-15},
  abstract = {In this paper we present US2016, the largest publicly available set of corpora of annotated dialogical argumentation. The annotation covers argumentative relations, dialogue acts and pragmatic features. The corpora comprise transcriptions of television debates leading up to the 2016 US presidential elections, and reactions to the debates on Reddit.These two constitutive parts of the corpora are integrated by means of the intertextual correspondence between them. The rhetorical richness and high argument density of the communicative context results in cross-genre corpora that are robust resources for the study of the dialogical dynamics of argumentation in three ways: first, in empirical strands of research in discourse analysis and argumentation studies; second,in the burgeoning field of argument mining where automatic techniques require such data; and third, in formulating algorithmic techniques for sensemaking through the development of Argument Analytics.},
  langid = {english}
}

@software{Voigt2014Argdown,
  title = {Argdown},
  author = {Voigt, Christian},
  date = {2014-03-16},
  origdate = {2014-03-21T22:28:08Z},
  url = {https://github.com/christianvoigt/argdown},
  urldate = {2022-04-21},
  abstract = {a simple syntax for complex argumentation}
}

@inproceedings{VonAhn2006VerbosityGameCollecting,
  title = {Verbosity: A Game for Collecting Common-Sense Facts},
  shorttitle = {Verbosity},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {family=Ahn, given=Luis, prefix=von, useprefix=true and Kedia, Mihir and Blum, Manuel},
  date = {2006-04-22},
  series = {{{CHI}} '06},
  pages = {75--78},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1124772.1124784},
  url = {https://doi.org/10.1145/1124772.1124784},
  urldate = {2021-02-12},
  abstract = {We address the problem of collecting a database of ""common-sense facts"" using a computer game. Informally, a common-sense fact is a true statement about the world that is known to most humans: ""milk is white,"" ""touching hot metal hurts,"" etc. Several efforts have been devoted to collecting common-sense knowledge for the purpose of making computer programs more intelligent. Such efforts, however, have not succeeded in amassing enough data because the manual process of entering these facts is tedious. We therefore introduce Verbosity, a novel interactive system in the form of an enjoyable game. People play Verbosity because it is fun, and as a side effect of them playing, we collect accurate common-sense knowledge. Verbosity is an example of a game that not only brings people together for leisure, but also collects useful data for computer science.},
  isbn = {978-1-59593-372-0}
}

@unpublished{Voskoglou2014AnalogyBasedCaseBasedReasoning,
  title = {Analogy-{{Based}} and {{Case-Based Reasoning}}: {{Two}} Sides of the Same Coin},
  shorttitle = {Analogy-{{Based}} and {{Case-Based Reasoning}}},
  author = {Voskoglou, Michael Gr and Salem, Abdel-Badeeh M.},
  date = {2014-05-29},
  eprint = {1405.7567},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1405.7567},
  urldate = {2020-05-26},
  abstract = {Analogy-Based (or Analogical) and Case-Based Reasoning (ABR and CBR) are two similar problem solving processes based on the adaptation of the solution of past problems for use with a new analogous problem. In this paper we review these two processes and we give some real world examples with emphasis to the field of Medicine, where one can find some of the most common and useful CBR applications. We also underline the differences between CBR and the classical rule-induction algorithms, we discuss the criticism for CBR methods and we focus on the future trends of research in the area of CBR.}
}

@inproceedings{Wacholder2014AnnotatingMultipartyDiscourse,
  title = {Annotating {{Multiparty Discourse}}: {{Challenges}} for {{Agreement Metrics}}},
  shorttitle = {Annotating {{Multiparty Discourse}}},
  booktitle = {Proceedings of {{LAW VIII}} - {{The}} 8th {{Linguistic Annotation Workshop}}},
  author = {Wacholder, Nina and Muresan, Smaranda and Ghosh, Debanjan and Aakhus, Mark},
  date = {2014-08},
  pages = {120--128},
  publisher = {{Association for Computational Linguistics and Dublin City University}},
  location = {Dublin, Ireland},
  doi = {10.3115/v1/W14-4918},
  url = {https://www.aclweb.org/anthology/W14-4918},
  urldate = {2021-02-22}
}

@inproceedings{Wachsmuth2016UsingArgumentMining,
  title = {Using {{Argument Mining}} to {{Assess}} the {{Argumentation Quality}} of {{Essays}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Wachsmuth, Henning and Al-Khatib, Khalid and Stein, Benno},
  date = {2016-12},
  pages = {1680--1691},
  publisher = {The COLING 2016 Organizing Committee},
  location = {Osaka, Japan},
  url = {https://www.aclweb.org/anthology/C16-1158},
  urldate = {2019-09-03},
  abstract = {Argument mining aims to determine the argumentative structure of texts. Although it is said to be crucial for future applications such as writing support systems, the benefit of its output has rarely been evaluated. This paper puts the analysis of the output into the focus. In particular, we investigate to what extent the mined structure can be leveraged to assess the argumentation quality of persuasive essays. We find insightful statistical patterns in the structure of essays. From these, we derive novel features that we evaluate in four argumentation-related essay scoring tasks. Our results reveal the benefit of argument mining for assessing argumentation quality. Among others, we improve the state of the art in scoring an essay's organization and its argument strength.},
  eventtitle = {{{COLING}} 2016}
}

@inproceedings{Wachsmuth2017ArgumentationQualityAssessment,
  title = {Argumentation {{Quality Assessment}}: {{Theory}} vs. {{Practice}}},
  shorttitle = {Argumentation {{Quality Assessment}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Wachsmuth, Henning and Naderi, Nona and Habernal, Ivan and Hou, Yufang and Hirst, Graeme and Gurevych, Iryna and Stein, Benno},
  date = {2017-07},
  pages = {250--255},
  publisher = {Association for Computational Linguistics},
  location = {Vancouver, Canada},
  doi = {10.18653/v1/P17-2039},
  url = {https://www.aclweb.org/anthology/P17-2039},
  urldate = {2019-09-04},
  abstract = {Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.},
  eventtitle = {{{ACL}} 2017}
}

@inproceedings{Wachsmuth2017BuildingArgumentSearch,
  title = {Building an {{Argument Search Engine}} for the {{Web}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Argument Mining}}},
  author = {Wachsmuth, Henning and Potthast, Martin and Al Khatib, Khalid and Ajjour, Yamen and Puschmann, Jana and Qu, Jiani and Dorsch, Jonas and Morari, Viorel and Bevendorff, Janek and Stein, Benno},
  date = {2017-09},
  pages = {49--59},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  url = {http://www.aclweb.org/anthology/W17-5106},
  urldate = {2018-10-17},
  abstract = {Computational argumentation is expected to play a critical role in the future of web search. To make this happen, many search-related questions must be revisited, such as how people query for arguments, how to mine arguments from the web, or how to rank them. In this paper, we develop an argument search framework for studying these and further questions. The framework allows for the composition of approaches to acquiring, mining, assessing, indexing, querying, retrieving, ranking, and presenting arguments while relying on standard infrastructure and interfaces. Based on the framework, we build a prototype search engine, called args, that relies on an initial, freely accessible index of nearly 300k arguments crawled from reliable web resources. The framework and the argument search engine are intended as an environment for collaborative research on computational argumentation and its practical evaluation.}
}

@inproceedings{Wachsmuth2017ComputationalArgumentationQuality,
  title = {Computational {{Argumentation Quality Assessment}} in {{Natural Language}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {Wachsmuth, Henning and Naderi, Nona and Hou, Yufang and Bilu, Yonatan and Prabhakaran, Vinodkumar and Thijm, Tim Alberdingk and Hirst, Graeme and Stein, Benno},
  date = {2017-04},
  pages = {176--187},
  publisher = {Association for Computational Linguistics},
  location = {Valencia, Spain},
  url = {https://www.aclweb.org/anthology/E17-1017},
  urldate = {2019-09-04},
  abstract = {Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in natural language processing, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in natural language. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a corpus with 320 arguments, annotated for all 15 dimensions in the taxonomy. Our results establish a common ground for research on computational argumentation quality assessment.},
  eventtitle = {{{EACL}} 2017}
}

@inproceedings{Wachsmuth2017PageRankArgumentRelevance,
  title = {“{{PageRank}}” for {{Argument Relevance}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {Wachsmuth, Henning and Stein, Benno and Ajjour, Yamen},
  date = {2017-04},
  pages = {1117--1127},
  publisher = {Association for Computational Linguistics},
  location = {Valencia, Spain},
  url = {https://www.aclweb.org/anthology/E17-1105},
  urldate = {2019-09-04},
  abstract = {Future search engines are expected to deliver pro and con arguments in response to queries on controversial topics. While argument mining is now in the focus of research, the question of how to retrieve the relevant arguments remains open. This paper proposes a radical model to assess relevance objectively at web scale: the relevance of an argument's conclusion is decided by what other arguments reuse it as a premise. We build an argument graph for this model that we analyze with a recursive weighting scheme, adapting key ideas of PageRank. In experiments on a large ground-truth argument graph, the resulting relevance scores correlate with human average judgments. We outline what natural language challenges must be faced at web scale in order to stepwise bring argument relevance to web search engines.},
  eventtitle = {{{EACL}} 2017}
}

@inproceedings{Wachsmuth2018ArgumentationSynthesisFollowing,
  title = {Argumentation {{Synthesis}} Following {{Rhetorical Strategies}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Wachsmuth, Henning and Stede, Manfred and El Baff, Roxanne and Al-Khatib, Khalid and Skeppstedt, Maria and Stein, Benno},
  date = {2018-08},
  pages = {3753--3765},
  publisher = {Association for Computational Linguistics},
  location = {Santa Fe, New Mexico, USA},
  url = {https://www.aclweb.org/anthology/C18-1318},
  urldate = {2020-09-09},
  abstract = {Persuasion is rarely achieved through a loose set of arguments alone. Rather, an effective delivery of arguments follows a rhetorical strategy, combining logical reasoning with appeals to ethics and emotion. We argue that such a strategy means to select, arrange, and phrase a set of argumentative discourse units. In this paper, we model rhetorical strategies for the computational synthesis of effective argumentation. In a study, we let 26 experts synthesize argumentative texts with different strategies for 10 topics. We find that the experts agree in the selection significantly more when following the same strategy. While the texts notably vary for different strategies, especially their arrangement remains stable. The results suggest that our model enables a strategical synthesis.},
  eventtitle = {{{COLING}} 2018}
}

@inproceedings{Wachsmuth2018RetrievalBestCounterargument,
  title = {Retrieval of the {{Best Counterargument}} without {{Prior Topic Knowledge}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wachsmuth, Henning and Syed, Shahbaz and Stein, Benno},
  editor = {Gurevych, Iryna and Miyao, Yusuke},
  date = {2018-07},
  pages = {241--251},
  publisher = {Association for Computational Linguistics},
  location = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1023},
  url = {https://aclanthology.org/P18-1023/},
  urldate = {2025-09-03},
  abstract = {Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments' premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60\% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.},
  eventtitle = {{{ACL}} 2018}
}

@article{Wagemans2023HowIdentifyArgument,
  title = {How to Identify an Argument Type? {{On}} the Hermeneutics of Persuasive Discourse},
  shorttitle = {How to Identify an Argument Type?},
  author = {Wagemans, Jean H. M.},
  date = {2023-01-01},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {203},
  pages = {117--129},
  issn = {0378-2166},
  doi = {10.1016/j.pragma.2022.11.015},
  url = {https://www.sciencedirect.com/science/article/pii/S0378216622002776},
  urldate = {2025-05-28},
  abstract = {This paper proposes a theory of interpreting argument types as an integral part of a systematic and comprehensive ‘hermeneutics of persuasive discourse’. It first explains how such a hermeneutics can be developed based on pragmatic insights about the use of language for persuasive purposes expressed in the philosophy of argument. Then, after having provided an overview of the main hermeneutical stages involved in interpreting persuasive discourse, the paper focuses on the stage of argument type identification. It formulates a ‘hermeneutics of argument type’ in terms of the Periodic Table of Arguments (PTA), an argument categorization framework systematizing existing accounts of arguments in the broad sense of the term (topoi, loci, argument schemes, fallacies, means of persuasion). For each of the three parameters within this framework, ‘argument form’, ‘argument substance’, and ‘argument lever’, the paper describes how to determine their value by analyzing several examples of natural arguments.}
}

@misc{Wagemans2025ArgumentTypeIdentification,
  title = {Argument {{Type Identification Procedure}}},
  shorttitle = {{{ATIP}}},
  author = {Wagemans, Jean H. M.},
  date = {2025-03-25},
  url = {https://periodic-table-of-arguments.org/argument-type-identification-procedure/},
  urldate = {2025-05-28},
  abstract = {What kind of argument is this? Identifying argument types in the wild can be significantly more complex than analyzing neatly structured examples found in textbooks. The Argument Type Identification Procedure (ATIP), outlined in this document, provides a systematic approach to tackle this issue. The ATIP offers clear guidelines to help identify the type of any given argument in terms of the categorization framework of the Periodic Table of Arguments (PTA). Following a step-by-step procedure, analysts can meticulously examine arguments in real-world discourse. The approach yields a detailed description of the key characteristics of the arguments and prepares the ground for an assessment of their validity, strength, and overall quality. The ATIP facilitates a theoretically informed evaluation, making it a valuable tool for researchers, educators, and anyone engaged in critical thinking and argument-checking.},
  langid = {english},
  organization = {Periodic Table of Arguments}
}

@inproceedings{Walker2012CorpusResearchDeliberation,
  title = {A {{Corpus}} for {{Research}} on {{Deliberation}} and {{Debate}}},
  booktitle = {Proceedings of the {{Eighth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'12)},
  author = {Walker, Marilyn and Tree, Jean Fox and Anand, Pranav and Abbott, Rob and King, Joseph},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Doğan, Mehmet Uğur and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  date = {2012-05},
  pages = {812--817},
  publisher = {European Language Resources Association (ELRA)},
  location = {Istanbul, Turkey},
  url = {https://aclanthology.org/L12-1643/},
  urldate = {2025-07-15},
  abstract = {Deliberative, argumentative discourse is an important component of opinion formation, belief revision, and knowledge discovery; it is a cornerstone of modern civil society. Argumentation is productively studied in branches ranging from theoretical artificial intelligence to political rhetoric, but empirical analysis has suffered from a lack of freely available, unscripted argumentative dialogs. This paper presents the Internet Argument Corpus (IAC), a set of 390,704 posts in 11,800 discussions extracted from the online debate site 4forums.com. A 2866 thread/130,206 post extract of the corpus has been manually sided for topic of discussion, and subsets of this topic-labeled extract have been annotated for several dialogic and argumentative markers: degrees of agreement with a previous post, cordiality, audience-direction, combativeness, assertiveness, emotionality of argumentation, and sarcasm. As an application of this resource, the paper closes with a discussion of the relationship between discourse marker pragmatics, agreement, emotionality, and sarcasm in the IAC corpus.},
  eventtitle = {{{LREC}} 2012}
}

@online{Wallace2024InstructionHierarchyTraining,
  title = {The {{Instruction Hierarchy}}: {{Training LLMs}} to {{Prioritize Privileged Instructions}}},
  shorttitle = {The {{Instruction Hierarchy}}},
  author = {Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex},
  date = {2024-04-19},
  eprint = {2404.13208},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.13208},
  urldate = {2024-04-25},
  abstract = {Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.},
  pubstate = {prepublished}
}

@book{Walton2008ArgumentationSchemes,
  title = {Argumentation {{Schemes}}},
  author = {Walton, Douglas and Reed, Christopher and Macagno, Fabrizio},
  date = {2008-08-04},
  publisher = {Cambridge University Press},
  abstract = {This book provides a systematic analysis of many common argumentation schemes and a compendium of 96 schemes. The study of these schemes, or forms of argument that capture stereotypical patterns of human reasoning, is at the core of argumentation research. Surveying all aspects of argumentation schemes from the ground up, the book takes the reader from the elementary exposition in the first chapter to the latest state of the art in the research efforts to formalize and classify the schemes, outlined in the last chapter. It provides a systematic and comprehensive account, with notation suitable for computational applications that increasingly make use of argumentation schemes.},
  isbn = {1-316-58313-9}
}

@book{Walton2013ArgumentationSchemesPresumptive,
  title = {Argumentation {{Schemes}} for {{Presumptive Reasoning}}},
  author = {Walton, Douglas},
  date = {2013-11-05},
  publisher = {Routledge},
  doi = {10.4324/9780203811160},
  url = {https://www.taylorfrancis.com/books/9781136687068},
  urldate = {2018-09-05},
  abstract = {Recent concerns with the evaluation of argumentation in informal logic and speech communication center around nondemonstrative arguments that lead to tentative},
  isbn = {978-1-136-68706-8},
  langid = {english}
}

@article{Walton2013TeleologicalJustificationArgumentation,
  title = {Teleological {{Justification}} of {{Argumentation Schemes}}},
  author = {Walton, Douglas and Sartor, Giovanni},
  date = {2013-05-01},
  journaltitle = {Argumentation},
  shortjournal = {Argumentation},
  volume = {27},
  number = {2},
  pages = {111--142},
  issn = {1572-8374},
  doi = {10.1007/s10503-012-9262-y},
  url = {https://link.springer.com/article/10.1007/s10503-012-9262-y},
  abstract = {Argumentation schemes are forms of reasoning that are fallible but correctable within a self-correcting framework. Their use provides a basis for taking rational action or for reasonably accepting a conclusion as a tentative hypothesis, but they are not deductively valid. We argue that teleological reasoning can provide the basis for justifying the use of argument schemes both in monological and dialogical reasoning. We consider how such a teleological justification, besides being inspired by the aim of directing a bounded cognizer to true belief and correct choices, needs to take into account the attitudes of dialogue partners as well as normative models of dialogue and communicative activity types, in particular social and cultural settings.},
  langid = {english}
}

@article{Walton2016ClassificationSystemArgumentation,
  title = {A Classification System for Argumentation Schemes},
  author = {Walton, Douglas and Macagno, Fabrizio},
  date = {2016-06-07},
  journaltitle = {Argument \& Computation},
  shortjournal = {Argument \& Computation},
  volume = {6},
  number = {3},
  pages = {219--245},
  doi = {10.1080/19462166.2015.1123772},
  url = {http://content.iospress.com/doi/1080/19462166.2015.1123772}
}

@article{Wambsganss2021ArgueBotConversationalAgent,
  title = {{{ArgueBot}}: {{A Conversational Agent}} for {{Adaptive Argumentation Feedback}}},
  shorttitle = {{{ArgueBot}}},
  author = {Wambsganss, Thiemo and Guggisberg, Sebastian and Soellner, Matthias},
  date = {2021-02-16},
  journaltitle = {Wirtschaftsinformatik 2021 Proceedings},
  url = {https://aisel.aisnet.org/wi2021/PHuman/Track11/2},
  abstract = {By combining recent advances in Natural Language Processing and Conversational Agent (CAs), we suggest a new form of human-computer interaction for individuals to receive formative feedback on their argumentation to help them to foster their logical reasoning skills. Hence, we introduce ArgueBot, a conversational agent, that provides adaptive feedback on students' logical argumentation. We, therefore, 1) leveraged a corpus of argumentative student-written peer-reviews in German, 2) trained, tuned, and benchmarked a model that identifies claims, premises and non-argumentative sections of a given text, and 3) built a conversational feedback tool. We evaluated ArgueBot in a proof-of-concept evaluation with students. The evaluation results regarding technology acceptance, the performance of our trained model, and the qualitative feedback indicate the potential of leveraging recent advances in Natural Language Processing for new human-computer interaction use cases for scalable educational feedback.}
}

@online{Wan2024TnTLLMTextMining,
  title = {{{TnT-LLM}}: {{Text Mining}} at {{Scale}} with {{Large Language Models}}},
  shorttitle = {{{TnT-LLM}}},
  author = {Wan, Mengting and Safavi, Tara and Jauhar, Sujay Kumar and Kim, Yujin and Counts, Scott and Neville, Jennifer and Suri, Siddharth and Shah, Chirag and White, Ryen W. and Yang, Longqi and Andersen, Reid and Buscher, Georg and Joshi, Dhruv and Rangan, Nagu},
  date = {2024-03-18},
  eprint = {2403.12173},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.12173},
  url = {http://arxiv.org/abs/2403.12173},
  urldate = {2024-03-30},
  abstract = {Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.},
  pubstate = {prepublished}
}

@article{Wang2014WebImageReRanking,
  title = {Web {{Image Re-Ranking Using Query-Specific Semantic Signatures}}},
  author = {Wang, Xiaogang and Qiu, Shi and Liu, Ke and Tang, Xiaoou},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {4},
  pages = {810--823},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.214},
  url = {https://ieeexplore.ieee.org/document/6654170},
  urldate = {2025-07-15},
  abstract = {Image re-ranking, as an effective way to improve the results of web-based image search, has been adopted by current commercial search engines such as Bing and Google. Given a query keyword, a pool of images are first retrieved based on textual information. By asking the user to select a query image from the pool, the remaining images are re-ranked based on their visual similarities with the query image. A major challenge is that the similarities of visual features do not well correlate with images’ semantic meanings which interpret users’ search intention. Recently people proposed to match images in a semantic space which used attributes or reference classes closely related to the semantic meanings of images as basis. However, learning a universal visual semantic space to characterize highly diverse images from the web is difficult and inefficient. In this paper, we propose a novel image re-ranking framework, which automatically offline learns different semantic spaces for different query keywords. The visual features of images are projected into their related semantic spaces to get semantic signatures. At the online stage, images are re-ranked by comparing their semantic signatures obtained from the semantic space specified by the query keyword. The proposed query-specific semantic signatures significantly improve both the accuracy and efficiency of image re-ranking. The original visual features of thousands of dimensions can be projected to the semantic signatures as short as 25 dimensions. Experimental results show that 25-40 percent relative improvement has been achieved on re-ranking precisions compared with the state-of-the-art methods.}
}

@unpublished{Wang2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Open Knowledge Graphs}}},
  author = {Wang, Chenguang and Liu, Xiao and Song, Dawn},
  date = {2020-10-22},
  eprint = {2010.11967},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11967},
  urldate = {2021-09-06},
  abstract = {This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.}
}

@unpublished{Wang2021EntailmentFewShotLearner,
  title = {Entailment as {{Few-Shot Learner}}},
  author = {Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi and Ma, Hao},
  date = {2021-04-29},
  eprint = {2104.14690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.14690},
  urldate = {2022-04-20},
  abstract = {Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\textbackslash\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.}
}

@inproceedings{Wang2023ArgumentCounterArgumentGeneration,
  title = {Argument and~{{Counter-Argument Generation}}: {{A Critical Survey}}},
  shorttitle = {Argument and~{{Counter-Argument Generation}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Wang, Xiaoou and Cabrio, Elena and Villata, Serena},
  editor = {Métais, Elisabeth and Meziane, Farid and Sugumaran, Vijayan and Manning, Warren and Reiff-Marganiec, Stephan},
  date = {2023},
  pages = {500--510},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-35320-8_37},
  abstract = {Argument Generation (AG) is becoming an increasingly active research topic in Natural Language Processing (NLP), and a large variety of terms has been used to highlight different aspects and methods of AG such as argument construction, argument retrieval, argument synthesis and argument summarization, producing a vast literature. This article aims to draw a comprehensive picture of the literature concerning argument generation and counter-argument generation (CAG). Despite the increasing interest on this topic, no attempt has been made yet to critically review the diverse and rich literature in AG and CAG. By confronting works from the relevant subareas of NLP, we provide a holistic vision that is essential for future works aiming to produce understandable, convincing and ethically sound arguments and counter-arguments.},
  isbn = {978-3-031-35320-8},
  langid = {english}
}

@inproceedings{Wang2023ChatGPTGoodNLG,
  title = {Is {{ChatGPT}} a {{Good NLG Evaluator}}? {{A Preliminary Study}}},
  shorttitle = {Is {{ChatGPT}} a {{Good NLG Evaluator}}?},
  booktitle = {Proceedings of the 4th {{New Frontiers}} in {{Summarization Workshop}}},
  author = {Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun, Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie},
  editor = {Dong, Yue and Xiao, Wen and Wang, Lu and Liu, Fei and Carenini, Giuseppe},
  date = {2023-12},
  pages = {1--11},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.newsum-1.1},
  url = {https://aclanthology.org/2023.newsum-1.1},
  urldate = {2024-06-21},
  abstract = {Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.},
  eventtitle = {{{NewSum}} 2023}
}

@inproceedings{Wang2023SimLMPretrainingRepresentation,
  title = {{{SimLM}}: {{Pre-training}} with {{Representation Bottleneck}} for {{Dense Passage Retrieval}}},
  shorttitle = {{{SimLM}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {2244--2258},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.125},
  url = {https://aclanthology.org/2023.acl-long.125/},
  urldate = {2025-07-15},
  abstract = {In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 (Santhanam et al., 2021) which incurs significantly more storage cost. Our code and model checkpoints are available at https://github.com/microsoft/unilm/tree/master/simlm .},
  eventtitle = {{{ACL}} 2023}
}

@online{Wang2024ChainofThoughtReasoningPrompting,
  title = {Chain-of-{{Thought Reasoning Without Prompting}}},
  author = {Wang, Xuezhi and Zhou, Denny},
  date = {2024-02-15},
  eprint = {2402.10200},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.10200},
  url = {http://arxiv.org/abs/2402.10200},
  urldate = {2024-02-19},
  abstract = {In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textbackslash textit\{decoding\} process. Rather than conventional greedy decoding, we investigate the top-\$k\$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textbackslash textit\{intrinsic\} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.},
  pubstate = {prepublished}
}

@inproceedings{Wang2024DAPRBenchmarkDocumentAware,
  title = {{{DAPR}}: {{A Benchmark}} on {{Document-Aware Passage Retrieval}}},
  shorttitle = {{{DAPR}}},
  author = {Wang, Kexin and Reimers, Nils and Gurevych, Iryna},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  date = {2024-08},
  pages = {4313--4330},
  publisher = {Association for Computational Linguistics},
  location = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.236},
  url = {https://aclanthology.org/2024.acl-long.236},
  urldate = {2024-09-30},
  abstract = {The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task Document-Aware Passage Retrieval (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5\%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available.},
  eventtitle = {{{ACL}} 2024}
}

@article{Wang2024SurveyLargeLanguage,
  title = {A Survey on Large Language Model Based Autonomous Agents},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
  date = {2024-03-22},
  journaltitle = {Frontiers of Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {18},
  number = {6},
  pages = {186345},
  issn = {2095-2236},
  doi = {10.1007/s11704-024-40231-1},
  url = {https://doi.org/10.1007/s11704-024-40231-1},
  urldate = {2024-09-17},
  abstract = {Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
  langid = {english}
}

@inproceedings{Wang2025CaseBasedReasoningDiffusion,
  title = {Case-{{Based Reasoning}} with~{{Diffusion Model}} for~{{Ransomware Detection}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wang, Haofan and Hardy, Jarrod and Kandah, Farah},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  pages = {469--483},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_31},
  abstract = {In recent years, the threat and damage caused by ransomware have been steadily increasing. Although many detection methods have been proposed, network security remains a severe challenge due to the continuous emergence of new attack techniques, data imbalance, low detection rates of existing methods, and the lack of interpretability in model decision-making. This study proposes a Case-Based Reasoning with Diffusion Model for Ransomware Detection (CBR-DRD), which classifies network traffic information without feature loss or redundancy by converting it into RGB images. The dataset is then augmented using image generation based on a U-Net diffusion model. Features extracted by a Swin Transformer are used to construct a case base, and a weighted K-Nearest Neighbors (KNN) algorithm is employed to classify the traffic by computing the similarity between a given sample and existing cases, to determine whether it is associated with ransomware activity. Compared with two similar ransomware detection approaches and commonly used generative models, namely the GAN model and a ResNet-based diffusion model designed to address data imbalance, the proposed method achieves superior performance on both the USTC-TFC2016 and ISOT datasets.},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@article{Wang2025WhenAutomatedFactchecking,
  title = {When Automated Fact-Checking Meets Argumentation: {{Unveiling}} Fake News through Argumentative Evidence},
  shorttitle = {When Automated Fact-Checking Meets Argumentation},
  author = {Wang, Xiaoou and Cabrio, Elena and Villata, Serena},
  date = {2025-10-01},
  journaltitle = {Argument \& Computation},
  volume = {16},
  number = {3},
  pages = {405--424},
  publisher = {SAGE Publications},
  issn = {1946-2166},
  doi = {10.1177/19462174251330980},
  url = {https://doi.org/10.1177/19462174251330980},
  urldate = {2025-09-03},
  abstract = {The need for automated fact-checking has become urgent with the rise of misleading content on social media. Recently, Fake News Classification (FNC) has evolved to incorporate justifications provided by fact-checkers to explain their decisions. In this work, we argue that an argumentative representation of fact-checkers’ justifications can improve the precision and explainability of FNC systems. To address this challenging task, we present LIARArg, a novel linguistic resource composed of 2,832 news and their justifications. LIARArg extends the 6-label FNC dataset LIAR-PLUS with argumentation structures, leading to the first FNC dataset annotated with argument components (claim and premise) and fine-grained relations (attack, support, partial support and partial attack). To integrate argumentation in FNC, we propose a novel joint learning method combining, for the first time, Argument Mining and FNC which outperforms state-of-the-art approaches, especially for news with intermediate truthfulness labels. Besides, our experimental setting demonstrates that fine-grained relations allow an extra performance boost. We also show that the argumentative representation of human justifications can be exploited in a Chain-of-Thought manner both in prompts and model output, paving a promising avenue for research in explainable fact-checking. Finally, our fully automated pipeline shows that integrating argumentation into FNC is not only feasible but also effective.},
  langid = {english}
}

@online{Warner2024SmarterBetterFaster,
  title = {Smarter, {{Better}}, {{Faster}}, {{Longer}}: {{A Modern Bidirectional Encoder}} for {{Fast}}, {{Memory Efficient}}, and {{Long Context Finetuning}} and {{Inference}}},
  shorttitle = {Smarter, {{Better}}, {{Faster}}, {{Longer}}},
  author = {Warner, Benjamin and Chaffin, Antoine and Clavié, Benjamin and Weller, Orion and Hallström, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and Cooper, Nathan and Adams, Griffin and Howard, Jeremy and Poli, Iacopo},
  date = {2024-12-18},
  eprint = {2412.13663},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.13663},
  url = {http://arxiv.org/abs/2412.13663},
  urldate = {2024-12-19},
  abstract = {Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.},
  pubstate = {prepublished}
}

@inproceedings{Watson2024CaseBasedPersistentMemory,
  title = {A {{Case-Based Persistent Memory}} for a {{Large Language Model}}},
  booktitle = {Proceedings of the {{Workshops}} at the 32nd {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2024)},
  author = {Watson, Ian},
  editor = {Malburg, Lukas},
  date = {2024-07-01},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3708},
  pages = {19--25},
  publisher = {CEUR},
  location = {Mérida, Mexico},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3708/#CBRLLM02},
  urldate = {2025-03-14},
  eventtitle = {{{ICCBR}} 2024 {{Workshop Proceedings}}},
  langid = {english}
}

@inproceedings{Weber2004CBRFlowEnablingAdaptive,
  title = {{{CBRFlow}}: {{Enabling Adaptive Workflow Management Through Conversational Case-Based Reasoning}}},
  shorttitle = {{{CBRFlow}}},
  booktitle = {Advances in {{Case-Based Reasoning}}},
  author = {Weber, Barbara and Wild, Werner and Breu, Ruth},
  editor = {Funk, Peter and González Calero, Pedro A.},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {434--448},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-28631-8_32},
  abstract = {In this paper we propose an architecture for an adaptive workflow management system (WFMS) and present the research prototype CBRFlow. CBRFlow extends workflow execution with conversational case-based reasoning (CCBR) to adapt the predefined workflow model to changing circumstances and to provide the WFMS with learning capabilities. Business rules within the predefined workflow model are annotated during run-time with context-specific information in the form of cases using the CCBR sub-system. When case reuse becomes frequent, the cases are manually refactored into rules to foster automatic execution. This feedback supports continuous process improvement, resulting in more manageable and more efficient business processes over time.},
  isbn = {978-3-540-28631-8},
  langid = {english}
}

@article{Weber2005TextualCasebasedReasoning,
  title = {Textual Case-Based Reasoning},
  author = {Weber, Rosina O. and Ashley, Kevin D. and Brüninghaus, Stefanie},
  date = {2005-09},
  journaltitle = {The Knowledge Engineering Review},
  shortjournal = {The Knowledge Engineering Review},
  volume = {20},
  number = {3},
  pages = {255--260},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S0269888906000713},
  url = {https://www.cambridge.org/core/product/identifier/S0269888906000713/type/journal_article},
  urldate = {2019-08-20},
  abstract = {This commentary provides a definition of textual case-based reasoning (TCBR) and surveys research contributions according to four research questions. We also describe how TCBR can be distinguished from text mining and information retrieval. We conclude with potential directions for TCBR research.},
  langid = {english}
}

@inproceedings{Webster1992TokenizationInitialPhase,
  title = {Tokenization {{As}} the {{Initial Phase}} in {{NLP}}},
  booktitle = {Proceedings of the 14th {{Conference}} on {{Computational Linguistics}}},
  author = {Webster, Jonathan J. and Kit, Chunyu},
  date = {1992},
  series = {{{COLING}} '92},
  volume = {4},
  pages = {1106--1110},
  publisher = {Association for Computational Linguistics},
  location = {Nantes, France},
  doi = {10.3115/992424.992434},
  abstract = {In this paper, the authors address the significance and complexity of tokenization, the beginning step of NLP. Notions of word and token are discussed and defined from the viewpoints of lexicography and pragmatic implementation, respectively. Automatic segmentation of Chinese words is presented as an illustration of tokenization. Practical approaches to identification of compound tokens in English, such as idioms, phrasal verbs and fixed expressions, are developed.}
}

@article{Wegerif2008DialogicDialecticSignificance,
  title = {Dialogic or Dialectic? {{The}} Significance of Ontological Assumptions in Research on Educational Dialogue},
  shorttitle = {Dialogic or Dialectic?},
  author = {Wegerif, Rupert},
  date = {2008-06},
  journaltitle = {British Educational Research Journal},
  shortjournal = {British Educational Res J},
  volume = {34},
  number = {3},
  pages = {347--361},
  issn = {0141-1926, 1469-3518},
  doi = {10.1080/01411920701532228},
  url = {https://bera-journals.onlinelibrary.wiley.com/doi/10.1080/01411920701532228},
  urldate = {2025-09-09},
  abstract = {This article explores the relationship between ontological assumptions and studies of educational dialogue through a focus on Bakhtin's ‘dialogic’. The term dialogic is frequently appropriated to a modernist framework of assumptions, in particular the neo‐Vygotskian or sociocultural tradition. However, Vygotsky's theory of education is dialectic, not dialogic. From a dialogic perspective the difference between voices in dialogue is constitutive of meaning in such a way that it makes no sense to imagine ‘overcoming’ this difference. By contrast, due to the implicit assumption that meaning is ultimately grounded on identity rather than upon difference, the dialectic perspective applied by Vygotsky interprets differences as ‘contradictions’ that need to be overcome or transcended. A case study of research on exploratory talk is used to illustrate the potential for a fruitful relationship between ‘high level’ theory and research that is relevant to classroom practice.},
  langid = {english}
}

@inproceedings{Wei2021FinetunedLanguageModels,
  title = {Finetuned {{Language Models}} Are {{Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=gEZrGCozdqR},
  urldate = {2025-09-11},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{Weich2025IntegrationTimeSeries,
  title = {Integration of~{{Time Series Embedding}} for~{{Efficient Retrieval}} in~{{Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Weich, Justin and Schultheis, Alexander and Hoffmann, Maximilian and Bergmann, Ralph},
  editor = {Bichindaritz, Isabelle and López, Beatriz},
  date = {2025},
  pages = {328--344},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-96559-3_22},
  abstract = {The increasing volume of time series data in Industry 4.0 applications creates substantial challenges for real-time data analysis. Such analyses that are conducted in the research area of Temporal Case-Based Reasoning (TCBR) face performance problems due to complex similarity measures. One potential approach already proven in other domains for addressing these problems is the usage of embedding techniques for time series data, which map these data into a simplified vector representation. Therefore, this paper investigates the integration of time series embedding techniques in the context of Case-Based Reasoning (CBR) to improve retrieval efficiency. Therefore, requirements for the application of embedding techniques in CBR are derived. A systematic literature study identifies possible approaches that are analyzed based on the requirements, with the result that no approach is suitable for the application. Therefore, a novel embedding architecture is proposed, using a Siamese neural network approach that can be trained with similarity values. The architecture is prototypically implemented in the ProCAKE framework and evaluated in an Internet of Things use case from a smart factory. The results demonstrate that the embedding-based retrieval achieves classification performance comparable to traditional similarity measures while significantly reducing retrieval time.},
  isbn = {978-3-031-96559-3},
  langid = {english}
}

@inproceedings{Weidmann2023CLEARNESSCoreferenceResolution,
  title = {{{CLEARNESS}}: {{Coreference Resolution}} for {{Generating}} and {{Ranking Arguments Extracted}} from {{Debate Portals}} for {{Queries}}},
  shorttitle = {{{CLEARNESS}}},
  booktitle = {Lernen, {{Wissen}}, {{Daten}}, {{Analysen}} ({{LWDA}}) {{Conference Proceedings}}},
  author = {Weidmann, Johannes and Dumani, Lorik and Schenkel, Ralf},
  editor = {Leyer, Michael and Wichmann, Johannes},
  date = {2023-10-09},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3630},
  pages = {161--174},
  publisher = {CEUR},
  location = {Marburg, Germany},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3630/#paper15},
  urldate = {2024-02-09},
  abstract = {Argumentation has always been used by humans to convince other people of certain viewpoints, e.g., to push through personal interests, or to resolve conflicts. The research field of computational argumentation deals with the extraction, analysis, retrieval, and generation of arguments in natural language texts. Modern argument search engines are able to generate appropriate arguments for controversial topics, very often based on posts taken from debate portals. However, an important issue is that posts in these portals are often quite long and incomprehensible. Apart from that, long posts in debate portals cannot be arguments by definition as not every text is argumentative. In this paper, we dock on preliminary work about argument search engines and present CLEARNESS (Coreference resoLution for gEnerating and rAnking aRgumeNts Extracted from portalS for querieS), an approach to generate arguments in response to a query. The arguments we focus on consist of two essential elements: a claim, which is a point of view on a topic, and a premise, which provides the reasons or evidence backing up the claim. While previous works address issues like the generation of claims or the creation of abstract summaries of texts, we pursue a high-precision retrieval approach. We extract fine-grained premises from argumentative texts and modify these through coreference resolution to obtain an isolated text that is –although short– both coherent and completed. We first build up a database by extracting arguments from the Args corpus of arguments from a number of popular debate portals. In these argumentative texts, we identify all coreferences and resolve them. Next, we examine classic and state-of-the-art approaches to rank arguments in response to a query. Additionally, we study the ranking behavior by utilizing query expansion. Lastly, we investigate the performance on relevance and coreference resolution.},
  eventtitle = {Lernen, {{Wissen}}, {{Daten}}, {{Analysen}} 2023},
  langid = {english}
}

@inproceedings{Weitl-Harms2024AutomatedKnowledgeDiscovery,
  title = {Toward {{Automated Knowledge Discovery}} in {{Case-Based Reasoning}}},
  booktitle = {Proceedings of the {{Thirty-Seventh International Florida Artificial Intelligence Research Society Conference}}},
  author = {Weitl-Harms, Sherri and Hastings, John and Powell, Jay},
  date = {2024-05-13},
  volume = {37},
  publisher = {AAAI Press},
  location = {Sandestin Beach, FL, USA},
  doi = {10.32473/flairs.37.1.135434},
  url = {https://journals.flvc.org/FLAIRS/article/view/135434},
  urldate = {2025-07-07},
  abstract = {Automated Case Elicitation (ACE) enables case-based reasoning (CBR) systems to automatically acquire knowledge through real-time exploration and interaction with environments. CBR is an explainable AI methodology, where decisions are based on previous encounters. ACE combined with CBR continues learning as it is being deployed, and produces specific cases that can be reviewed by humans, unlike pretrained large language models (LLMs) that learn by training offline on prior data. ACE and CBR may be useful methods to gather training data for use with generative AI, or to help them to adapt on the fly. This research explores ACE's potential by applying it to chess and conducting extensive experiments against Stockfish, the world's highest rated chess engine. An ACE agent was developed that combines random exploration with shallow alpha-beta search for novel game states. Results over 1000+ games showed the ACE player defeated Stockfish in nearly 10\% of games—a notable achievement given Stockfish's extreme strength. Notably, the ACE agent required only 0.1 seconds per game compared an average of 8 minutes for Stockfish, while still gradually improving its win rate through accrued experience. Detailed analyses revealed how the relaxation of ACE's case matching criteria along with selective retention of useful cases enabled accumulation of strategic chess knowledge. The research provides valuable insights into ACE's proficiency for knowledge discovery in complex, adversarial domains. It lays groundwork for integrating ACE, an unsupervised CBR learner, with modern deep learning techniques like neural networks and large language models to combine the strengths of symbolic and subsymbolic AI. By demonstrating ACE's ability to extract strategic knowledge against world-class opponents, this work highlights its potential for impact across gaming, autonomous systems, and other complex problem-solving domains.},
  eventtitle = {{{FLAIRS}}},
  langid = {english}
}

@online{Weller2025TheoreticalLimitationsEmbeddingBased,
  title = {On the {{Theoretical Limitations}} of {{Embedding-Based Retrieval}}},
  author = {Weller, Orion and Boratko, Michael and Naim, Iftekhar and Lee, Jinhyuk},
  date = {2025-08-28},
  eprint = {2508.21038},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2508.21038},
  url = {http://arxiv.org/abs/2508.21038},
  urldate = {2025-09-01},
  abstract = {Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.},
  pubstate = {prepublished}
}

@inproceedings{Wells2020DatastoresArgumentationData,
  title = {Datastores for {{Argumentation Data}}},
  booktitle = {Proceedings of the 20th {{Workshop}} on {{Computational Models}} of {{Natural Argument}}},
  author = {Wells, Simon},
  editor = {Grasso, Floriana and Green, Nancy L. and Schneider, Jodi and Wells, Simon},
  date = {2020},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2669},
  pages = {31--40},
  publisher = {CEUR-WS.org},
  location = {Perugia, Italy},
  url = {http://ceur-ws.org/Vol-2669/paper4.pdf},
  urldate = {2022-04-21},
  eventtitle = {{{COMMA}} 2020}
}

@inproceedings{Wells2020OpenArgumentationPLatform,
  title = {The {{Open Argumentation PLatform}} ({{OAPL}})},
  booktitle = {Computational {{Models}} of {{Argument}}},
  author = {Wells, Simon},
  date = {2020},
  pages = {475--476},
  publisher = {IOS Press},
  doi = {10.3233/FAIA200541},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA200541},
  urldate = {2021-03-28}
}

@inproceedings{Wenzlitschke2022UsingBERTRetrieve,
  title = {Using {{BERT}} to Retrieve Relevant and Argumentative Sentence Pairs},
  booktitle = {Proceedings of the {{Working Notes}} of {{CLEF}} 2022 - {{Conference}} and {{Labs}} of the {{Evaluation Forum}}},
  author = {Wenzlitschke, Nils and Sülzle, Pia},
  editor = {Faggioli, Guglielmo and Ferro, Nicola and Hanbury, Allan and Potthast, Martin},
  date = {2022-09},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3180},
  pages = {3149--3163},
  publisher = {CEUR},
  location = {Bologna, Italy},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3180/#paper-264},
  urldate = {2025-03-24},
  eventtitle = {{{CLEF}} 2022 {{Working Notes}}},
  langid = {english}
}

@inproceedings{Wijekoon2023CBRDrivenInteractive,
  title = {{{CBR Driven Interactive Explainable AI}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wijekoon, Anjana and Wiratunga, Nirmalie and Martin, Kyle and Corsar, David and Nkisi-Orji, Ikechukwu and Palihawadana, Chamath and Bridge, Derek and Pradeep, Preeja and Agudo, Belen Diaz and Caro-Martínez, Marta},
  editor = {Massie, Stewart and Chakraborti, Sutanu},
  date = {2023},
  pages = {169--184},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-40177-0_11},
  abstract = {Explainable AI~(XAI) can greatly enhance user trust and satisfaction in AI-assisted decision-making processes. Numerous explanation techniques (explainers) exist in the literature, and recent findings suggest that addressing multiple user needs requires employing a combination of these explainers. We refer to such combinations as explanation strategies. This paper introduces iSee - Intelligent Sharing of Explanation Experience, an interactive platform that facilitates the reuse of explanation strategies and promotes best practices in XAI by employing the Case-based Reasoning~(CBR) paradigm. iSee uses an ontology-guided approach to effectively capture explanation requirements, while a behaviour tree-driven conversational chatbot captures user experiences of interacting with the explanations and provides feedback. In a case study, we illustrate the iSee CBR system capabilities by formalising a real-world radiograph fracture detection system and demonstrating how each interactive tools facilitate the CBR processes.},
  isbn = {978-3-031-40177-0},
  langid = {english}
}

@inproceedings{Wilkerson2021CombiningKnowledgeEngineeredNetworkExtracted,
  title = {On {{Combining Knowledge-Engineered}} and {{Network-Extracted Features}} for~{{Retrieval}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wilkerson, Zachary and Leake, David and Crandall, David J.},
  editor = {Sánchez-Ruiz, Antonio A. and Floyd, Michael W.},
  date = {2021},
  pages = {248--262},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-86957-1_17},
  abstract = {The quality of case retrieval in case-based reasoning (CBR) systems depends on assigning appropriate case indices. Defining feature vocabularies for indexing is an important knowledge acquisition problem for CBR, often addressed by hand. The manual process may result in high-quality vocabularies, but at considerable effort and expense, and it may be difficult for non-symbolic input such as images. Recently, the ability of deep learning (DL) to identify important features has made it appealing for learning to assign case features. However, such methods may miss features apparent to knowledge engineers. This paper presents a case study on methods for combining benefits of both engineered and DL-generated features. It considers case-based classification of cases described by both symbolic features and images. It evaluates the power of both types of features individually, examines how quality of engineered feature information affects their combined benefit, and tests network methods to generate weights for their combination. Experimental results show that in the test domain under suitable circumstances, the combined approach can outperform either method individually.},
  isbn = {978-3-030-86957-1},
  langid = {english}
}

@inproceedings{Wilkerson2024ImplementingCaseBasedReasoning,
  title = {On {{Implementing Case-Based Reasoning}} with~{{Large Language Models}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wilkerson, Kaitlynne and Leake, David},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {404--417},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_26},
  abstract = {Systems based on Large Language Models (LLMs), such as ChatGPT, have impressive performance but also well-known issues with erroneous output. Retrieval Augmented Generation (RAG), which typically presents the LLM with text snippets of additional knowledge retrieved from an external knowledge base, is a popular method for increasing LLM accuracy. This paper presents initial studies exploring augmenting LLMs with cases rather than snippets and prompting LLMs towards performing case-based reasoning. The studies consider four possible scenarios, exploring the potential benefit of LLMs performing different subparts of the CBR process: (1) a scenario in which the LLM is prompted to adapt a presented case, (2) a scenario in which the LLM is first prompted to perform similarity assessment to select a case from a set of candidates, and then to adapt the selected case, (3) a scenario in which the LLM is prompted to select the two most similar cases to a problem and generate an adapted/combined solution in light of both, and (4) a scenario in which the LLM selects the nearest neighbor and nearest unlike neighbor and generates an adapted/combined solution based on both. Results of tests using Llama and ChatGPT are encouraging for the accuracy benefits of providing LLMs with cases and raise questions for future study.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@article{Winston1980LearningReasoningAnalogy,
  title = {Learning and Reasoning by Analogy},
  author = {Winston, Patrick H.},
  date = {1980-12-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {23},
  number = {12},
  pages = {689--703},
  issn = {0001-0782},
  doi = {10.1145/359038.359042},
  url = {https://doi.org/10.1145/359038.359042},
  urldate = {2020-05-26},
  abstract = {We use analogy when we say something is a Cinderella story and when we learn about resistors by thinking about water pipes. We also use analogy when we learn subjects like economics, medicine, and law. This paper presents a theory of analogy and describes an implemented system that embodies the theory. The specific competence to be understood is that of using analogies to do certain kinds of learning and reasoning. Learning takes place when analogy is used to generate a constraint description in one domain, given a constraint description in another, as when we learn Ohm's law by way of knowledge about water pipes. Reasoning takes place when analogy is used to answer questions about one situation, given another situation that is supposed to be a precedent, as when we answer questions about Hamlet by way of knowledge about Macbeth.}
}

@inproceedings{Wiratunga2024CBRRAGCaseBasedReasoning,
  title = {{{CBR-RAG}}: {{Case-Based Reasoning}} for~{{Retrieval Augmented Generation}} in~{{LLMs}} for~{{Legal Question Answering}}},
  shorttitle = {{{CBR-RAG}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wiratunga, Nirmalie and Abeyratne, Ramitha and Jayawardena, Lasal and Martin, Kyle and Massie, Stewart and Nkisi-Orji, Ikechukwu and Weerasinghe, Ruvan and Liret, Anne and Fleisch, Bruno},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {445--460},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_29},
  abstract = {Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle’s initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR’s case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.},
  eventtitle = {{{ICCBR}} 2024},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Witten1999KEAPracticalAutomatic,
  title = {{{KEA}}: Practical Automatic Keyphrase Extraction},
  shorttitle = {{{KEA}}},
  booktitle = {Proceedings of the Fourth {{ACM}} Conference on {{Digital}} Libraries},
  author = {Witten, Ian H. and Paynter, Gordon W. and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig G.},
  date = {1999-08-01},
  series = {{{DL}} '99},
  pages = {254--255},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/313238.313437},
  url = {https://doi.org/10.1145/313238.313437},
  urldate = {2021-02-07},
  isbn = {978-1-58113-145-1}
}

@inproceedings{Wolf2020TransformersStateoftheArtNatural,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor = {Liu, Qun and Schlangen, David},
  date = {2020-10},
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://aclanthology.org/2020.emnlp-demos.6/},
  urldate = {2025-07-15},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.}
}

@thesis{Wong2022ImprovingQualityAutomated,
  type = {mathesis},
  title = {Improving {{Quality}} of {{Automated Constructed Argument Graphs}}: {{Towards}} a {{Pipeline}} for {{Argument Graph Reconstruction}}},
  author = {Wong, Wai Lun},
  date = {2022-03-28},
  institution = {Trier University},
  location = {Trier, Germany},
  langid = {english}
}

@inproceedings{Wu1994VerbsSemanticsLexical,
  title = {Verbs Semantics and Lexical Selection},
  booktitle = {Proceedings of the 32nd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Zhibiao and Palmer, Martha},
  date = {1994-06-27},
  series = {{{ACL}} '94},
  pages = {133--138},
  publisher = {Association for Computational Linguistics},
  location = {USA},
  doi = {10.3115/981732.981751},
  url = {https://doi.org/10.3115/981732.981751},
  urldate = {2021-02-17},
  abstract = {This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentences as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.}
}

@inproceedings{Wu2021DeepLearningGraphs,
  title = {Deep {{Learning}} on {{Graphs}} for {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Tutorials}}},
  author = {Wu, Lingfei and Chen, Yu and Ji, Heng and Li, Yunyao},
  date = {2021-06},
  pages = {11--14},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  url = {https://www.aclweb.org/anthology/2021.naacl-tutorials.3},
  urldate = {2021-06-09},
  abstract = {Due to its great power in modeling non-Euclidean data like graphs or manifolds, deep learning on graph techniques (i.e., Graph Neural Networks (GNNs)) have opened a new door to solving challenging graph-related NLP problems. There has seen a surge of interests in applying deep learning on graph techniques to NLP, and has achieved considerable success in many NLP tasks, ranging from classification tasks like sentence classification, semantic role labeling and relation extraction, to generation tasks like machine translation, question generation and summarization. Despite these successes, deep learning on graphs for NLP still face many challenges, including automatically transforming original text sequence data into highly graph-structured data, and effectively modeling complex data that involves mapping between graph-based inputs and other highly structured output data such as sequences, trees, and graph data with multi-types in both nodes and edges. This tutorial will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, hands-on demonstration sessions will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library – Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks.}
}

@inproceedings{Wurf2022SimilarDifferentSimple,
  title = {Similar but {{Different}}: {{Simple Re-ranking Approaches}} for {{Argument Retrieval}}},
  shorttitle = {Similar but {{Different}}},
  booktitle = {Proceedings of the {{Working Notes}} of {{CLEF}} 2022 - {{Conference}} and {{Labs}} of the {{Evaluation Forum}}},
  author = {Würf, Jerome},
  editor = {Faggioli, Guglielmo and Ferro, Nicola and Hanbury, Allan and Potthast, Martin},
  date = {2022-09},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3180},
  pages = {3164--3177},
  publisher = {CEUR},
  location = {Bologna, Italy},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3180/#paper-265},
  urldate = {2025-03-24},
  eventtitle = {{{CLEF}} 2022 {{Working Notes}}},
  langid = {english}
}

@online{Wynter2025ThinLineComprehension,
  title = {The {{Thin Line Between Comprehension}} and {{Persuasion}} in {{LLMs}}},
  author = {family=Wynter, given=Adrian, prefix=de, useprefix=false and Yuan, Tangming},
  date = {2025-07-10},
  eprint = {2507.01936},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.01936},
  url = {http://arxiv.org/abs/2507.01936},
  urldate = {2025-09-11},
  abstract = {Large language models (LLMs) are excellent at maintaining high-level, convincing dialogues. They are being fast deployed as chatbots and evaluators in sensitive areas, such as peer review and mental health applications. This, along with the disparate accounts on their reasoning capabilities, calls for a closer examination of LLMs and their comprehension of dialogue. In this work we begin by evaluating LLMs' ability to maintain a debate--one of the purest yet most complex forms of human communication. Then we measure how this capability relates to their understanding of what is being talked about, namely, their comprehension of dialogical structures and the pragmatic context. We find that LLMs are capable of maintaining coherent, persuasive debates, often swaying the beliefs of participants and audiences alike. We also note that awareness or suspicion of AI involvement encourage people to be more critical of the arguments made. When polling LLMs on their comprehension of deeper structures of dialogue, however, they cannot demonstrate said understanding. Our findings tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand the context. More broadly, for the field of argumentation theory we posit that, if an agent can convincingly maintain a dialogue, it is not necessary for it to know what it is talking about. Hence, the modelling of pragmatic context and coherence are secondary to effectiveness.},
  pubstate = {prepublished}
}

@online{Xiao2025FoundationsLargeLanguage,
  title = {Foundations of {{Large Language Models}}},
  author = {Xiao, Tong and Zhu, Jingbo},
  date = {2025-01-16},
  eprint = {2501.09223},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.09223},
  url = {http://arxiv.org/abs/2501.09223},
  urldate = {2025-01-23},
  abstract = {This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.},
  pubstate = {prepublished}
}

@article{Xu2021UnderstandingGraphEmbedding,
  title = {Understanding {{Graph Embedding Methods}} and {{Their Applications}}},
  author = {Xu, Mengjia},
  date = {2021-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {63},
  number = {4},
  pages = {825--853},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/20M1386062},
  url = {https://epubs.siam.org/doi/10.1137/20M1386062},
  urldate = {2025-07-15},
  abstract = {Machine learning and deep learning algorithms are frequently used in critical tasks where their output is used in  high-stakes downstream applications. In these cases it is important to quantify the uncertainty in the predictions of these algorithms. Motivated by this, we present a novel  learning-based Bayesian inference approach for quantifying uncertainty in a prediction. Our approach uses samples drawn from the joint distribution of measurements and predictions to train a generative adversarial network (GAN). Thereafter, given a noisy measurement, we use the generator component of the GAN as a prior in a Bayesian update. By reformulating the resulting high-dimensional posterior sampling problem to the low-dimensional latent space of the GAN, we are able to perform efficient Markov Chain Monte Carlo (MCMC) updates. We apply this approach to image classification and image inpainting problems in computer vision and to forward and inverse uncertainty quantification tasks arising in computational physics, and demonstrate how the ability to quantify uncertainty can be used to (a) detect samples that lie outside the distribution of the training samples, (b) quantify the confidence in the prediction, and (c) determine the subsequent measurement within an active learning strategy.}
}

@article{Xu2024LargeLanguageModels,
  title = {Large Language Models for Generative Information Extraction: A Survey},
  shorttitle = {Large Language Models for Generative Information Extraction},
  author = {Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  date = {2024-11-11},
  journaltitle = {Frontiers of Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {18},
  number = {6},
  pages = {186357},
  issn = {2095-2236},
  doi = {10.1007/s11704-024-40555-y},
  url = {https://doi.org/10.1007/s11704-024-40555-y},
  urldate = {2025-09-17},
  abstract = {Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).},
  langid = {english}
}

@inproceedings{Yang2019CasebasedReasoningFacilitation,
  title = {Toward {{Case-based Reasoning Facilitation}} for {{Online Discussion}} in {{Deliberation}}},
  booktitle = {2019 {{IEEE}} 23rd {{International Conference}} on {{Computer Supported Cooperative Work}} in {{Design}} ({{CSCWD}})},
  author = {Yang, Chunsheng and Gu, Wen and Ito, Takayuki},
  date = {2019-05},
  pages = {517--523},
  doi = {10.1109/CSCWD.2019.8791866},
  abstract = {This paper presents a novel case-based reasoning (CBR) application to crowd-scale deliberation. We propose a CBR approach to facilitating online discussion for crowd-scale deliberation. The objective is to smooth the discussion by avoiding flaming and to efficiently achieve a consensus. Accordingly, several challenging issues are addressed, including case definition and its structure, case creation, and implementation by incorporating with COLLAGREE, a crowd-scale deliberation platform supporting online discussion with the help of facilitators. After introducing an overview of the crowd-scale deliberation and the COLLAGREE, the paper presents the details of the proposed CBR approach for facilitation of online discussion along with some preliminary results. The feasibility of CBR-based facilitation support for crowd-scale deliberations is demonstrated.},
  eventtitle = {2019 {{IEEE}} 23rd {{International Conference}} on {{Computer Supported Cooperative Work}} in {{Design}} ({{CSCWD}})}
}

@online{Yasunaga2023LargeLanguageModels,
  title = {Large {{Language Models}} as {{Analogical Reasoners}}},
  author = {Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H. and Zhou, Denny},
  date = {2023-10-02},
  eprint = {2310.01714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.01714},
  url = {http://arxiv.org/abs/2310.01714},
  urldate = {2023-10-09},
  abstract = {Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.},
  pubstate = {prepublished}
}

@inproceedings{Ye2024NetworkImplementationCBR,
  title = {Towards {{Network Implementation}} of~{{CBR}}: {{Case Study}} of~a~{{Neural Network K-NN Algorithm}}},
  shorttitle = {Towards {{Network Implementation}} of~{{CBR}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Ye, Xiaomeng and Leake, David and Wang, Yu and Zhao, Ziwei and Crandall, David},
  editor = {Recio-Garcia, Juan A. and Orozco-del-Castillo, Mauricio G. and Bridge, Derek},
  date = {2024},
  pages = {354--370},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63646-2_23},
  abstract = {Recent research brings the strengths of neural networks to bear on CBR tasks such as similarity assessment and case adaptation. This paper further advances this direction by implementing both retrieval and adaptation as a single neural network. Such an approach has multiple goals: From the perspective of CBR, it enables harmonizing the interaction between feature extraction, retrieval/similarity assessment, and case adaptation through end-to-end training. From the perspective of neural networks, a neural network implementing CBR processes ceases to be a black box and provides the natural interpretability of CBR. As a first step towards this goal, this paper presents neural network based k-nearest neighbor (NN-kNN), a network architecture that can be interpreted as a k-NN method. Unlike other network architectures, NN-kNN’s decisions can be fully explained in terms of surface features, feature/case weights and nearest neighbors. It can be trained or fine-tuned using existing neural network methods. This study illustrates its feasibility and examines its strengths and limitations. The approach is evaluated for classification and regression tasks comparing NN-kNN, a standard neural network, and k-NN models using state-of-the-art distance metric learning algorithms. In these tests, NN-kNN achieves equal or less error when compared to the other models, while being fully interpretable as a k-NN method. The study also considered the limitations of NN-kNN and future directions to alleviate them.},
  isbn = {978-3-031-63646-2},
  langid = {english}
}

@inproceedings{Yeginbergen2025DynamicKnowledgeIntegration,
  title = {Dynamic {{Knowledge Integration}} for {{Evidence-Driven Counter-Argument Generation}} with {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2025},
  author = {Yeginbergen, Anar and Oronoz, Maite and Agerri, Rodrigo},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {22568--22584},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.findings-acl.1161/},
  urldate = {2025-07-29},
  abstract = {This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially non-factual responses highlights the need for more controlled and evidence-based approaches. We introduce a reconstructed and manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems. Data and code are publicly available: https://github.com/anaryegen/ counter-argument-generation},
  eventtitle = {Findings 2025},
  isbn = {979-8-89176-256-5}
}

@unpublished{Young2018NotesAbstractArgumentation,
  title = {Notes on {{Abstract Argumentation Theory}}},
  author = {Young, Anthony Peter},
  date = {2018-06-18},
  eprint = {1806.07709},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.07709},
  urldate = {2019-08-21},
  abstract = {This note reviews Section 2 of Dung's seminal 1995 paper on abstract argumentation theory. In particular, we clarify and make explicit all of the proofs mentioned therein, and provide many more examples to the definitions, in a way that should be helpful to readers approaching abstract argumentation theory for the first time. However, we provide minimal commentary and will refer the reader to Dung's paper for the intuitions behind various concepts. The appropriate mathematical prerequisites are provided in the appendices.}
}

@article{Young2022ModellingOnlineDebates,
  title = {Modelling Online Debates with Argumentation Theory},
  author = {Young, Anthony P. and Joglekar, Sagar and Agarwal, Vibhor and Sastry, Nishanth},
  date = {2022-05-12},
  journaltitle = {ACM SIGWEB Newsletter},
  shortjournal = {SIGWEB Newsl.},
  volume = {2022},
  pages = {4:1--4:9},
  issn = {1931-1745},
  doi = {10.1145/3533274.3533278},
  url = {https://dl.acm.org/doi/10.1145/3533274.3533278},
  urldate = {2023-10-20},
  abstract = {It is important to study and understand Internet debates because they often have consequences in the offline world, for better or worse. We show that argumentation theory, a branch of AI concerned with the resolution of disagreements, provides a powerful toolbox with which we can represent and reason about such debates. After summarising the relevant ideas of argumentation theory, we overview three recent contributions from the authors and their collaborators: (1) on how to automatically identify reply polarity (agreement or disagreement) between arguments submitted in online debates, (2) on locating where the justified arguments are likely to be and how that depends on the "degree of antagonism" of the debate, and (3) on how to present the arguments made in debates such that a reader would get as many of the justified arguments as possible without having to read the entire debate. We hope that this will lead to further work that applies argumentation theory to model and analyse online debates.},
  issue = {Spring}
}

@inproceedings{Yousaf2026AccessibleLanguageSimplification,
  title = {Accessible {{Language Simplification}}: {{Large Language Models}} for~{{Generating Easy German}}},
  shorttitle = {Accessible {{Language Simplification}}},
  booktitle = {{{KI}} 2025: {{Advances}} in {{Artificial Intelligence}}},
  author = {Yousaf, Raeesa and Walther, Marina and Gertz, Michael},
  editor = {Braun, Tanya and Paaßen, Benjamin and Stolzenburg, Frieder},
  date = {2026},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {15956},
  pages = {204--217},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-032-02813-6_15},
  abstract = {The growing application of Large Language Models (LLMs) in text simplification holds significant promise for improving accessibility for non-native speakers and individuals with learning or cognitive disabilities. We examine the effectiveness of LLMs in generating Easy German responses through a domain-agnostic Question-Answering (QA) framework, leveraging explicit simplification rules and tailored prompting strategies. Focused on health-related questions, the framework transforms complex medical information into accessible language to enhance health literacy. Automated readability and semantic alignment metrics are combined with human evaluations from non-native speakers (A1-B1 CEFR proficiency) and individuals with special needs. Results show that GPT-4 consistently outperforms open-source models like Llama and Mixtral, generating factually accurate, clear, and accessible responses, while the latter models often struggle with coherence. Though developed for Easy German, the domain-agnostic methodology can be adapted to any language with minimal prompt adjustments. For reproducibility and material, please refer to our GitHub repository\$\$\textasciicircum\{1\}\$\$1https://github.com/raeesay/easy-german-qa.},
  eventtitle = {German {{Conference}} on {{Artificial Intelligence}}},
  isbn = {978-3-032-02813-6},
  langid = {english}
}

@article{Zapf2016MeasuringInterraterReliability,
  title = {Measuring Inter-Rater Reliability for Nominal Data – Which Coefficients and Confidence Intervals Are Appropriate?},
  author = {Zapf, Antonia and Castell, Stefanie and Morawietz, Lars and Karch, André},
  date = {2016-08-05},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {16},
  number = {1},
  pages = {93},
  issn = {1471-2288},
  doi = {10.1186/s12874-016-0200-9},
  url = {https://doi.org/10.1186/s12874-016-0200-9},
  urldate = {2021-02-23},
  abstract = {Reliability of measurements is a prerequisite of medical research. For nominal data, Fleiss’ kappa (in the following labelled as Fleiss’ K) and Krippendorff’s alpha provide the highest flexibility of the available reliability measures with respect to number of raters and categories. Our aim was to investigate which measures and which confidence intervals provide the best statistical properties for the assessment of inter-rater reliability in different situations.}
}

@inproceedings{Zelch2025ReproducingArgumentQuality,
  title = {Reproducing the {{Argument Quality Prediction}} of {{Project Debater}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Zelch, Ines and Hagen, Matthias and Stein, Benno and Kiesel, Johannes},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {181--188},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.17/},
  urldate = {2025-07-28},
  abstract = {A crucial task when analyzing arguments is to determine their quality. Especially when you have to choose from a large number of suitable arguments, the determination of a reliable argument quality value is of great benefit. Probably the best-known model for determining such an argument quality value was developed in IBM's Project Debater and made available to the research community free of charge via an API. In fact, the model was never open and the API is no longer available. In this paper, IBM's model is reproduced using the freely available training data and the description in the corresponding publication. Our reproduction achieves similar results on the test data as described in the original publication. Further, the predicted quality scores of reproduction and original show a very high correlation (Pearson's r=0.9) on external data.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@inproceedings{Zelch2025SegmentationArgumentativeTexts,
  title = {Segmentation of {{Argumentative Texts}} by {{Key Statements}} for {{Argument Mining}} from the {{Web}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Zelch, Ines and Hagen, Matthias and Stein, Benno and Kiesel, Johannes},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {228--242},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.22/},
  urldate = {2025-07-28},
  abstract = {Argument mining is the task of identifying the argument structure of a text: claims, premises, support/attack relations, etc. However, determining the complete argument structure can be quite involved, especially for unpolished texts from online forums, while for many applications the identification of argumentative key statements would suffice (e.g., for argument search). To this end, we introduce and investigate the new task of segmenting an argumentative text by its key statements. We formalize the task, create a first dataset from online communities, propose an evaluation scheme, and conduct a pilot study with several approaches. Interestingly, our experimental results indicate that none of the tested approaches (even LLM-based ones) can actually satisfactorily solve key statement segmentation yet.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@article{Zeng2009ComparingStarsApproximating,
  title = {Comparing Stars: On Approximating Graph Edit Distance},
  shorttitle = {Comparing Stars},
  author = {Zeng, Zhiping and Tung, Anthony K. H. and Wang, Jianyong and Feng, Jianhua and Zhou, Lizhu},
  date = {2009-08-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {2},
  number = {1},
  pages = {25--36},
  issn = {2150-8097},
  doi = {10.14778/1687627.1687631},
  url = {https://dl.acm.org/doi/10.14778/1687627.1687631},
  urldate = {2025-06-02},
  abstract = {Graph data have become ubiquitous and manipulating them based on similarity is essential for many applications. Graph edit distance is one of the most widely accepted measures to determine similarities between graphs and has extensive applications in the fields of pattern recognition, computer vision etc. Unfortunately, the problem of graph edit distance computation is NP-Hard in general. Accordingly, in this paper we introduce three novel methods to compute the upper and lower bounds for the edit distance between two graphs in polynomial time. Applying these methods, two algorithms AppFull and AppSub are introduced to perform different kinds of graph search on graph databases. Comprehensive experimental studies are conducted on both real and synthetic datasets to examine various aspects of the methods for bounding graph edit distance. Result shows that these methods achieve good scalability in terms of both the number of graphs and the size of graphs. The effectiveness of these algorithms also confirms the usefulness of using our bounds in filtering and searching of graphs.}
}

@inproceedings{Zeyen2019AdaptationScientificWorkflows,
  title = {Adaptation of {{Scientific Workflows}} by {{Means}} of {{Process-Oriented Case-Based Reasoning}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Zeyen, Christian and Malburg, Lukas and Bergmann, Ralph},
  editor = {Bach, Kerstin and Marling, Cindy},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {388--403},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-29249-2_26},
  abstract = {This paper investigates automatic adaptation of scientific workflows in process-oriented case-based reasoning with the goal of providing modeling assistance. With regard to our previous work on the adaptation of business workflows, we discuss the differences between the workflow types and the implications for transferring the approaches to scientific workflows. An experimental evaluation with RapidMiner workflows demonstrates that the approaches can significantly improve workflows towards a given query while mostly maintaining their executability and semantic correctness.},
  isbn = {978-3-030-29249-2},
  langid = {english}
}

@inproceedings{Zeyen2020ABasedSimilarityAssessment,
  title = {A*-{{Based Similarity Assessment}} of {{Semantic Graphs}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Zeyen, Christian and Bergmann, Ralph},
  editor = {Watson, Ian and Weber, Rosina},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {17--32},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58342-2_2},
  abstract = {The similarity assessment of graphs is a fundamental problem that is particularly challenging if efficiency is of core importance. In this paper, we focus on a similarity measure for semantically labeled graphs whose labels are composed in an object-oriented manner. The measure is based on A* search and is particularly suited for case-based reasoning as it can be combined with knowledge-intensive local similarity measures and outputs similarities and corresponding mappings usable for explanation and adaptation. However, particularly for large graphs, the search space must be pruned to improve efficiency of A* search at the cost of sacrificing global optimality. We address this issue and present complementary improvements of the measure, which we systematically evaluate for the similarity assessment of semantic workflow graphs. The experimental results demonstrate that the new measure considerably reduces the computation time and memory consumption while increasing the accuracy.},
  isbn = {978-3-030-58342-2},
  langid = {english}
}

@article{Zhan2020GraphMatchingBased,
  title = {Graph Matching Based on Local and Global Information of the Graph Nodes},
  author = {Zhan, Yaru and Zhao, Xiuyang and Lin, Xue and Liu, Junkai and Liu, Mingjun and Niu, Dongmei},
  date = {2020-05-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {79},
  number = {17},
  pages = {11567--11590},
  issn = {1573-7721},
  doi = {10.1007/s11042-019-08516-x},
  url = {https://doi.org/10.1007/s11042-019-08516-x},
  urldate = {2025-05-14},
  abstract = {Graph matching is an essential NP-problem in computer vision and pattern recognition. In this paper, we propose an approximate graph matching method. This method formulates the problem of computing the correspondences between two graphs as a problem of selecting nodes on an association graph. The nodes of the association graph represent candidate correspondences between the two original graphs. Our method first constructs an affinity matrix based on both the global and local information of the original graphs’ nodes. Each element of this matrix is used to measure the mutual consistency of a pair of nodes within the association graph. Our method then applies the reweighted random walks technique that preserves the one-to-one matching constraint to simulate random walks on the association graph and to iteratively compute a quasi-stationary distribution. To discretize this distribution, our method finally applies the Hungarian algorithm and obtains an approximate matching between the original two graphs. Experimental results demonstrate the effectiveness of our method for graph matching and the ability of our method for being robust to outlier and deformation noise.},
  langid = {english}
}

@unpublished{Zhang2020BERTScoreEvaluatingText,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  shorttitle = {{{BERTScore}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  date = {2020-02-24},
  eprint = {1904.09675},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.09675},
  urldate = {2021-03-09},
  abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.}
}

@article{Zhang2021StudyBugResolution,
  title = {A {{Study}} of {{Bug Resolution Characteristics}} in {{Popular Programming Languages}}},
  author = {Zhang, Jie M. and Li, Feng and Hao, Dan and Wang, Meng and Tang, Hao and Zhang, Lu and Harman, Mark},
  date = {2021-12},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {12},
  pages = {2684--2697},
  issn = {1939-3520},
  doi = {10.1109/TSE.2019.2961897},
  abstract = {This paper presents a large-scale study that investigates the bug resolution characteristics among popular Github projects written in different programming languages. We explore correlations but, of course, we cannot infer causation. Specifically, we analyse bug resolution data from approximately 70 million Source Line of Code, drawn from 3 million commits to 600 GitHub projects, primarily written in 10 programming languages. We find notable variations in apparent bug resolution time and patch (fix) size. While interpretation of results from such large-scale empirical studies is inherently difficult, we believe that the differences in medians are sufficiently large to warrant further investigation, replication, re-analysis and follow up research. For example, in our corpus, the median apparent bug resolution time (elapsed time from raise to resolve) for Ruby was 4X that for Go and 2.5X for Java. We also found that patches tend to touch more files for the corpus of strongly typed and for statically typed programs. However, we also found evidence for a lower elapsed resolution time for bug resolution committed to projects constructed from statically typed languages. These findings, if replicated in subsequent follow on studies, may shed further empirical light on the debate about the importance of static typing.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}}
}

@inproceedings{Zhang2023ImprovingSentenceEmbedding,
  title = {Improving {{Sentence Embedding With Sentence Relationships From Word Analogies}}},
  booktitle = {Proceedings of the {{Workshops}} at the 31st {{International Conference}} on {{Case-Based Reasoning}} ({{ICCBR-WS}} 2023)},
  author = {Zhang, Qixuan and Lepage, Yves},
  editor = {Malburg, Lukas and Verma, Deepika},
  date = {2023-07-17},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3438},
  pages = {43--53},
  publisher = {CEUR},
  location = {Aberdeen, Scotland},
  issn = {1613-0073},
  url = {https://ceur-ws.org/Vol-3438/#paper_04},
  urldate = {2023-07-31},
  eventtitle = {{{ICCBR}} 2023 {{Workshop Proceedings}}},
  langid = {english}
}

@article{Zhao2021EvaluationIndicatorsOpensource,
  title = {Evaluation Indicators for Open-Source Software: A Review},
  shorttitle = {Evaluation Indicators for Open-Source Software},
  author = {Zhao, Yuhang and Liang, Ruigang and Chen, Xiang and Zou, Jing},
  date = {2021-06-02},
  journaltitle = {Cybersecurity},
  shortjournal = {Cybersecurity},
  volume = {4},
  number = {1},
  pages = {20},
  issn = {2523-3246},
  doi = {10.1186/s42400-021-00084-8},
  url = {https://doi.org/10.1186/s42400-021-00084-8},
  urldate = {2023-10-05},
  abstract = {In recent years, the widespread applications of open-source software (OSS) have brought great convenience for software developers. However, it is always facing unavoidable security risks, such as open-source code defects and security vulnerabilities. To find out the OSS risks in time, we carry out an empirical study to identify the indicators for evaluating the OSS. To achieve a comprehensive understanding of the OSS assessment, we collect 56 papers from prestigious academic venues (such as IEEE Xplore, ACM Digital Library, DBLP, and Google Scholar) in the past 21 years. During the process of the investigation, we first identify the main concerns for selecting OSS and distill five types of commonly used indicators to assess OSS. We then conduct a comparative analysis to discuss how these indicators are used in each surveyed study and their differences. Moreover, we further undertake a correlation analysis between these indicators and uncover 13 confirmed conclusions and four cases with controversy occurring in these studies. Finally, we discuss several possible applications of these conclusions, which are insightful for the research on OSS and software supply chain.}
}

@inproceedings{Zhao2023RetrievingMultimodalInformation,
  title = {Retrieving {{Multimodal Information}} for {{Augmented Generation}}: {{A Survey}}},
  shorttitle = {Retrieving {{Multimodal Information}} for {{Augmented Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Zhao, Ruochen and Chen, Hailin and Wang, Weishi and Jiao, Fangkai and Do, Xuan Long and Qin, Chengwei and Ding, Bosheng and Guo, Xiaobao and Li, Minzhi and Li, Xingxuan and Joty, Shafiq},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {4736--4756},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.314},
  url = {https://aclanthology.org/2023.findings-emnlp.314},
  urldate = {2024-09-30},
  abstract = {As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs' generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods' applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.},
  eventtitle = {Findings 2023}
}

@unpublished{Zhelezniak2019DonSettleAverage,
  title = {Don't {{Settle}} for {{Average}}, {{Go}} for the {{Max}}: {{Fuzzy Sets}} and {{Max-Pooled Word Vectors}}},
  shorttitle = {Don't {{Settle}} for {{Average}}, {{Go}} for the {{Max}}},
  author = {Zhelezniak, Vitalii and Savkov, Aleksandar and Shen, April and Moramarco, Francesco and Flann, Jack and Hammerla, Nils Y.},
  date = {2019-04-30},
  eprint = {1904.13264},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.13264},
  urldate = {2019-09-12},
  abstract = {Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin and is even competitive with supervised word vectors trained to directly optimise cosine similarity.}
}

@inproceedings{Zheng2025FanChuanMultilingualGraphStructured,
  title = {{{FanChuan}}: {{A Multilingual}} and {{Graph-Structured Benchmark For Parody Detection}} and {{Analysis}}},
  shorttitle = {{{FanChuan}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2025},
  author = {Zheng, Yilun and Li, Sha and Wu, Fangkun and Ziyi, Yang and Hongchao, Lin and Hu, Zhichao and Xinjun, Cai and Wang, Ziming and Chen, Jinxuan and Luan, Sitao and Xu, Jiahao and Chen, Lihui},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {21937--21957},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.findings-acl.1131/},
  urldate = {2025-08-01},
  abstract = {Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs.},
  eventtitle = {Findings 2025},
  isbn = {979-8-89176-256-5}
}

@inproceedings{Zhong2002ConceptualGraphMatching,
  title = {Conceptual {{Graph Matching}} for {{Semantic Search}}},
  booktitle = {Conceptual {{Structures}}: {{Integration}} and {{Interfaces}}},
  author = {Zhong, Jiwei and Zhu, Haiping and Li, Jianming and Yu, Yong},
  editor = {Priss, Uta and Corbett, Dan and Angelova, Galia},
  date = {2002},
  pages = {92--106},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45483-7_8},
  abstract = {Semantic search becomes a research hotspot. The combined use of linguistic ontologies and structured semantic matching is one of the promising ways to improve both recall and precision. In this paper, we propose an approach for semantic search by matching conceptual graphs. The detailed definitions of semantic similarities between concepts, relations and conceptual graphs are given. According to these definitions of semantic similarity, we propose our conceptual graph matching algorithm that calculates the semantic similarity. The computation complexity of this algorithm is constrained to be polynomial. A prototype of our approach is currently under development with IBM China Research Lab.},
  isbn = {978-3-540-45483-0},
  langid = {english}
}

@article{Zhou2019PredictingConceptNetPath,
  title = {Predicting {{ConceptNet Path Quality Using Crowdsourced Assessments}} of {{Naturalness}}},
  author = {Zhou, Yilun and Schockaert, Steven and Shah, Julie A.},
  date = {2019},
  journaltitle = {The World Wide Web Conference on   - WWW '19},
  eprint = {1902.07831},
  eprinttype = {arXiv},
  pages = {2460--2471},
  doi = {10.1145/3308558.3313486},
  url = {http://arxiv.org/abs/1902.07831},
  urldate = {2020-05-30},
  abstract = {In many applications, it is important to characterize the way in which two concepts are semantically related. Knowledge graphs such as ConceptNet provide a rich source of information for such characterizations by encoding relations between concepts as edges in a graph. When two concepts are not directly connected by an edge, their relationship can still be described in terms of the paths that connect them. Unfortunately, many of these paths are uninformative and noisy, which means that the success of applications that use such path features crucially relies on their ability to select high-quality paths. In existing applications, this path selection process is based on relatively simple heuristics. In this paper we instead propose to learn to predict path quality from crowdsourced human assessments. Since we are interested in a generic task-independent notion of quality, we simply ask human participants to rank paths according to their subjective assessment of the paths' naturalness, without attempting to define naturalness or steering the participants towards particular indicators of quality. We show that a neural network model trained on these assessments is able to predict human judgments on unseen paths with near optimal performance. Most notably, we find that the resulting path selection method is substantially better than the current heuristic approaches at identifying meaningful paths.}
}

@article{Zhou2020SurveyFakeNews,
  title = {A {{Survey}} of {{Fake News}}: {{Fundamental Theories}}, {{Detection Methods}}, and {{Opportunities}}},
  shorttitle = {A {{Survey}} of {{Fake News}}},
  author = {Zhou, Xinyi and Zafarani, Reza},
  date = {2020-09-28},
  journaltitle = {ACM Comput. Surv.},
  volume = {53},
  number = {5},
  pages = {109:1--109:40},
  issn = {0360-0300},
  doi = {10.1145/3395046},
  url = {https://dl.acm.org/doi/10.1145/3395046},
  urldate = {2025-07-15},
  abstract = {The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news detection and intervention. This survey reviews and evaluates methods that can detect fake news from four perspectives: the false knowledge it carries, its writing style, its propagation patterns, and the credibility of its source. The survey also highlights some potential research tasks based on the review. In particular, we identify and detail related fundamental theories across various disciplines to encourage interdisciplinary research on fake news. It is our hope that this survey can facilitate collaborative efforts among experts in computer and information sciences, social sciences, political science, and journalism to research fake news, where such efforts can lead to fake news detection that is not only efficient but, more importantly, explainable.}
}

@inproceedings{Zhou2025AspectBasedOpinionSummarization,
  title = {Aspect-{{Based Opinion Summarization}} with {{Argumentation Schemes}}},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Argument Mining}}},
  author = {Zhou, Wendi and Saadat-Yazdi, Ameer and Kökciyan, Nadin},
  editor = {Chistova, Elena and Cimiano, Philipp and Haddadan, Shohreh and Lapesa, Gabriella and Ruiz-Dolz, Ramon},
  date = {2025-07},
  pages = {116--125},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.argmining-1.11/},
  urldate = {2025-07-28},
  abstract = {Reviews are valuable resources for customers making purchase decisions in online shopping. However, it is impractical for customers to go over the vast number of reviews and manually conclude the prominent opinions, which prompts the need for automated opinion summarization systems. Previous approaches, either extractive or abstractive, face challenges in automatically producing grounded aspect-centric summaries. In this paper, we propose a novel summarization system that not only captures predominant opinions from an aspect perspective with supporting evidence, but also adapts to varying domains without relying on a pre-defined set of aspects. Our proposed framework, ASESUM, summarizes viewpoints relevant to the critical aspects of a product by extracting aspect-centric arguments and measuring their salience and validity. We conduct experiments on a real-world dataset to demonstrate the superiority of our approach in capturing diverse perspectives of the original reviews compared to new and existing methods.},
  eventtitle = {{{ArgMining}} 2025},
  isbn = {979-8-89176-258-9}
}

@article{Zhu2017ComputingSemanticSimilarity,
  title = {Computing {{Semantic Similarity}} of {{Concepts}} in {{Knowledge Graphs}}},
  author = {Zhu, Ganggao and Iglesias, Carlos A.},
  date = {2017-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {29},
  number = {1},
  pages = {72--85},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2016.2610428},
  abstract = {This paper presents a method for measuring the semantic similarity between concepts in Knowledge Graphs (KGs) such as WordNet and DBpedia. Previous work on semantic similarity methods have focused on either the structure of the semantic network between concepts (e.g., path length and depth), or only on the Information Content (IC) of concepts. We propose a semantic similarity method, namely wpath, to combine these two approaches, using IC to weight the shortest path length between concepts. Conventional corpus-based IC is computed from the distributions of concepts over textual corpus, which is required to prepare a domain corpus containing annotated concepts and has high computational cost. As instances are already extracted from textual corpus and annotated by concepts in KGs, graph-based IC is proposed to compute IC based on the distributions of concepts over instances. Through experiments performed on well known word similarity datasets, we show that the wpath semantic similarity method has produced a statistically significant improvement over other semantic similarity methods. Moreover, in a real category classification evaluation, the wpath method has shown the best performance in terms of accuracy and F score.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}}
}

@online{Zhu2023CanChatGPTReproduce,
  title = {Can {{ChatGPT Reproduce Human-Generated Labels}}? {{A Study}} of {{Social Computing Tasks}}},
  shorttitle = {Can {{ChatGPT Reproduce Human-Generated Labels}}?},
  author = {Zhu, Yiming and Zhang, Peixian and Haq, Ehsan-Ul and Hui, Pan and Tyson, Gareth},
  date = {2023-04-22},
  eprint = {2304.10145},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.10145},
  url = {http://arxiv.org/abs/2304.10145},
  urldate = {2024-02-10},
  abstract = {The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to relabel five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average accuracy 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9\% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploitation of ChatGPT for human annotation tasks.},
  pubstate = {prepublished}
}

@inproceedings{Zhuocheng2025FlexRAGFlexibleComprehensive,
  title = {{{FlexRAG}}: {{A Flexible}} and {{Comprehensive Framework}} for {{Retrieval-Augmented Generation}}},
  shorttitle = {{{FlexRAG}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Zhuocheng, Zhang and Feng, Yang and Zhang, Min},
  editor = {Mishra, Pushkar and Muresan, Smaranda and Yu, Tao},
  date = {2025-07},
  pages = {621--631},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  url = {https://aclanthology.org/2025.acl-demo.60/},
  urldate = {2025-07-28},
  abstract = {Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems.However, we have identified several persistent challenges in these frameworks, including lack of new techniques, difficulties in algorithm reproduction and sharing, and high system overhead.To address these limitations, we introduce **FlexRAG**, an open-source framework specifically designed for research and prototyping.FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities.By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems.Our toolkit and resources are available at https://github.com/ictnlp/FlexRAG.},
  isbn = {979-8-89176-253-4}
}

@inproceedings{Ziegenbein2024ObjectiveArgumentSummarization,
  title = {Objective {{Argument Summarization}} in~{{Search}}},
  booktitle = {Robust {{Argumentation Machines}}},
  author = {Ziegenbein, Timon and Syed, Shahbaz and Potthast, Martin and Wachsmuth, Henning},
  editor = {Cimiano, Philipp and Frank, Anette and Kohlhase, Michael and Stein, Benno},
  date = {2024},
  pages = {335--351},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-63536-6_20},
  abstract = {Decision-making and opinion formation are influenced by arguments from various online sources, including social media, web publishers, and, not least, the search engines used to retrieve them. However, many, if not most, arguments on the web are informal, especially in online discussions or on personal pages. They can be long and unstructured, subjective and emotional, and contain inappropriate language. This makes it difficult to find relevant arguments efficiently. We hypothesize that, on search engine results pages, “objective snippets” of arguments are better suited than the commonly used extractive snippets and develop corresponding methods for two important tasks: snippet generation and neutralization. For each of these tasks, we investigate two approaches based on (1)~prompt engineering for large language models~(LLMs), and (2)~supervised models trained on existing datasets. We find that a supervised summarization model outperforms zero-shot summarization with LLMs for snippet generation. For neutralization, using reinforcement learning to align an LLM with human preferences for suitable arguments leads to the best results. Both tasks are complementary, and their combination leads to the best snippets of arguments according to automatic and human evaluation.},
  isbn = {978-3-031-63536-6},
  langid = {english}
}

@inproceedings{Zografistou2017ArgQLDeclarativeLanguage,
  title = {{{ArgQL}}: {{A Declarative Language}} for {{Querying Argumentative Dialogues}}},
  shorttitle = {{{ArgQL}}},
  booktitle = {Rules and {{Reasoning}}},
  author = {Zografistou, Dimitra and Flouris, Giorgos and Plexousakis, Dimitris},
  editor = {Costantini, Stefania and Franconi, Enrico and Van Woensel, William and Kontchakov, Roman and Sadri, Fariba and Roman, Dumitru},
  date = {2017},
  pages = {230--237},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-61252-2_16},
  abstract = {We introduce ArgQL, a declarative query language, which performs on a data model designed according to the principles of argumentation. Its syntax is based on Cypher (language for graph databases) and SPARQL 1.1 and is adjusted for querying dialogues, composed by sets of arguments and their interrelations. We use formal semantics to show how queries in ArgQL match against data in the argumentation model. The execution is realized by translating both data and queries into standard models for storage and querying.},
  isbn = {978-3-319-61252-2},
  langid = {english}
}

@inproceedings{Zografistou2018ImplementingArgQLQuery,
  title = {Implementing the {{ArgQL Query Language}}},
  booktitle = {International {{Conference}} on {{Computational Models}} of {{Argument}}},
  author = {Zografistou, Dimitra and Flouris, Giorgos and Patkos, Theodore and Plexousakis, Dimitris},
  date = {2018},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {305},
  pages = {241--248},
  publisher = {IOS Press},
  location = {Warsaw, Poland},
  doi = {10.3233/978-1-61499-906-5-241},
  url = {https://ebooks.iospress.nl/publication/50196},
  urldate = {2025-03-21},
  abstract = {Exploration and information identification constitute challenging research problems, with important applications in sensemaking over structured argumentative dialogues. In this paper, we present the implementation ArgQL, a high-level declarative query language, designed for querying dialogical data, structured in the principles of argumentation. We implement the language using an AIF-based representation and a translation of ArgQL into (complex) SPARQL queries. ArgQL provides a simple and intuitive way to query a structured dialogue using pure argumentative terminology.},
  eventtitle = {Computational {{Models}} of {{Argument}}},
  langid = {english}
}

@article{Zubiaga2018DetectionResolutionRumours,
  title = {Detection and {{Resolution}} of {{Rumours}} in {{Social Media}}: {{A Survey}}},
  shorttitle = {Detection and {{Resolution}} of {{Rumours}} in {{Social Media}}},
  author = {Zubiaga, Arkaitz and Aker, Ahmet and Bontcheva, Kalina and Liakata, Maria and Procter, Rob},
  date = {2018-02-20},
  journaltitle = {ACM Comput. Surv.},
  volume = {51},
  number = {2},
  pages = {32:1--32:36},
  issn = {0360-0300},
  doi = {10.1145/3161603},
  url = {https://dl.acm.org/doi/10.1145/3161603},
  urldate = {2025-07-15},
  abstract = {Despite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e., items of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how to automatically assess their veracity, using natural language processing and data mining techniques. In this article, we introduce and discuss two types of rumours that circulate on social media: long-standing rumours that circulate for long periods of time, and newly emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification, and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far toward the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for the detection and resolution of rumours.}
}

@online{Zweiger2025SelfAdaptingLanguageModels,
  title = {Self-{{Adapting Language Models}}},
  author = {Zweiger, Adam and Pari, Jyothish and Guo, Han and Akyürek, Ekin and Kim, Yoon and Agrawal, Pulkit},
  date = {2025-06-12},
  eprint = {2506.10943},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.10943},
  url = {http://arxiv.org/abs/2506.10943},
  urldate = {2025-06-16},
  abstract = {Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.},
  pubstate = {prepublished}
}

@article{Zwick1988AnotherLookInterrater,
  title = {Another Look at Interrater Agreement.},
  author = {Zwick, Rebecca},
  date = {1988},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {103},
  number = {3},
  pages = {374--378},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.103.3.374},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.103.3.374},
  urldate = {2021-03-07},
  langid = {english}
}
